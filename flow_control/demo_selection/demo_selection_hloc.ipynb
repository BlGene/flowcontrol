{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50eb4b93",
   "metadata": {},
   "source": [
    "# Demo Manipulation\n",
    "\n",
    "## Export Data\n",
    "\n",
    "Feature extraction and matching work directly from images, so export these first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f54f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm  # notebook-friendly progress bars\n",
    "\n",
    "from hloc import extract_features, match_features, reconstruction, visualization, pairs_from_exhaustive\n",
    "from hloc.visualization import plot_images, read_image, plot_keypoints\n",
    "from hloc.utils import viz_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39568d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_control.demo.playback_env_servo import PlaybackEnvServo\n",
    "from flow_control.localize.hloc_utils import export_images_by_parts\n",
    "\n",
    "# root_dir = Path(\"/home/argusm/CLUSTER/robot_recordings/flow/recombination/2023-01-24\")\n",
    "root_dir = Path(\"/home/argusm/Desktop/Demonstrations/2023-01-24\")\n",
    "# root_dir = Path(\"/home/nayakab/Desktop/Courses/WS2022/Project/cluster/\")\n",
    "parts_fn = root_dir / 'parts.json'\n",
    "hloc_root = root_dir.parent / ( str(root_dir.name) + '_hloc')\n",
    "\n",
    "mapping_dir = hloc_root / 'mapping'\n",
    "outputs = hloc_root / 'outputs'\n",
    "sfm_pairs = outputs / 'pairs-sfm.txt'\n",
    "loc_pairs = outputs / 'pairs-loc.txt'\n",
    "sfm_dir = outputs / 'sfm'\n",
    "features_path = outputs / 'features.h5'\n",
    "matches_path = outputs / 'matches.h5'\n",
    "features_seg_path = outputs / 'features_seg.h5'\n",
    "\n",
    "if parts_fn.is_file():\n",
    "    with open(parts_fn) as f_obj:\n",
    "        parts_references = json.load(f_obj)\n",
    "        print(\"Succesfully loaded parts. --> Skip to \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db439a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_control.localize.hloc_utils import to_hloc_ref\n",
    "with open(parts_fn) as f_obj:\n",
    "    tmp = json.load(f_obj)\n",
    "\n",
    "parts_references = {}\n",
    "parts_references['locate'] = [to_hloc_ref(k,v['locate'][0]) for k,v in tmp.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $outputs\n",
    "!rm -rf $mapping_dir\n",
    "parts_references = export_images_by_parts(root_dir, parts_fn, mapping_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517be8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "references_all = [ref for ref_part in parts_references.values() for ref in ref_part]\n",
    "references_files = sorted([p.relative_to(hloc_root).as_posix() for p in (hloc_root / 'mapping/').iterdir()])\n",
    "assert len(set(references_all)-set(references_files)) == 0\n",
    "references = parts_references['locate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c0e09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(references), \"mapping images\")\n",
    "plot_images([read_image(hloc_root / r) for r in references[:4]], dpi=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e468e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_control.localize.hloc_utils import save_features_seg\n",
    "\n",
    "\n",
    "feature_conf = extract_features.confs['superpoint_aachen']\n",
    "matcher_conf = match_features.confs['superglue']\n",
    "\n",
    "extract_features.main(feature_conf, hloc_root, image_list=references_all, feature_path=features_path)\n",
    "save_features_seg(root_dir, features_seg_path, features_path, references_all)\n",
    "\n",
    "pairs_from_exhaustive.main(sfm_pairs, image_list=references)\n",
    "match_features.main(matcher_conf, sfm_pairs, features=features_path, matches=matches_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7c318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hloc.utils.io import get_keypoints\n",
    "\n",
    "num_images = 4\n",
    "plot_images([read_image(hloc_root / r) for r in references[:num_images]], dpi=75)\n",
    "plot_keypoints([get_keypoints(features_path, r) for r in references[:num_images]], colors='lime', ps=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c5f1ac",
   "metadata": {},
   "source": [
    "## Load Match Database\n",
    "\n",
    "hloc saves all features and matches in a SQL database, so reading these is the easiest option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b06f705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hloc.utils.io import get_keypoints\n",
    "from flow_control.localize.hloc_utils import get_segmentation\n",
    "\n",
    "name0 = references[1]\n",
    "kps0, noise0 = get_keypoints(features_path, name0, return_uncertainty=True)\n",
    "kps0_seg, noise0 = get_keypoints(features_seg_path, name0, return_uncertainty=True)\n",
    "seg = get_segmentation(root_dir, name0)\n",
    "\n",
    "plot_images([read_image(hloc_root / r) for r in [name0, ]]+[seg], dpi=75)\n",
    "plot_keypoints([kps0, kps0_seg], colors='lime', ps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e241235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hloc.utils.io import get_matches\n",
    "from flow_control.localize.hloc_utils import kp_seg_filter\n",
    "\n",
    "name_q = references[1]\n",
    "name_d = references[3]\n",
    "\n",
    "matches, scores = get_matches(matches_path, name_q, name_d)\n",
    "kps_q, noise_q = get_keypoints(features_path, name_q, return_uncertainty=True)\n",
    "kps_d, noise_d = get_keypoints(features_path, name_d, return_uncertainty=True)\n",
    "kps_q_match = kps_q[matches[:, 0]]\n",
    "kps_d_match = kps_d[matches[:, 1]]\n",
    "\n",
    "#%prun in_seg = kp_seg_filter_pb(kps_d_match, name_d)\n",
    "in_seg = kp_seg_filter(kps_d_match, name_d, features_seg_path)\n",
    "\n",
    "print(\"in_seg\", in_seg)\n",
    "print(kps_d_match[in_seg].shape)\n",
    "\n",
    "kps_q_seg = kps_q_match[in_seg]\n",
    "kps_d_seg = kps_d_match[in_seg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74fc0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "import ipdb\n",
    "\n",
    "with open(parts_fn, 'r') as f_obj:\n",
    "    part_info = json.load(f_obj)\n",
    "\n",
    "def get_tcp_position_orn(demo_dir, frame_index):\n",
    "    arr = np.load(os.path.join(demo_dir, f\"frame_{frame_index:06d}.npz\"),allow_pickle=True)\n",
    "    state = arr[\"robot_state\"].item()\n",
    "    return state[\"tcp_pos\"], state['tcp_orn']\n",
    "\n",
    "def get_tcp_position_at_grasp(name_q):\n",
    "    tmp = name_q.strip().split('/')[1]\n",
    "    rec_name = tmp.split('_')[0]\n",
    "    \n",
    "    rec_path = os.path.join(root_dir, rec_name)\n",
    "    gripper_close_idx = part_info[rec_name]['insert'][0]\n",
    "    \n",
    "    pos, orn = get_tcp_position_orn(rec_path, gripper_close_idx)\n",
    "    return pos, orn\n",
    "\n",
    "def compute_distance(name_q, name_d):\n",
    "    pos_q, orn_q = get_tcp_position_at_grasp(name_q)\n",
    "    pos_d, orn_d = get_tcp_position_at_grasp(name_d)\n",
    "    \n",
    "    orn_q = R.from_quat(orn_q)\n",
    "    orn_d = R.from_quat(orn_d)\n",
    "    \n",
    "    orn_d = orn_d.inv().as_matrix()\n",
    "    orn_q = orn_q.as_matrix()\n",
    "\n",
    "    pos_dist = np.linalg.norm(pos_q - pos_d)\n",
    "    \n",
    "    orn_dist = orn_q @ orn_d\n",
    "    orn_dist = R.from_matrix(orn_dist)\n",
    "    \n",
    "    error_mag = np.linalg.norm(orn_dist.as_rotvec(degrees=True))\n",
    "    \n",
    "#     ipdb.set_trace()\n",
    "    \n",
    "    return pos_dist, error_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b72c64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hloc.visualization import plot_matches\n",
    "from flow_control.localize.hloc_utils import get_playback, align_pointclouds\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "    \n",
    "def find_best_demo(name_q, query_cam, references):\n",
    "    results = {}\n",
    "    for name_d in tqdm(references):\n",
    "        if name_q == name_d:\n",
    "            continue\n",
    "        \n",
    "        res = align_pointclouds(root_dir, matches_path, features_path, features_seg_path,\n",
    "                                           name_q, name_d, query_cam=query_cam)\n",
    "        if res is None:\n",
    "            continue\n",
    "            \n",
    "        res['trf_est'] = res['trf_est']\n",
    "        res['num_inliers'] = int(res['num_inliers'])\n",
    "        res['num_candidates'] = int(res['num_candidates'])\n",
    "        res['in_score'] = float(res['num_candidates'])\n",
    "        \n",
    "        results[name_d] = res\n",
    "#         plot_images([read_image(hloc_root / r) for r in [name_q, name_d]], dpi=75)\n",
    "#         plot_matches(res[\"kps_q\"], res[\"kps_d\"], a=0.1)\n",
    "#         plt.show()\n",
    "\n",
    "    results = {k: v for k, v in results.items() if v is not None}\n",
    "    results_sorted = sorted(results.items(), key=lambda t: -t[1][\"num_inliers\"])\n",
    "    \n",
    "    name_d_best = results_sorted[0][0]\n",
    "    res_best = results_sorted[0][1]\n",
    "    return name_d_best, res_best, results\n",
    "\n",
    "hloc_pos_errors = []\n",
    "hloc_orn_errors = []\n",
    "\n",
    "name_q_list = []\n",
    "name_d_list = []\n",
    "\n",
    "\n",
    "for idx in range(len(references)):\n",
    "    name_q = references[idx]\n",
    "    \n",
    "    name_q_list.append(name_q)\n",
    "    pb, frame_index = get_playback(root_dir, name_q)\n",
    "    query_cam = pb[frame_index].cam\n",
    "\n",
    "    #%prun -D program.prof \n",
    "    name_d_best, res_best, results = find_best_demo(name_q, query_cam, references)\n",
    "    print(name_q, name_d_best)\n",
    "    \n",
    "    name_d_list.append(name_d_best)\n",
    "    \n",
    "    pos_err, orn_err = compute_distance(name_q, name_d_best)\n",
    "    \n",
    "    hloc_pos_errors.append(pos_err)\n",
    "    hloc_orn_errors.append(orn_err)\n",
    "    \n",
    "    trf_best = res_best[\"trf_est\"]\n",
    "\n",
    "    plot_images([read_image(hloc_root / r) for r in [name_q, name_d_best]], dpi=75)\n",
    "    plot_matches(res_best[\"kps_q\"], res_best[\"kps_d\"], a=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f605d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_d_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# res = stats.cumfreq(distances, numbins=30, defaultreallimits=(0.0, 0.15))\n",
    "# x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size, res.cumcount.size)\n",
    "# plt.plot(x * 1000, res.cumcount / len(distances))\n",
    "# plt.xlabel(\"Error (mm)\")\n",
    "# plt.ylabel(\"Samples (%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56841c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name_q in references:\n",
    "#     name_d_best, res_best = find_best_demo(name_q, references)\n",
    "#     print(name_q, name_d_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5db3aa",
   "metadata": {},
   "source": [
    "## Visual Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "recordings = sorted([root_dir / f for f in os.listdir(root_dir)])\n",
    "recordings = recordings[:-1]\n",
    "\n",
    "# Load Servoing Module\n",
    "from flow_control.servoing.module import ServoingModule\n",
    "control_config = dict(mode=\"pointcloud-abs-rotz\", threshold=0.40)\n",
    "servo_module = ServoingModule(recordings[0], control_config=control_config,\n",
    "                              start_paused=False, flow_module='RAFT')\n",
    "\n",
    "def similarity_from_reprojection(live_rgb, demo_rgb, demo_mask, return_images=False):\n",
    "    # evaluate the similarity via flow reprojection error\n",
    "    flow = servo_module.flow_module.step(demo_rgb, live_rgb)\n",
    "    warped = servo_module.flow_module.warp_image(live_rgb / 255.0, flow)\n",
    "    error = np.linalg.norm((warped - (demo_rgb / 255.0)), axis=2) * demo_mask\n",
    "    error = error.sum() / demo_mask.sum()\n",
    "    mean_flow = np.linalg.norm(flow[demo_mask],axis=1).mean()\n",
    "    if return_images:\n",
    "        return error, mean_flow, flow, warped\n",
    "    return error, mean_flow\n",
    "\n",
    "sim_scores = np.ones((len(recordings), len(recordings)))  # lower is better\n",
    "mean_flows = np.zeros((len(recordings), len(recordings)))\n",
    "\n",
    "demo_good = [True for rec in recordings]\n",
    "\n",
    "bad_pair_arr = np.zeros((len(recordings), len(recordings)), dtype=bool)\n",
    "for idx in np.where(np.array(demo_good) == False)[0]:\n",
    "    bad_pair_arr[:,idx] = True\n",
    "    bad_pair_arr[idx,:] = True\n",
    "bad_pair_arr += np.eye(len(recordings), len(recordings), dtype=bool)\n",
    "\n",
    "good_pairs = list(zip(*np.where(bad_pair_arr==False)))\n",
    "\n",
    "for live_i, demo_i in tqdm(good_pairs):\n",
    "    live_rgb = read_image(hloc_root / references[live_i])\n",
    "\n",
    "    demo_rgb =  read_image(hloc_root / references[demo_i])\n",
    "    demo_mask = get_segmentation(root_dir, references[demo_i])\n",
    "    \n",
    "    error, mean_flow = similarity_from_reprojection(live_rgb.copy(), demo_rgb.copy(), demo_mask.copy())\n",
    "    assert error <= 1.0\n",
    "    sim_scores[demo_i, live_i] = error\n",
    "    mean_flows[demo_i, live_i] = mean_flow\n",
    "\n",
    "\n",
    "sim_l = sim_scores[demo_good]\n",
    "mean_flows_l = mean_flows[demo_good]\n",
    "\n",
    "sim_scores_norm = np.ones(sim_scores.shape)\n",
    "w = .5\n",
    "sim_scores_norm[demo_good] = np.mean((1*minmax_scale(sim_l), w*minmax_scale(mean_flows_l)),axis=0)/(1+w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_pos_errors = []\n",
    "vs_orn_errors = []\n",
    "\n",
    "best_demo_idx = np.argmin(sim_scores_norm, axis=0)\n",
    "\n",
    "for idx in range(len(recordings)):\n",
    "    name_q = references[idx]\n",
    "    name_d_best = references[best_demo_idx[idx]]\n",
    "    \n",
    "    pos_err, orn_err = compute_distance(name_q, name_d_best)\n",
    "    vs_pos_errors.append(pos_err)\n",
    "    vs_orn_errors.append(orn_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e8541",
   "metadata": {},
   "source": [
    "## R3M "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f013d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from r3m import load_r3m\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import ipdb\n",
    "\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "def get_r3m_embeddings(playbacks, transform=None, device='cuda', masked=False):\n",
    "    embeddings = []\n",
    "\n",
    "    r3m = load_r3m(\"resnet50\")\n",
    "    r3m.eval()\n",
    "    r3m.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for pb in playbacks:\n",
    "            im = pb[18].cam.get_image()[0]\n",
    "            \n",
    "            if masked:\n",
    "                mask = pb.get_fg_mask()\n",
    "                mask = mask[..., np.newaxis].repeat(3, axis=2)\n",
    "                im = im * mask\n",
    "                \n",
    "            im = transform(im)\n",
    "\n",
    "            im = im.unsqueeze(0).cuda()\n",
    "            embeddings.append(r3m(im * 255.0))\n",
    "        \n",
    "        embeddings = torch.cat(embeddings)\n",
    "    \n",
    "    embeddings = embeddings.detach().cpu().numpy()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "playbacks = [PlaybackEnvServo(rec, load='keep') for rec in recordings[:]]\n",
    "\n",
    "embeddings_with_mask = get_r3m_embeddings(playbacks, transform=transform, device='cuda', masked=True)\n",
    "embeddings_without_mask = get_r3m_embeddings(playbacks, transform=transform, device='cuda', masked=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ab661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "r3m_sim_scores_masked = np.ones(bad_pair_arr.shape) * 10.0  # lower is better\n",
    "r3m_sim_scores_no_mask = np.ones(bad_pair_arr.shape) * 10.0\n",
    "\n",
    "for live_i, demo_i in tqdm(good_pairs):\n",
    "    live_embedding = embeddings_without_mask[live_i, :] \n",
    "    \n",
    "    demo_embedding = embeddings_without_mask[demo_i, :]\n",
    "    demo_embedding_masked = embeddings_with_mask[demo_i, :]\n",
    "    \n",
    "    error_masked = np.linalg.norm(demo_embedding_masked - live_embedding)\n",
    "    error_no_mask = np.linalg.norm(demo_embedding - live_embedding)\n",
    "\n",
    "    r3m_sim_scores_masked[demo_i, live_i] = error_masked    \n",
    "    r3m_sim_scores_no_mask[demo_i, live_i] = error_no_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r3m_masked_pos_errors = []\n",
    "r3m_masked_orn_errors = []\n",
    "\n",
    "r3m_unmasked_pos_errors = []\n",
    "r3m_unmasked_orn_errors = []\n",
    "\n",
    "best_demo_idx_masked = np.argmin(r3m_sim_scores_masked, axis=0)\n",
    "best_demo_idx_no_mask = np.argmin(r3m_sim_scores_no_mask, axis=0)\n",
    "\n",
    "for idx in range(len(recordings)):\n",
    "    name_q = references[idx]\n",
    "    \n",
    "    name_d_best_masked = references[best_demo_idx_masked[idx]]\n",
    "    name_d_best_no_mask = references[best_demo_idx_no_mask[idx]]\n",
    "    \n",
    "    pos_err_m, orn_err_m = compute_distance(name_q, name_d_best_masked)\n",
    "    pos_err_nm, orn_err_nm = compute_distance(name_q, name_d_best_no_mask)\n",
    "    \n",
    "    r3m_masked_pos_errors.append(pos_dist_m)\n",
    "    r3m_masked_orn_errors.append(orn_dist_m)\n",
    "    \n",
    "    r3m_unmasked_pos_errors.append(pos_dist_nm)\n",
    "    r3m_unmasked_orn_errors.append(orn_dist_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4860c31",
   "metadata": {},
   "source": [
    "## Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641597ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orientation Errors\n",
    "from scipy import stats\n",
    "\n",
    "max_orn_error = max(np.max(vs_orn_errors), np.max(hloc_orn_err), \n",
    "                    np.max(r3m_masked_orn_errors), np.max(r3m_unmasked_orn_errors))\n",
    "\n",
    "res = stats.cumfreq(vs_orn_errors, numbins=30, defaultreallimits=(0.0, max_orn_error))\n",
    "x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size, res.cumcount.size)\n",
    "plt.plot(x, res.cumcount / len(vs_orn_errors), label='VS')\n",
    "\n",
    "res = stats.cumfreq(hloc_orn_err, numbins=30, defaultreallimits=(0.0, max_orn_error))\n",
    "plt.plot(x, res.cumcount / len(hloc_orn_err), label='HLOC')\n",
    "\n",
    "res = stats.cumfreq(r3m_masked_orn_errors, numbins=30, defaultreallimits=(0.0, max_orn_error))\n",
    "plt.plot(x, res.cumcount / len(r3m_masked_orn_errors), label='R3M_masked')\n",
    "\n",
    "res = stats.cumfreq(r3m_unmasked_orn_errors, numbins=30, defaultreallimits=(0.0, max_orn_error))\n",
    "plt.plot(x, res.cumcount / len(r3m_unmasked_orn_errors), label='R3M_no_mask')\n",
    "\n",
    "plt.xlabel(\"Error (Mag)\")\n",
    "plt.ylabel(\"Samples (%)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "max_dist_error = max(np.max(vs_pos_errors), np.max(hloc_pos_errors), \n",
    "                    np.max(r3m_masked_pos_errors), np.max(r3m_unmasked_pos_errors))\n",
    "\n",
    "res = stats.cumfreq(vs_pos_errors, numbins=30, defaultreallimits=(0.0, max_dist_error))\n",
    "x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size, res.cumcount.size)\n",
    "plt.plot(x * 1000, res.cumcount / len(vs_pos_errors), label='VS')\n",
    "\n",
    "res = stats.cumfreq(hloc_pos_errors, numbins=30, defaultreallimits=(0.0, max_dist_error))\n",
    "plt.plot(x * 1000, res.cumcount / len(hloc_pos_errors), label='HLOC')\n",
    "\n",
    "res = stats.cumfreq(r3m_masked_pos_errors, numbins=30, defaultreallimits=(0.0, max_dist_error))\n",
    "plt.plot(x * 1000, res.cumcount / len(r3m_masked_pos_errors), label='R3M_masked')\n",
    "\n",
    "res = stats.cumfreq(r3m_unmasked_pos_errors, numbins=30, defaultreallimits=(0.0, max_dist_error))\n",
    "plt.plot(x * 1000, res.cumcount / len(r3m_unmasked_pos_errors), label='R3M_no_mask')\n",
    "\n",
    "plt.xlabel(\"Error (mm)\")\n",
    "plt.ylabel(\"Samples (%)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from flow_control.localize.hloc_utils import get_pointcloud, get_segmented_pointcloud\n",
    "\n",
    "def draw_registration_result(source_arr, target_arr, transformation, color=\"rgb\"):\n",
    "    source = o3d.geometry.PointCloud()\n",
    "    source.points = o3d.utility.Vector3dVector(source_arr[:, :3])\n",
    "    target = o3d.geometry.PointCloud()\n",
    "    target.points = o3d.utility.Vector3dVector(target_arr[:, :3])\n",
    "    \n",
    "    if color == \"rgb\":\n",
    "        source.colors = o3d.utility.Vector3dVector(source_arr[:, 4:7] )\n",
    "        target.colors = o3d.utility.Vector3dVector(target_arr[:, 4:7] )\n",
    "        source_temp = copy.deepcopy(source)\n",
    "        target_temp = copy.deepcopy(target)\n",
    "    else:\n",
    "        source_temp = copy.deepcopy(source)\n",
    "        target_temp = copy.deepcopy(target)\n",
    "        source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "        target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "        \n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "    return source_temp, target_temp\n",
    "    \n",
    "pc_seg_q, bbox = get_segmented_pointcloud(name_q, root_dir=root_dir)\n",
    "pc_seg_d, bbox = get_segmented_pointcloud(name_d_best, root_dir=root_dir)\n",
    "pcd_q, pcd_d = draw_registration_result(pc_seg_q, pc_seg_d, trf_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00445e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the ICP registration error (does not include color)\n",
    "pcd_q = o3d.t.geometry.PointCloud(pc_seg_q[:, 0:3])\n",
    "pcd_d = o3d.t.geometry.PointCloud(pc_seg_d[:, 0:3])\n",
    "treg = o3d.t.pipelines.registration\n",
    "max_correspondence_distance = 0.02\n",
    "evaluation = treg.evaluate_registration(pcd_q, pcd_d,\n",
    "                                        max_correspondence_distance, trf_best)\n",
    "print(\"Fitness: \", evaluation.fitness)\n",
    "print(\"Inlier RMSE: \", evaluation.inlier_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50af168",
   "metadata": {},
   "source": [
    "## Warp Demo Segmentation to Live\n",
    "\n",
    "We don't have segmentation for the live views. In the case of trying to get a segmented pointcloud for the live view we could try using an estimated segmentation map which is warped from the demo frame. \n",
    "\n",
    "1. Transform Demo Pointcloud to get 'live' pointcloud \n",
    "2. Project pointcloud to get live segmentation mask\n",
    "3. Create a 'live' pointcloud using live rgb and depth images, along with masked points\n",
    "4. Computer ICP Metrics between demo pointcloud and the live pointcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15917c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_control.rgbd_camera import RGBDCamera\n",
    "\n",
    "debug_plot = False\n",
    "\n",
    "# Demo pointcloud\n",
    "pc_seg_d, _ = get_segmented_pointcloud(name_d_best, root_dir=root_dir)\n",
    "trf_demo_to_live = np.linalg.inv(trf_best)\n",
    "pc_d = pc_seg_d[:, 0:4]\n",
    "pc_live_transformed = trf_demo_to_live @ pc_d.T\n",
    "\n",
    "pts0, pts1 = query_cam.project(pc_live_transformed)\n",
    "masked_points = np.array([(x, y) for (x, y) in zip(pts1, pts0)])\n",
    "\n",
    "if debug_plot:\n",
    "    seg_q = get_segmentation(root_dir, name_q)\n",
    "    seg_d = get_segmentation(root_dir, name_d_best)\n",
    "    seg_w = np.zeros_like(seg_q)\n",
    "    seg_w[pts1, pts0] = True\n",
    "    tmp = np.stack([seg_q, seg_d, seg_w], axis=2)*255\n",
    "    plot_images([tmp,])\n",
    "    print(\"red: live, green: demo, blue: warped\")\n",
    "    \n",
    "def to_o3d_pc(arr):\n",
    "    pc = o3d.geometry.PointCloud()\n",
    "    pc.points = o3d.utility.Vector3dVector(arr[:, :3])\n",
    "    pc.colors = o3d.utility.Vector3dVector(arr[:, 4:7]/255.)\n",
    "    return pc\n",
    "\n",
    "rgb_image, depth_image = query_cam.get_image()\n",
    "rgbd_cam = RGBDCamera(query_cam)\n",
    "pc_live = rgbd_cam.generate_pointcloud(rgb_image, depth_image, masked_points)\n",
    "#o3d.visualization.draw_geometries([to_o3d_pc(pc_live), to_o3d_pc(pc_seg_d)])\n",
    "\n",
    "# compute ICP metrics\n",
    "pcd_q = o3d.t.geometry.PointCloud(pc_live[:, 0:3])\n",
    "pcd_d = o3d.t.geometry.PointCloud(pc_seg_d[:, 0:3])\n",
    "treg = o3d.t.pipelines.registration\n",
    "max_correspondence_distance = 0.02\n",
    "evaluation = treg.evaluate_registration(pcd_q, pcd_d,\n",
    "                                        max_correspondence_distance, trf_best)\n",
    "print(\"Fitness: \", evaluation.fitness)\n",
    "print(\"Inlier RMSE: \", evaluation.inlier_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf09a498",
   "metadata": {},
   "source": [
    "# Localization (Live Inferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48621b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from PIL import Image\n",
    "def create_query_image(query_cam):\n",
    "    query_dir = hloc_root / \"query\"\n",
    "    Path(query_dir).mkdir(parents=True, exist_ok=True)\n",
    "    image_path_query = query_dir / \"live.jpg\"\n",
    "    image_arr = query_cam.get_image()[0]\n",
    "    Image.fromarray(image_arr).save(image_path_query)\n",
    "    return image_path_query.relative_to(hloc_root).as_posix()\n",
    "\n",
    "name_q = references[0]\n",
    "pb, frame_index = get_playback(root_dir, name_q)\n",
    "query_cam = pb[frame_index].cam\n",
    "query = create_query_image(query_cam)\n",
    "\n",
    "references_live = [x for x in references if x != name_q]\n",
    "extract_features.main(feature_conf, hloc_root, image_list=[query], feature_path=features_path, overwrite=True)\n",
    "pairs_from_exhaustive.main(loc_pairs, image_list=[query], ref_list=references_live)\n",
    "match_features.main(matcher_conf, loc_pairs, features=features_path, matches=matches_path, overwrite=True)\n",
    "name_d_best_live, res_best_live = find_best_demo(query, qery_cam, references_live)\n",
    "\n",
    "print(name_q, name_d_best_live)\n",
    "plot_images([read_image(hloc_root / r) for r in [name_q, name_d_best_live]], dpi=75)\n",
    "plot_matches(res_best_live[\"kps_q\"], res_best_live[\"kps_d\"], a=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_control.localize.hloc_utils import get_playback\n",
    "\n",
    "name_q = selection_hloc.parts_references['locate'][0]\n",
    "pb, frame_index = get_playback(root_dir, name_q)\n",
    "query_cam = pb[frame_index].cam\n",
    "name_best, res_best = selection_hloc.get_best_demo(query_cam)\n",
    "print(name_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09420f0d",
   "metadata": {},
   "source": [
    "# Original File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fbdb5c",
   "metadata": {},
   "source": [
    "In this notebook, we will build a 3D map of a scene from a small set of images and then localize an image downloaded from the Internet. This demo was contributed by [Philipp Lindenberger](https://github.com/Phil26AT/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e663b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tqdm, tqdm.notebook\n",
    "tqdm.tqdm = tqdm.notebook.tqdm  # notebook-friendly progress bars\n",
    "from pathlib import Path\n",
    "\n",
    "from hloc import extract_features, match_features, reconstruction, visualization, pairs_from_exhaustive\n",
    "from hloc.visualization import plot_images, read_image, plot_keypoints\n",
    "from hloc.utils import viz_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ac394",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Here we define some output paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = Path('datasets/sacre_coeur')\n",
    "outputs = Path('outputs/demo/')\n",
    "!rm -rf $outputs\n",
    "sfm_pairs = outputs / 'pairs-sfm.txt'\n",
    "loc_pairs = outputs / 'pairs-loc.txt'\n",
    "sfm_dir = outputs / 'sfm'\n",
    "features = outputs / 'features.h5'\n",
    "matches = outputs / 'matches.h5'\n",
    "\n",
    "feature_conf = extract_features.confs['superpoint_aachen']\n",
    "matcher_conf = match_features.confs['superglue']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7b21e",
   "metadata": {},
   "source": [
    "# 3D mapping\n",
    "First we list the images used for mapping. These are all day-time shots of Sacre Coeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [p.relative_to(images).as_posix() for p in (images / 'mapping/').iterdir()]\n",
    "print(len(references), \"mapping images\")\n",
    "plot_images([read_image(images / r) for r in references[:4]], dpi=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23739ad",
   "metadata": {},
   "source": [
    "Then we extract features and match them across image pairs. Since we deal with few images, we simply match all pairs exhaustively. For larger scenes, we would use image retrieval, as demonstrated in the other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features.main(feature_conf, images, image_list=references, feature_path=features)\n",
    "pairs_from_exhaustive.main(sfm_pairs, image_list=references)\n",
    "match_features.main(matcher_conf, sfm_pairs, features=features, matches=matches);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9adf4",
   "metadata": {},
   "source": [
    "The we run incremental Structure-From-Motion and display the reconstructed 3D model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52fe785",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = reconstruction.main(sfm_dir, images, sfm_pairs, features, matches, image_list=references)\n",
    "fig = viz_3d.init_figure()\n",
    "viz_3d.plot_reconstruction(fig, model, color='rgba(255,0,0,0.5)', name=\"mapping\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5478094d",
   "metadata": {},
   "source": [
    "We also visualize which keypoints were triangulated into the 3D model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.visualize_sfm_2d(model, images, color_by='visibility', n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b08268",
   "metadata": {},
   "source": [
    "# Localization\n",
    "Now that we have a 3D map of the scene, we can localize any image. To demonstrate this, we download [a night-time image from Wikimedia](https://commons.wikimedia.org/wiki/File:Paris_-_Basilique_du_Sacr%C3%A9_Coeur,_Montmartre_-_panoramio.jpg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f07f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://upload.wikimedia.org/wikipedia/commons/5/53/Paris_-_Basilique_du_Sacr%C3%A9_Coeur%2C_Montmartre_-_panoramio.jpg\"\n",
    "# try other queries by uncommenting their url\n",
    "# url = \"https://upload.wikimedia.org/wikipedia/commons/5/59/Basilique_du_Sacr%C3%A9-C%C5%93ur_%285430392880%29.jpg\"\n",
    "# url = \"https://upload.wikimedia.org/wikipedia/commons/8/8e/Sacr%C3%A9_C%C5%93ur_at_night%21_%285865355326%29.jpg\"\n",
    "query = 'query/night.jpg'\n",
    "!mkdir -p $images/query && wget $url -O $images/$query -q\n",
    "plot_images([read_image(images / query)], dpi=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a035ca4",
   "metadata": {},
   "source": [
    "Again, we extract features for the query and match them exhaustively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features.main(feature_conf, images, image_list=[query], feature_path=features, overwrite=True)\n",
    "pairs_from_exhaustive.main(loc_pairs, image_list=[query], ref_list=references)\n",
    "match_features.main(matcher_conf, loc_pairs, features=features, matches=matches, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b037419",
   "metadata": {},
   "source": [
    "We read the EXIF data of the query to infer a rough initial estimate of camera parameters like the focal length. Then we estimate the absolute camera pose using PnP+RANSAC and refine the camera parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd559ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycolmap\n",
    "from hloc.localize_sfm import QueryLocalizer, pose_from_cluster\n",
    "\n",
    "camera = pycolmap.infer_camera_from_image(images / query)\n",
    "ref_ids = [model.find_image_with_name(r).image_id for r in references]\n",
    "conf = {\n",
    "    'estimation': {'ransac': {'max_error': 12}},\n",
    "    'refinement': {'refine_focal_length': True, 'refine_extra_params': True},\n",
    "}\n",
    "localizer = QueryLocalizer(model, conf)\n",
    "ret, log = pose_from_cluster(localizer, query, camera, ref_ids, features, matches)\n",
    "\n",
    "print(f'found {ret[\"num_inliers\"]}/{len(ret[\"inliers\"])} inlier correspondences.')\n",
    "visualization.visualize_loc_from_log(images, query, log, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e5518",
   "metadata": {},
   "source": [
    "We visualize the correspondences between the query images a few mapping images. We can also visualize the estimated camera pose in the 3D map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab5306",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = pycolmap.Image(tvec=ret['tvec'], qvec=ret['qvec'])\n",
    "viz_3d.plot_camera_colmap(fig, pose, camera, color='rgba(0,255,0,0.5)', name=query)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
