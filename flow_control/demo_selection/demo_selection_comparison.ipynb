{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50eb4b93",
   "metadata": {},
   "source": [
    "# Comparing different methods of Demonstration Selection\n",
    "\n",
    "In this notebook, we compare different methods that can be used for demonstration selection.\n",
    "We compare Hierarchical Localization, Visual Similarity, R3M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f54f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm  # notebook-friendly progress bars\n",
    "\n",
    "from hloc import extract_features, match_features, reconstruction, visualization, pairs_from_exhaustive\n",
    "from hloc.visualization import plot_images, read_image, plot_keypoints\n",
    "from hloc.utils import viz_3d\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39568d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_control.demo.playback_env_servo import PlaybackEnvServo\n",
    "from flow_control.localize.hloc_utils import export_images_by_parts\n",
    "\n",
    "# root_dir = Path(\"/home/argusm/CLUSTER/robot_recordings/flow/recombination/2023-01-24\")\n",
    "root_dir = Path(\"/home/argusm/Desktop/Demonstrations/2023-01-24\")\n",
    "# root_dir = Path(\"/home/nayakab/Desktop/Courses/WS2022/Project/cluster/\")\n",
    "parts_fn = root_dir / 'parts.json'\n",
    "hloc_root = root_dir.parent / ( str(root_dir.name) + '_hloc')\n",
    "\n",
    "mapping_dir = hloc_root / 'mapping'\n",
    "outputs = hloc_root / 'outputs'\n",
    "sfm_pairs = outputs / 'pairs-sfm.txt'\n",
    "loc_pairs = outputs / 'pairs-loc.txt'\n",
    "sfm_dir = outputs / 'sfm'\n",
    "features_path = outputs / 'features.h5'\n",
    "matches_path = outputs / 'matches.h5'\n",
    "features_seg_path = outputs / 'features_seg.h5'\n",
    "\n",
    "if parts_fn.is_file():\n",
    "    with open(parts_fn) as f_obj:\n",
    "        parts_references = json.load(f_obj)\n",
    "        print(\"Succesfully loaded parts. --> Skip to \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db439a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_control.localize.hloc_utils import to_hloc_ref\n",
    "with open(parts_fn) as f_obj:\n",
    "    tmp = json.load(f_obj)\n",
    "\n",
    "parts_references = {}\n",
    "parts_references['locate'] = [to_hloc_ref(k,v['locate'][0]) for k,v in tmp.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $outputs\n",
    "!rm -rf $mapping_dir\n",
    "parts_references = export_images_by_parts(root_dir, parts_fn, mapping_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517be8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "references_all = [ref for ref_part in parts_references.values() for ref in ref_part]\n",
    "references_files = sorted([p.relative_to(hloc_root).as_posix() for p in (hloc_root / 'mapping/').iterdir()])\n",
    "assert len(set(references_all)-set(references_files)) == 0\n",
    "references = parts_references['locate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c0e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(references), \"mapping images\")\n",
    "plot_images([read_image(hloc_root / r) for r in references[:4]], dpi=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e468e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_control.localize.hloc_utils import save_features_seg\n",
    "\n",
    "\n",
    "feature_conf = extract_features.confs['superpoint_aachen']\n",
    "matcher_conf = match_features.confs['superglue']\n",
    "\n",
    "extract_features.main(feature_conf, hloc_root, image_list=references_all, feature_path=features_path)\n",
    "save_features_seg(root_dir, features_seg_path, features_path, references_all)\n",
    "\n",
    "pairs_from_exhaustive.main(sfm_pairs, image_list=references)\n",
    "match_features.main(matcher_conf, sfm_pairs, features=features_path, matches=matches_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7c318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hloc.utils.io import get_keypoints\n",
    "\n",
    "num_images = 4\n",
    "plot_images([read_image(hloc_root / r) for r in references[:num_images]], dpi=75)\n",
    "plot_keypoints([get_keypoints(features_path, r) for r in references[:num_images]], colors='lime', ps=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c5f1ac",
   "metadata": {},
   "source": [
    "## Load Match Database\n",
    "\n",
    "hloc saves all features and matches in a SQL database, so reading these is the easiest option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b06f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hloc.utils.io import get_keypoints\n",
    "from flow_control.localize.hloc_utils import get_segmentation\n",
    "\n",
    "name0 = references[1]\n",
    "kps0, noise0 = get_keypoints(features_path, name0, return_uncertainty=True)\n",
    "kps0_seg, noise0 = get_keypoints(features_seg_path, name0, return_uncertainty=True)\n",
    "seg = get_segmentation(root_dir, name0)\n",
    "\n",
    "plot_images([read_image(hloc_root / r) for r in [name0, ]]+[seg], dpi=75)\n",
    "plot_keypoints([kps0, kps0_seg], colors='lime', ps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e241235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hloc.utils.io import get_matches\n",
    "from flow_control.localize.hloc_utils import kp_seg_filter\n",
    "\n",
    "name_q = references[1]\n",
    "name_d = references[3]\n",
    "\n",
    "matches, scores = get_matches(matches_path, name_q, name_d)\n",
    "kps_q, noise_q = get_keypoints(features_path, name_q, return_uncertainty=True)\n",
    "kps_d, noise_d = get_keypoints(features_path, name_d, return_uncertainty=True)\n",
    "kps_q_match = kps_q[matches[:, 0]]\n",
    "kps_d_match = kps_d[matches[:, 1]]\n",
    "\n",
    "#%prun in_seg = kp_seg_filter_pb(kps_d_match, name_d)\n",
    "in_seg = kp_seg_filter(kps_d_match, name_d, features_seg_path)\n",
    "\n",
    "print(\"in_seg\", in_seg)\n",
    "print(kps_d_match[in_seg].shape)\n",
    "\n",
    "kps_q_seg = kps_q_match[in_seg]\n",
    "kps_d_seg = kps_d_match[in_seg]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b6e94",
   "metadata": {},
   "source": [
    "## Get ground truth positions and orientations\n",
    "\n",
    "Use TCP position and orientation at the point of grasp to get actual object positions and orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74fc0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "with open(parts_fn, 'r') as f_obj:\n",
    "    part_info = json.load(f_obj)\n",
    "\n",
    "def get_tcp_position_orn(demo_dir, frame_index):\n",
    "    arr = np.load(os.path.join(demo_dir, f\"frame_{frame_index:06d}.npz\"),allow_pickle=True)\n",
    "    state = arr[\"robot_state\"].item()\n",
    "    return state[\"tcp_pos\"], state['tcp_orn']\n",
    "\n",
    "def get_tcp_position_at_grasp(name_q):\n",
    "    tmp = name_q.strip().split('/')[1]\n",
    "    rec_name = tmp.split('_')[0]\n",
    "    \n",
    "    rec_path = os.path.join(root_dir, rec_name)\n",
    "    gripper_close_idx = part_info[rec_name]['insert'][0]\n",
    "    \n",
    "    pos, orn = get_tcp_position_orn(rec_path, gripper_close_idx)\n",
    "    return pos, orn\n",
    "\n",
    "def compute_distance(name_q, name_d):\n",
    "    pos_q, orn_q = get_tcp_position_at_grasp(name_q)\n",
    "    pos_d, orn_d = get_tcp_position_at_grasp(name_d)\n",
    "    \n",
    "    orn_q = R.from_quat(orn_q)\n",
    "    orn_d = R.from_quat(orn_d)\n",
    "    \n",
    "    orn_d = orn_d.inv().as_matrix()\n",
    "    orn_q = orn_q.as_matrix()\n",
    "\n",
    "    pos_err = np.linalg.norm(pos_q - pos_d)\n",
    "    \n",
    "    orn_dist = orn_q @ orn_d\n",
    "    orn_dist = R.from_matrix(orn_dist)\n",
    "    \n",
    "    orn_err = np.linalg.norm(orn_dist.as_rotvec(degrees=True))\n",
    "    \n",
    "    return pos_err, orn_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312cc28",
   "metadata": {},
   "source": [
    "## HLOC Errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b72c64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hloc.visualization import plot_matches\n",
    "from flow_control.localize.hloc_utils import get_playback, align_pointclouds\n",
    "\n",
    "    \n",
    "def find_best_demo(name_q, query_cam, references):\n",
    "    results = {}\n",
    "    for name_d in tqdm(references):\n",
    "        if name_q == name_d:\n",
    "            continue\n",
    "        \n",
    "        res = align_pointclouds(root_dir, matches_path, features_path, features_seg_path,\n",
    "                                           name_q, name_d, query_cam=query_cam)\n",
    "        if res is None:\n",
    "            continue\n",
    "            \n",
    "        res['trf_est'] = res['trf_est']\n",
    "        res['num_inliers'] = int(res['num_inliers'])\n",
    "        res['num_candidates'] = int(res['num_candidates'])\n",
    "        res['in_score'] = float(res['num_candidates'])\n",
    "        \n",
    "        results[name_d] = res\n",
    "#         plot_images([read_image(hloc_root / r) for r in [name_q, name_d]], dpi=75)\n",
    "#         plot_matches(res[\"kps_q\"], res[\"kps_d\"], a=0.1)\n",
    "#         plt.show()\n",
    "\n",
    "    results = {k: v for k, v in results.items() if v is not None}\n",
    "    results_sorted = sorted(results.items(), key=lambda t: -t[1][\"num_inliers\"])\n",
    "    \n",
    "    name_d_best = results_sorted[0][0]\n",
    "    res_best = results_sorted[0][1]\n",
    "    return name_d_best, res_best, results\n",
    "\n",
    "hloc_pos_errors = []\n",
    "hloc_orn_errors = []\n",
    "\n",
    "\n",
    "for idx in range(len(references)):\n",
    "    name_q = references[idx]\n",
    "    pb, frame_index = get_playback(root_dir, name_q)\n",
    "    query_cam = pb[frame_index].cam\n",
    "\n",
    "    #%prun -D program.prof \n",
    "    name_d_best, res_best, results = find_best_demo(name_q, query_cam, references)\n",
    "    \n",
    "    pos_err, orn_err = compute_distance(name_q, name_d_best)\n",
    "    \n",
    "    hloc_pos_errors.append(pos_err)\n",
    "    hloc_orn_errors.append(orn_err)\n",
    "\n",
    "    # plot_images([read_image(hloc_root / r) for r in [name_q, name_d_best]], dpi=75)\n",
    "    # plot_matches(res_best[\"kps_q\"], res_best[\"kps_d\"], a=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5db3aa",
   "metadata": {},
   "source": [
    "## Visual Similarity Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "recordings = sorted([root_dir / f for f in os.listdir(root_dir)])\n",
    "recordings = recordings[:-1]\n",
    "\n",
    "playbacks = [PlaybackEnvServo(rec, load='keep') for rec in recordings[:]]\n",
    "\n",
    "# Load Servoing Module\n",
    "from flow_control.servoing.module import ServoingModule\n",
    "control_config = dict(mode=\"pointcloud-abs-rotz\", threshold=0.40)\n",
    "servo_module = ServoingModule(recordings[0], control_config=control_config,\n",
    "                              start_paused=False, flow_module='RAFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ac3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_good = [True for rec in recordings]\n",
    "\n",
    "bad_pair_arr = np.zeros((len(recordings), len(recordings)), dtype=bool)\n",
    "for idx in np.where(np.array(demo_good) == False)[0]:\n",
    "    bad_pair_arr[:,idx] = True\n",
    "    bad_pair_arr[idx,:] = True\n",
    "bad_pair_arr += np.eye(len(recordings), len(recordings), dtype=bool)\n",
    "\n",
    "good_pairs = list(zip(*np.where(bad_pair_arr==False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-projection Error\n",
    "\n",
    "# def similarity_from_reprojection(live_rgb, demo_rgb, demo_mask, return_images=False):\n",
    "#     # evaluate the similarity via flow reprojection error\n",
    "#     flow = servo_module.flow_module.step(demo_rgb, live_rgb)\n",
    "#     warped = servo_module.flow_module.warp_image(live_rgb / 255.0, flow)\n",
    "#     error = np.linalg.norm((warped - (demo_rgb / 255.0)), axis=2) * demo_mask\n",
    "#     error = error.sum() / demo_mask.sum()\n",
    "#     mean_flow = np.linalg.norm(flow[demo_mask],axis=1).mean()\n",
    "#     if return_images:\n",
    "#         return error, mean_flow, flow, warped\n",
    "#     return error, mean_flow\n",
    "\n",
    "# sim_scores = np.ones((len(recordings), len(recordings)))  # lower is better\n",
    "# mean_flows = np.zeros((len(recordings), len(recordings)))\n",
    "\n",
    "# for live_i, demo_i in tqdm(good_pairs):\n",
    "#     live_rgb = read_image(hloc_root / references[live_i])\n",
    "\n",
    "#     demo_rgb =  read_image(hloc_root / references[demo_i])\n",
    "#     demo_mask = get_segmentation(root_dir, references[demo_i])\n",
    "    \n",
    "#     error, mean_flow = similarity_from_reprojection(live_rgb.copy(), demo_rgb.copy(), demo_mask.copy())\n",
    "#     assert error <= 1.0\n",
    "#     sim_scores[demo_i, live_i] = error\n",
    "#     mean_flows[demo_i, live_i] = mean_flow\n",
    "\n",
    "\n",
    "# sim_l = sim_scores[demo_good]\n",
    "# mean_flows_l = mean_flows[demo_good]\n",
    "\n",
    "# sim_scores_norm = np.ones(sim_scores.shape)\n",
    "# w = .5\n",
    "# sim_scores_norm[demo_good] = np.mean((1*minmax_scale(sim_l), w*minmax_scale(mean_flows_l)),axis=0)/(1+w)\n",
    "\n",
    "# np.savez(\"vs_scores_norm.npz\", sim_scores_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46621e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_scores_norm = np.load('vs_scores_norm.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_pos_errors = []\n",
    "vs_orn_errors = []\n",
    "\n",
    "best_demo_idx = np.argmin(sim_scores_norm, axis=0)\n",
    "\n",
    "for idx in range(len(recordings)):\n",
    "    name_q = references[idx]\n",
    "    name_d_best = references[best_demo_idx[idx]]\n",
    "    \n",
    "    pos_err, orn_err = compute_distance(name_q, name_d_best)\n",
    "    vs_pos_errors.append(pos_err)\n",
    "    vs_orn_errors.append(orn_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e8541",
   "metadata": {},
   "source": [
    "## R3M Errors, Masked and unmasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f013d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from r3m import load_r3m\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import ipdb\n",
    "\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "def get_r3m_embeddings(playbacks, transform=None, device='cuda', masked=False):\n",
    "    embeddings = []\n",
    "\n",
    "    r3m = load_r3m(\"resnet50\")\n",
    "    r3m.eval()\n",
    "    r3m.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for pb in playbacks:\n",
    "            im = pb[18].cam.get_image()[0]\n",
    "            \n",
    "            if masked:\n",
    "                mask = pb.get_fg_mask()\n",
    "                mask = mask[..., np.newaxis].repeat(3, axis=2)\n",
    "                im = im * mask\n",
    "                \n",
    "            im = transform(im)\n",
    "\n",
    "            im = im.unsqueeze(0).cuda()\n",
    "            embeddings.append(r3m(im * 255.0))\n",
    "        \n",
    "        embeddings = torch.cat(embeddings)\n",
    "    \n",
    "    embeddings = embeddings.detach().cpu().numpy()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "embeddings_with_mask = get_r3m_embeddings(playbacks, transform=transform, device='cuda', masked=True)\n",
    "embeddings_without_mask = get_r3m_embeddings(playbacks, transform=transform, device='cuda', masked=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ab661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "r3m_sim_scores_masked = np.ones(bad_pair_arr.shape) * 10.0\n",
    "r3m_sim_scores_no_mask = np.ones(bad_pair_arr.shape) * 10.0\n",
    "\n",
    "for live_i, demo_i in tqdm(good_pairs):\n",
    "    live_embedding = embeddings_without_mask[live_i, :] \n",
    "    \n",
    "    demo_embedding = embeddings_without_mask[demo_i, :]\n",
    "    demo_embedding_masked = embeddings_with_mask[demo_i, :]\n",
    "\n",
    "    error_masked = np.linalg.norm(demo_embedding_masked - live_embedding)\n",
    "    error_no_mask = np.linalg.norm(demo_embedding - live_embedding)\n",
    "\n",
    "    r3m_sim_scores_masked[demo_i, live_i] = error_masked    \n",
    "    r3m_sim_scores_no_mask[demo_i, live_i] = error_no_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r3m_masked_pos_errors = []\n",
    "r3m_masked_orn_errors = []\n",
    "\n",
    "r3m_unmasked_pos_errors = []\n",
    "r3m_unmasked_orn_errors = []\n",
    "\n",
    "best_demo_idx_masked = np.argmin(r3m_sim_scores_masked, axis=0)\n",
    "best_demo_idx_no_mask = np.argmin(r3m_sim_scores_no_mask, axis=0)\n",
    "\n",
    "for idx in range(len(recordings)):\n",
    "    name_q = references[idx]\n",
    "    \n",
    "    name_d_best_masked = references[best_demo_idx_masked[idx]]\n",
    "    name_d_best_no_mask = references[best_demo_idx_no_mask[idx]]\n",
    "    \n",
    "    pos_err_m, orn_err_m = compute_distance(name_q, name_d_best_masked)\n",
    "    pos_err_nm, orn_err_nm = compute_distance(name_q, name_d_best_no_mask)\n",
    "    \n",
    "    r3m_masked_pos_errors.append(pos_err_m)\n",
    "    r3m_masked_orn_errors.append(orn_err_m)\n",
    "    \n",
    "    r3m_unmasked_pos_errors.append(pos_err_nm)\n",
    "    r3m_unmasked_orn_errors.append(orn_err_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9f1c9",
   "metadata": {},
   "source": [
    "## R3M features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7109a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "\n",
    "x_, y_ = 15, 20\n",
    "\n",
    "def get_r3m_features(playbacks, transform=None, masked=False):\n",
    "    embeddings = []\n",
    "    features = []\n",
    "    max_locations = []\n",
    "    \n",
    "    r3m = load_r3m(\"resnet50\")\n",
    "    r3m.cuda()\n",
    "    \n",
    "    # Update network\n",
    "    r3m = r3m.module\n",
    "    convnet = r3m.convnet\n",
    "\n",
    "    arch = list(convnet.children())\n",
    "\n",
    "    del arch[-2]  # AvgPool\n",
    "    del arch[-1]  # FC\n",
    "\n",
    "    convnet = nn.Sequential(*arch)\n",
    "    r3m.convnet = convnet\n",
    "\n",
    "    r3m = nn.DataParallel(r3m)\n",
    "    r3m.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for pb in playbacks:\n",
    "            im = pb[18].cam.get_image()[0]\n",
    "            \n",
    "            if masked:\n",
    "                mask = pb.get_fg_mask()\n",
    "                mask = np.array(mask, dtype=float)\n",
    "                \n",
    "#                 tmp_mask = resize(mask, (x_, y_))\n",
    "                tmp_mask = np.asarray(Image.fromarray(mask).resize((x_, y_)))\n",
    "                \n",
    "                max_x, max_y = np.where(tmp_mask == np.amax(tmp_mask))\n",
    "                x_max = max_x[0]\n",
    "                y_max = max_y[0]\n",
    "                \n",
    "            im = transform(im)\n",
    "\n",
    "            im = im.unsqueeze(0).cuda()\n",
    "            emb = r3m(im * 255.0)\n",
    "            feat = emb[:, :, x_max, y_max]\n",
    "            max_locations.append((x_max, y_max))\n",
    "            \n",
    "            embeddings.append(emb)\n",
    "            features.append(feat)\n",
    "        \n",
    "        features = torch.cat(features)\n",
    "        embeddings = torch.cat(embeddings)\n",
    "    \n",
    "    features = features.detach().cpu().numpy()\n",
    "    embeddings = embeddings.detach().cpu().numpy()\n",
    "    max_locations = np.stack(max_locations)\n",
    "    \n",
    "    return embeddings, features, max_locations\n",
    "\n",
    "r3m_embeddings, r3m_features, feature_loc = get_r3m_features(playbacks, transform=transform, masked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b16eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "r3m_sim_scores_features = np.ones(bad_pair_arr.shape) * 10.0\n",
    "\n",
    "for live_i, demo_i in tqdm(good_pairs):\n",
    "    live_embedding = np.mean(r3m_embeddings[live_i, ...], axis=(1, 2)) \n",
    "    \n",
    "    demo_embedding = r3m_features[demo_i, :]\n",
    "    \n",
    "    error = np.linalg.norm(demo_embedding - live_embedding)\n",
    "\n",
    "    r3m_sim_scores_features[demo_i, live_i] = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd92960",
   "metadata": {},
   "outputs": [],
   "source": [
    "r3m_feat_pos_errors = []\n",
    "r3m_feat_orn_errors = []\n",
    "\n",
    "best_demo_idx_r3m_feat = np.argmin(r3m_sim_scores_features, axis=0)\n",
    "\n",
    "for idx in range(len(recordings)):\n",
    "    name_q = references[idx]\n",
    "    \n",
    "    name_d_best = references[best_demo_idx_r3m_feat[idx]]\n",
    "    \n",
    "    pos_err, orn_err = compute_distance(name_q, name_d_best)\n",
    "    \n",
    "    r3m_feat_pos_errors.append(pos_err)\n",
    "    r3m_feat_orn_errors.append(orn_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985fb498",
   "metadata": {},
   "source": [
    "## VINN-BYOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927fc3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from torchvision import models\n",
    "# from torchvision import transforms as T\n",
    "# from torch.utils.data import DataLoader\n",
    "# from byol_pytorch.byol_pytorch import *\n",
    "# from torchsummary import summary\n",
    "\n",
    "\n",
    "# net = models.resnet50(pretrained=False)\n",
    "# load_model = '/home/argusm/lmb_abhi/VINN/vinn_byol/BYOL_70__pretrained_1.pt'\n",
    "\n",
    "# learner = BYOL(\n",
    "#     net,\n",
    "#     image_size=256, \n",
    "#     hidden_layer='avgpool')\n",
    "\n",
    "# vinn_model = learner._get_target_encoder()\n",
    "\n",
    "# # Load the state dictionary of the saved model\n",
    "# state_dict = torch.load(load_model)\n",
    "\n",
    "# # Rename the weights by removing the extra prefix\n",
    "# new_state_dict = {}\n",
    "# for key in state_dict['model_state_dict'].keys():\n",
    "#     new_key = 'net.' + key  #.replace('net.', '')\n",
    "#     new_state_dict[new_key] = state_dict['model_state_dict'][key]\n",
    "\n",
    "# # Set the state dictionary of the new model\n",
    "# vinn_model.load_state_dict(new_state_dict)\n",
    "# vinn_model.cuda()\n",
    "\n",
    "# vinn_model.eval()\n",
    "\n",
    "# for module in vinn_model.modules():\n",
    "#     if isinstance(module, nn.BatchNorm2d):\n",
    "#         module.track_running_stats = False\n",
    "\n",
    "# summary(vinn_model.cuda(), (3, 480, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f3396",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def get_vinn_byol_features(playbacks, transform=None, masked=False):\n",
    "#     features = []\n",
    "    \n",
    "#     load_model = '/home/argusm/lmb_abhi/VINN/vinn_byol/BYOL_70__pretrained_1.pt'\n",
    "#     net = models.resnet50(pretrained=False)\n",
    "#     learner = BYOL(net, image_size=256, hidden_layer='avgpool')\n",
    "\n",
    "#     vinn_model = learner._get_target_encoder()\n",
    "\n",
    "#     # Load the state dictionary of the saved model\n",
    "#     state_dict = torch.load(load_model)\n",
    "\n",
    "#     # Rename the weights by removing the extra prefix\n",
    "#     new_state_dict = {}\n",
    "#     for key in state_dict['model_state_dict'].keys():\n",
    "#         new_key = 'net.' + key  #.replace('net.', '')\n",
    "#         new_state_dict[new_key] = state_dict['model_state_dict'][key]\n",
    "\n",
    "#     # Set the state dictionary of the new model\n",
    "#     vinn_model.load_state_dict(new_state_dict)\n",
    "    \n",
    "#     vinn_model.cuda()    \n",
    "#     vinn_model.eval()\n",
    "    \n",
    "#     for module in vinn_model.modules():\n",
    "#         if isinstance(module, nn.BatchNorm2d):\n",
    "#             module.track_running_stats = False\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "        \n",
    "#         for pb in playbacks:\n",
    "#             im = pb[18].cam.get_image()[0]\n",
    "                \n",
    "#             im = transform(im)\n",
    "\n",
    "#             im = im.unsqueeze(0).cuda()\n",
    "            \n",
    "#             embedding = vinn_model(im)\n",
    "#             embedding = embedding[..., 0, 0]\n",
    "            \n",
    "#             features.append(embedding)\n",
    "            \n",
    "# #         ipdb.set_trace()\n",
    "        \n",
    "#         features = torch.cat(features)\n",
    "    \n",
    "#     features = features.detach().cpu().numpy()\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# summary(vinn_model.cuda(), (3, 480, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a66792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from byol_pytorch.byol_pytorch import *\n",
    "from torchsummary import summary\n",
    "\n",
    "transform = T.Compose([T.ToTensor(),\n",
    "                       T.Normalize(mean=torch.tensor([0.485, 0.456, 0.406]),\n",
    "                                   std=torch.tensor([0.229, 0.224, 0.225]))])\n",
    "\n",
    "def get_vinn_byol_features(playbacks, transform=None, masked=False):\n",
    "    features = []\n",
    "    \n",
    "    vinn_model = models.resnet50(pretrained=False)\n",
    "    load_model = '/home/argusm/lmb_abhi/VINN/vinn_byol/BYOL_70__pretrained_1.pt'\n",
    "\n",
    "    state_dict = torch.load(load_model)\n",
    "\n",
    "    vinn_model.load_state_dict(state_dict['model_state_dict'])\n",
    "\n",
    "    arch = list(vinn_model.children())\n",
    "\n",
    "    # del arch[-2]  # AvgPool\n",
    "    del arch[-1]  # FC\n",
    "\n",
    "    vinn_model = nn.Sequential(*arch)\n",
    "    \n",
    "    vinn_model.cuda()    \n",
    "    vinn_model.eval()\n",
    "    \n",
    "    for module in vinn_model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            module.track_running_stats = False\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for pb in playbacks:\n",
    "            im = pb[18].cam.get_image()[0]\n",
    "                \n",
    "            im = transform(im)\n",
    "\n",
    "            im = im.unsqueeze(0).cuda()\n",
    "            \n",
    "            embedding = vinn_model(im)\n",
    "            embedding = embedding[..., 0, 0]\n",
    "            \n",
    "            features.append(embedding)\n",
    "            \n",
    "#         ipdb.set_trace()\n",
    "        \n",
    "        features = torch.cat(features)\n",
    "    \n",
    "    features = features.detach().cpu().numpy()\n",
    "    \n",
    "    return features\n",
    "\n",
    "vinn_features = get_vinn_byol_features(playbacks, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3033c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vinn_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ecc8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "vinn_feat_scores = np.ones(bad_pair_arr.shape) * 10.0\n",
    "\n",
    "for live_i, demo_i in tqdm(good_pairs):\n",
    "    live_feat = vinn_features[live_i, :] \n",
    "    demo_feat = vinn_features[demo_i, :]\n",
    "    \n",
    "    feat_error = np.linalg.norm(demo_feat - live_feat)\n",
    "\n",
    "    vinn_feat_scores[demo_i, live_i] = feat_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95728016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial import distance\n",
    "\n",
    "# vinn_feat_scores = np.ones(bad_pair_arr.shape) * 10.0\n",
    "# vinn_proj_scores = np.ones(bad_pair_arr.shape) * 10.0\n",
    "\n",
    "# for live_i, demo_i in tqdm(good_pairs):\n",
    "#     live_feat = vinn_features[live_i, :] \n",
    "#     demo_feat = vinn_features[demo_i, :]\n",
    "    \n",
    "#     live_proj = vinn_projections[live_i, :]\n",
    "#     demo_proj = vinn_projections[demo_i, :]\n",
    "    \n",
    "#     feat_error = np.linalg.norm(demo_feat - live_feat)\n",
    "#     proj_error = np.linalg.norm(demo_proj - live_proj)\n",
    "\n",
    "#     vinn_feat_scores[demo_i, live_i] = feat_error    \n",
    "#     vinn_proj_scores[demo_i, live_i] = proj_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# vinn_feat_pos_errors = []\n",
    "# vinn_feat_orn_errors = []\n",
    "\n",
    "# vinn_proj_pos_errors = []\n",
    "# vinn_proj_orn_errors = []\n",
    "\n",
    "# best_demo_idx_feat = np.argmin(vinn_feat_scores, axis=0)\n",
    "# best_demo_idx_proj = np.argmin(vinn_proj_scores, axis=0)\n",
    "\n",
    "# for idx in range(len(recordings)):\n",
    "#     name_q = references[idx]\n",
    "    \n",
    "#     name_d_best_feat = references[best_demo_idx_feat[idx]]\n",
    "#     name_d_best_proj = references[best_demo_idx_proj[idx]]\n",
    "    \n",
    "#     pos_err_f, orn_err_f = compute_distance(name_q, name_d_best_feat)\n",
    "#     pos_err_p, orn_err_p = compute_distance(name_q, name_d_best_proj)\n",
    "    \n",
    "#     vinn_feat_pos_errors.append(pos_err_f)\n",
    "#     vinn_feat_orn_errors.append(orn_err_f)\n",
    "    \n",
    "#     vinn_proj_pos_errors.append(pos_err_p)\n",
    "#     vinn_proj_orn_errors.append(orn_err_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43036aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vinn_feat_pos_errors = []\n",
    "vinn_feat_orn_errors = []\n",
    "\n",
    "best_demo_idx_feat = np.argmin(vinn_feat_scores, axis=0)\n",
    "\n",
    "for idx in range(len(recordings)):\n",
    "    name_q = references[idx]\n",
    "    \n",
    "    name_d_best_feat = references[best_demo_idx_feat[idx]]\n",
    "    \n",
    "    pos_err_f, orn_err_f = compute_distance(name_q, name_d_best_feat)\n",
    "    \n",
    "    vinn_feat_pos_errors.append(pos_err_f)\n",
    "    vinn_feat_orn_errors.append(orn_err_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85729db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, y_ = 15, 20\n",
    "\n",
    "def get_vinn_byol_embeddings(playbacks, transform=None):\n",
    "    features = []\n",
    "    embeddings = []\n",
    "    \n",
    "    vinn_model = models.resnet50(pretrained=False)\n",
    "    load_model = '/home/argusm/lmb_abhi/VINN/vinn_byol/BYOL_70__pretrained_1.pt'\n",
    "\n",
    "    state_dict = torch.load(load_model)\n",
    "\n",
    "    vinn_model.load_state_dict(state_dict['model_state_dict'])\n",
    "\n",
    "    arch = list(vinn_model.children())\n",
    "\n",
    "    del arch[-2]  # AvgPool\n",
    "    del arch[-1]  # FC\n",
    "\n",
    "    vinn_model = nn.Sequential(*arch)\n",
    "    \n",
    "    vinn_model.cuda()    \n",
    "    vinn_model.eval()\n",
    "    \n",
    "    for module in vinn_model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            module.track_running_stats = False\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for pb in playbacks:\n",
    "            im = pb[18].cam.get_image()[0]\n",
    "            \n",
    "            mask = pb.get_fg_mask()\n",
    "            mask = np.array(mask, dtype=float)\n",
    "\n",
    "#             tmp_mask = resize(mask, (x_, y_))\n",
    "            tmp_mask = np.asarray(Image.fromarray(mask).resize((x_, y_)))\n",
    "\n",
    "            max_x, max_y = np.where(tmp_mask == np.amax(tmp_mask))\n",
    "        \n",
    "            x_max = max_x[0]\n",
    "            y_max = max_y[0]\n",
    "                \n",
    "            im = transform(im)\n",
    "\n",
    "            im = im.unsqueeze(0).cuda()\n",
    "            \n",
    "            embedding = vinn_model(im)\n",
    "            \n",
    "            embeddings.append(embedding)\n",
    "            \n",
    "            features.append(embedding[:, :, x_max, y_max])\n",
    "            \n",
    "#         ipdb.set_trace()\n",
    "        \n",
    "        features = torch.cat(features)\n",
    "        embeddings = torch.cat(embeddings)\n",
    "    \n",
    "    features = features.detach().cpu().numpy()\n",
    "    embeddings = embeddings.detach().cpu().numpy()\n",
    "    \n",
    "    return features, embeddings\n",
    "\n",
    "vinn_pix_feat, vinn_pix_embeddings = get_vinn_byol_embeddings(playbacks, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8143c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "vinn_sim_scores_features = np.ones(bad_pair_arr.shape) * 10.0\n",
    "\n",
    "for live_i, demo_i in tqdm(good_pairs):\n",
    "    live_embedding = np.mean(vinn_pix_embeddings[live_i, ...], axis=(1, 2)) \n",
    "    \n",
    "    demo_embedding = vinn_pix_feat[demo_i, :]\n",
    "    \n",
    "    error = np.linalg.norm(demo_embedding - live_embedding)\n",
    "\n",
    "    vinn_sim_scores_features[demo_i, live_i] = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcea298",
   "metadata": {},
   "outputs": [],
   "source": [
    "vinn_pix_feat_pos_errors = []\n",
    "vinn_pix_feat_orn_errors = []\n",
    "\n",
    "best_demo_idx_vinn_feat = np.argmin(vinn_sim_scores_features, axis=0)\n",
    "\n",
    "for idx in range(len(recordings)):\n",
    "    name_q = references[idx]\n",
    "    \n",
    "    name_d_best = references[best_demo_idx_vinn_feat[idx]]\n",
    "    \n",
    "    pos_err, orn_err = compute_distance(name_q, name_d_best)\n",
    "    \n",
    "    vinn_pix_feat_pos_errors.append(pos_err)\n",
    "    vinn_pix_feat_orn_errors.append(orn_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4860c31",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641597ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orientation Errors\n",
    "\n",
    "max_orn_error = max(np.max(vs_orn_errors), np.max(hloc_orn_errors), \n",
    "                    np.max(r3m_masked_orn_errors), np.max(r3m_unmasked_orn_errors), np.max(r3m_feat_orn_errors),\n",
    "                    np.max(vinn_feat_orn_errors), np.max(vinn_pix_feat_orn_errors))\n",
    "\n",
    "res = stats.cumfreq(vs_orn_errors, numbins=30, defaultreallimits=(0.0, max_orn_error))\n",
    "x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size, res.cumcount.size)\n",
    "plt.plot(x, res.cumcount / len(vs_orn_errors), label='VS')\n",
    "\n",
    "res = stats.cumfreq(hloc_orn_errors, numbins=30, defaultreallimits=(0.0, max_orn_error))\n",
    "plt.plot(x, res.cumcount / len(hloc_orn_errors), label='HLOC')\n",
    "\n",
    "# res = stats.cumfreq(r3m_masked_orn_errors, numbins=30, defaultreallimits=(0.0, max_orn_error))\n",
    "# plt.plot(x, res.cumcount / len(r3m_masked_orn_errors), label='R3M_masked')\n",
    "\n",
    "res = stats.cumfreq(r3m_unmasked_orn_errors, numbins=30, defaultreallimits=(0.0, max_orn_error))\n",
    "plt.plot(x, res.cumcount / len(r3m_unmasked_orn_errors), label='R3M', linestyle='dotted', color='green')\n",
    "\n",
    "res = stats.cumfreq(r3m_feat_orn_errors, numbins=30, defaultreallimits=(0.0, max_orn_error))\n",
    "plt.plot(x, res.cumcount / len(r3m_feat_orn_errors), label='R3M Features', color='green')\n",
    "\n",
    "res = stats.cumfreq(vinn_feat_orn_errors, numbins=30, defaultreallimits=(0.0, max_orn_error))\n",
    "plt.plot(x, res.cumcount / len(vinn_feat_orn_errors), label='VINN', linestyle='dotted', color='red')\n",
    "\n",
    "res = stats.cumfreq(vinn_pix_feat_orn_errors, numbins=30, defaultreallimits=(0.0, max_orn_error))\n",
    "plt.plot(x, res.cumcount / len(vinn_pix_feat_orn_errors), label='VINN Features', color='red')\n",
    "\n",
    "plt.xlabel(\"Error (Degrees)\")\n",
    "plt.ylabel(\"Samples (%)\")\n",
    "plt.title(\"Orientation Errors\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"orn_errors_new.png\", dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position Errors\n",
    "\n",
    "max_pos_error = max(np.max(vs_pos_errors), np.max(hloc_pos_errors), \n",
    "                    np.max(r3m_masked_pos_errors), np.max(r3m_unmasked_pos_errors), np.max(r3m_feat_pos_errors),\n",
    "                    np.max(vinn_feat_pos_errors), np.max(vinn_pix_feat_pos_errors))\n",
    "\n",
    "res = stats.cumfreq(vs_pos_errors, numbins=30, defaultreallimits=(0.0, max_pos_error))\n",
    "x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size, res.cumcount.size)\n",
    "plt.plot(x * 1000, res.cumcount / len(vs_pos_errors), label='VS')\n",
    "\n",
    "res = stats.cumfreq(hloc_pos_errors, numbins=30, defaultreallimits=(0.0, max_pos_error))\n",
    "plt.plot(x * 1000, res.cumcount / len(hloc_pos_errors), label='HLOC')\n",
    "\n",
    "# res = stats.cumfreq(r3m_masked_pos_errors, numbins=30, defaultreallimits=(0.0, max_pos_error))\n",
    "# plt.plot(x * 1000, res.cumcount / len(r3m_masked_pos_errors), label='R3M_masked')\n",
    "\n",
    "res = stats.cumfreq(r3m_unmasked_pos_errors, numbins=30, defaultreallimits=(0.0, max_pos_error))\n",
    "plt.plot(x * 1000, res.cumcount / len(r3m_unmasked_pos_errors), label='R3M', linestyle='dotted', color='green')\n",
    "\n",
    "res = stats.cumfreq(r3m_feat_pos_errors, numbins=30, defaultreallimits=(0.0, max_pos_error))\n",
    "plt.plot(x * 1000, res.cumcount / len(r3m_feat_pos_errors), label='R3M Features', color='green')\n",
    "\n",
    "res = stats.cumfreq(vinn_feat_pos_errors, numbins=30, defaultreallimits=(0.0, max_pos_error))\n",
    "plt.plot(x * 1000, res.cumcount / len(vinn_feat_pos_errors), label='VINN', linestyle='dotted', color='red')\n",
    "\n",
    "res = stats.cumfreq(vinn_pix_feat_pos_errors, numbins=30, defaultreallimits=(0.0, max_pos_error))\n",
    "plt.plot(x * 1000, res.cumcount / len(vinn_pix_feat_pos_errors), label='VINN Features', color='red')\n",
    "\n",
    "plt.xlabel(\"Error (mm)\")\n",
    "plt.ylabel(\"Samples (%)\")\n",
    "plt.title(\"Position Errors\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"pos_errors_new.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c835b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7a494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
