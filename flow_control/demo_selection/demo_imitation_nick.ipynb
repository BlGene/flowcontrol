{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383824bb",
   "metadata": {},
   "source": [
    "# Immitation Minimal Viable Program\n",
    "\n",
    "This notebook tests an imitation system using other demonstrations as pseudo live views.\n",
    "\n",
    "\n",
    "## 1. Load Existing Demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ca909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import widgets, interact, Layout\n",
    "from flow_control.demo.simple_loader_nick import SimpleLoader\n",
    "\n",
    "root_dir = Path(\"/home/argusm/CLUSTER/robot_recordings/hand_recordings/view_1/\")\n",
    "num_runs = len(list(root_dir.iterdir()))\n",
    "loaders = [SimpleLoader(root_dir, run=r) for r in range(num_runs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b600c",
   "metadata": {},
   "source": [
    "### 1.1 Visualze Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig, ax = plt.subplots(1,figsize=(8, 6))\n",
    "fig.suptitle(\"Demonstration Frames\")\n",
    "ax.set_axis_off()\n",
    "image_h = ax.imshow(loaders[0].get_image(0))\n",
    "\n",
    "def update(demo_index, frame_index):\n",
    "    demo_len = loaders[demo_index].get_len()\n",
    "    if frame_index >= demo_len:\n",
    "        print(f\"invalid frame index: {frame_index}, demo length: {demo_len}\")\n",
    "        frame_index = demo_len -1\n",
    "    image = loaders[demo_index].get_image(frame_index)\n",
    "    image_h.set_data(image)\n",
    "    fig.canvas.draw_idle()\n",
    "    \n",
    "slider_w = widgets.IntSlider(min=0, max=num_runs-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "slider_i = widgets.IntSlider(min=0, max=200-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "\n",
    "interact(update, demo_index=slider_w, frame_index=slider_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fbd8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate data: clip demonstrations to frames where manipulation happens\n",
    "motion_bounds = {0:(25, 52),\n",
    "                 1:(30, 120),\n",
    "                 2:(38, 100),\n",
    "                 3:(36, 71)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a99b4",
   "metadata": {},
   "source": [
    "# 2. Find Object Trajectories (2D)\n",
    "\n",
    "## 2.1 Find Object Segmentation Mask (using motion between demos)\n",
    "\n",
    "This finds the object segmentation for the initial frame using the motion of the object between different demos. This works if the camera position is static. Instead of this we could also use:\n",
    "1. Manual annotation\n",
    "2. Hands23 (Nick is planning to do this)\n",
    "3. Segment-Anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Servoing Module\n",
    "from flow_control.flow.module_raft import FlowModule\n",
    "from flow_control.flow.flow_plot import FlowPlot\n",
    "\n",
    "flow_module = FlowModule(size=(640,480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "live_index = 0\n",
    "demo_index = (live_index + 1) % len(loaders)\n",
    "\n",
    "live_rgb = loaders[live_index].get_image(0)\n",
    "demo_rgb = loaders[demo_index].get_image(0)\n",
    "\n",
    "# compute flow, magnitude and then threshold\n",
    "flow = flow_module.step(live_rgb, demo_rgb)\n",
    "fn = np.linalg.norm(flow, axis=2)\n",
    "flow_threshold = 5\n",
    "object_segmentation = fn > flow_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot flow image\n",
    "fp = FlowPlot()\n",
    "flow_image = fp.compute_image(flow)\n",
    "fig, ax = plt.subplots(1,3, figsize=(24, 6))\n",
    "[x.set_axis_off() for x in ax]\n",
    "ax[0].imshow(live_rgb)\n",
    "ax[1].imshow(demo_rgb)\n",
    "ax[2].imshow(flow_image)\n",
    "#ax[2].scatter(centroid_x, centroid_y, marker='x', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483671f0",
   "metadata": {},
   "source": [
    "## 2.2 Find Object Keypoints\n",
    "\n",
    "Currently just use the center of object segmentation.\n",
    "\n",
    "Find the keypoints for all demonstrations by using optical flow to warp between demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6934e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid = np.mean(np.argwhere(object_segmentation), axis=0)\n",
    "centroid_x, centroid_y = int(centroid[1]), int(centroid[0])\n",
    "print(f\"centroid for demo {live_index} is ({centroid_x}, {centroid_y})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot centroid\n",
    "ax = plt.imshow(fn > flow_threshold)\n",
    "print(flow[centroid_y,centroid_x])\n",
    "print(centroid_x, centroid_y)\n",
    "plt.scatter(centroid_x, centroid_y, marker='x', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89889e1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert np.all(centroid == (411, 181))\n",
    "warped_centroids = [centroid]\n",
    "\n",
    "plot_warped_centroids = False\n",
    "for other in (1,2,3):\n",
    "    live_rgb = loaders[0].get_image(0)\n",
    "    demo_rgb = loaders[other].get_image(0)\n",
    "    flow = flow_module.step(live_rgb, demo_rgb)\n",
    "    \n",
    "    change = flow[centroid[1], centroid[0]]\n",
    "    centroid_x_n, centroid_y_n = int(round(centroid_x+change[0])), int(round(centroid_y+change[1]))\n",
    "    warped_centroids.append((centroid_x_n, centroid_y_n))\n",
    "    \n",
    "    if plot_warped_centroids:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        [x.set_axis_off() for x in ax]\n",
    "        ax[0].imshow(live_rgb)\n",
    "        ax[1].imshow(demo_rgb)\n",
    "        ax[0].scatter(centroid_x, centroid_y, marker='x', color='red')\n",
    "        ax[1].scatter(centroid_x_n, centroid_y_n, marker='x', color='red')\n",
    "        plt.show()\n",
    "    \n",
    "print(\"warped centroids\", warped_centroids)\n",
    "# should be roughly warped_centroids = [(411, 181), (387, 203), (395, 198), (381, 196)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae0200",
   "metadata": {},
   "source": [
    "## 2.3 Track Keypoint Trajectories\n",
    "\n",
    "by following the optical flow between demonstration frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b4355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(demo_index, centroid, frames):\n",
    "    centroid_list = [centroid]\n",
    "    centroid_x, centroid_y = centroid\n",
    "    for i in range(len(frames)-1):\n",
    "        frame_a, frame_b = frames[i], frames[i+1]\n",
    "\n",
    "        image_a = loaders[demo_index].get_image(frame_a)\n",
    "        image_b = loaders[demo_index].get_image(frame_b)\n",
    "        flow = flow_module.step(image_a, image_b)\n",
    "        flow_image = fp.compute_image(flow)\n",
    "\n",
    "        change = flow[centroid_y, centroid_x]\n",
    "        centroid_x_n, centroid_y_n = int(round(centroid_x+change[0])), int(round(centroid_y+change[1]))\n",
    "        centroid_x, centroid_y = centroid_x_n, centroid_y_n\n",
    "        centroid_list.append((centroid_x,centroid_y))\n",
    "        \n",
    "    return np.array(centroid_list)\n",
    "\n",
    "trajectories = {}\n",
    "for demo_index in range(len(loaders)):\n",
    "    centroid = warped_centroids[demo_index]\n",
    "    frames = np.linspace(motion_bounds[demo_index][0], motion_bounds[demo_index][1], 10).astype(int)\n",
    "    trajectory = get_trajectory(demo_index, centroid, frames)\n",
    "    trajectories[demo_index] = trajectory\n",
    "    print(\"done with demo_index\", demo_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de4a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot live and demo trajectories with smoothing\n",
    "%matplotlib inline\n",
    "from scipy import interpolate\n",
    "\n",
    "def smooth_line(arr, samples=100):\n",
    "    x, y = zip(*arr)\n",
    "    #create spline function\n",
    "    f, u = interpolate.splprep([x, y], s=0)\n",
    "    #create interpolated lists of points\n",
    "    xint, yint = interpolate.splev(np.linspace(0, 1, samples), f)\n",
    "    return np.stack((xint, yint),axis=1)\n",
    "\n",
    "demo_index = 0\n",
    "trajectory = trajectories[demo_index]\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax = [ax,]\n",
    "[x.set_axis_off() for x in ax]\n",
    "image_a = loaders[demo_index].get_image(motion_bounds[demo_index][0])\n",
    "ax[0].imshow(image_a)\n",
    "#ax[0].plot(trajectory[:, 0], trajectory[:, 1], marker='.', color='lime')\n",
    "st = smooth_line(trajectory)\n",
    "ax[0].plot(st[:,0], st[:,1], color='lime')\n",
    "other_trajectories = []\n",
    "for i in range(len(loaders)):\n",
    "    if i == demo_index:\n",
    "        continue\n",
    "    trj = trajectories[i].copy()\n",
    "    start_other = trj[0]\n",
    "    start_curr= trajectory[0]\n",
    "    trj += start_curr - start_other\n",
    "    #ax[0].plot(trj[:, 0], trj[:, 1], marker='.', color='blue')\n",
    "    trj = smooth_line(trj)\n",
    "    ax[0].plot(trj[:, 0], trj[:, 1], marker='', color='blue')\n",
    "    \n",
    "other_trajectories = np.array(other_trajectories)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62e42c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the live trajectory\n",
    "demo_index = 0\n",
    "trajectory = trajectories[demo_index]\n",
    "frames = 12, 37, 52\n",
    "fig_frames = [loaders[0].get_image(f) for f in frames]\n",
    "fig_frames = np.mean(fig_frames, axis=0).round().astype(np.uint8)\n",
    "fig_frames = Image.fromarray(fig_frames)\n",
    "print(fig_frames.size)\n",
    "fig_frames_small = ImageOps.contain(fig_frames, (640,480))\n",
    "rel_size = np.array(fig_frames.size) / np.array(fig_frames_small.size)\n",
    "plt.imshow(fig_frames_small)\n",
    "plt.plot(trajectory[:, 0]/rel_size[0], trajectory[:, 1]/rel_size[1],marker='.', c='lime')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a41bd",
   "metadata": {},
   "source": [
    "## 2.4 Model Demo Trajectories (2D)\n",
    "\n",
    "Create a 2D model of the demo distributions by fitting gaussians for each point.\n",
    "This probably does resampling according to the percentage of trajectory completed or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import scipy.stats as st\n",
    "\n",
    "samples = 8  # number of gauss curves\n",
    "\n",
    "# start plot\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.set_axis_off()\n",
    "image_a = loaders[demo_index].get_image(motion_bounds[demo_index][0])\n",
    "ax.imshow(image_a)\n",
    "st = smooth_line(trajectory)\n",
    "ax.plot(st[:,0], st[:,1], color='lime')\n",
    "\n",
    "sample_list = []\n",
    "for i in trajectories:\n",
    "    trj = trajectories[i].copy()\n",
    "    start_other = trj[0]\n",
    "    start_curr = trajectories[0][0]\n",
    "    trj += start_curr - start_other\n",
    "    res = smooth_line(trj, samples=samples)\n",
    "    sample_list.append(res)\n",
    "    #plt.plot(trj[:,0], trj[:,1],\".-\")  # plot raw points\n",
    "    #plt.scatter(xint, yint, c=np.linspace(0,1,len(xint)))  # plot smoothed points\n",
    "    \n",
    "gauss_n = 50\n",
    "sample_list = np.array(sample_list)\n",
    "for s in range(samples):\n",
    "    x = sample_list[:, s, 0]\n",
    "    y = sample_list[:, s, 1]\n",
    "    xmin, xmax = max(x.min()-gauss_n, 0), x.max()+gauss_n\n",
    "    ymin, ymax = max(y.min()-gauss_n, 0), y.max()+gauss_n\n",
    "    mean = np.mean(sample_list[:, s, :], axis=0)\n",
    "    cov = np.cov(sample_list[:, s, :], rowvar=0)\n",
    "    cov*= 3  # looks nicer\n",
    "    x, y = np.mgrid[xmin:xmax:25j, ymin:ymax:25j]\n",
    "    rv = multivariate_normal(mean, cov)\n",
    "    data = np.dstack((x, y))\n",
    "    z = rv.pdf(data)\n",
    "    plt.contour(x, y, z, levels=3, cmap='jet')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb419c91",
   "metadata": {},
   "source": [
    "## 2.5 Find 3D Trajectories\n",
    "\n",
    "...maybe start by just using depth observation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324796b",
   "metadata": {},
   "source": [
    "# 3. Find Object Masks\n",
    "\n",
    "Here we try to find the segmentation of a live image based on a demo image segmentation.\n",
    "\n",
    "We only have to do this for the inital images, as its for grasp computation.\n",
    "\n",
    "Here we use optical flow, several alternative stratagies are possible here:\n",
    "1. SIFT features\n",
    "2. Segment-Anything + Dino features\n",
    "\n",
    "## 3.1 Same Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_index = 0  # this needs to match the variable demo_segmentation\n",
    "live_index = 1\n",
    "\n",
    "def plot_mask_edge(mask, image):\n",
    "    # note: does in-place modification of image variable\n",
    "    edge = np.gradient(mask.astype(float))\n",
    "    edge = (np.abs(edge[0]) + np.abs(edge[1])) > 0\n",
    "    image[edge] = (0, 255, 0)\n",
    "    \n",
    "demo_rgb = loaders[demo_index].get_image(0)\n",
    "demo_seg = object_segmentation.copy()\n",
    "demo_seg = demo_seg[:, :, np.newaxis]\n",
    "print(demo_seg.shape)\n",
    "\n",
    "live_rgb = loaders[live_index].get_image(0)\n",
    "live_seg = flow_module.warp_mask(demo_seg, demo_rgb, live_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d35aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the image up a bit\n",
    "from skimage.morphology import dilation, erosion\n",
    "live_seg_2 = dilation(erosion(live_seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d6318",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "[x.set_axis_off() for x in ax]\n",
    "demo_rgb_plot = np.copy(demo_rgb)\n",
    "live_rgb_plot = np.copy(live_rgb)\n",
    "\n",
    "plot_mask_edge(demo_seg[:, :, 0], demo_rgb_plot)\n",
    "plot_mask_edge(live_seg_2[:, :, 0], live_rgb_plot)\n",
    "\n",
    "ax[0].imshow(demo_rgb_plot)\n",
    "ax[1].imshow(live_rgb_plot)\n",
    "#ax[1].imshow(live_seg_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9664e22f",
   "metadata": {},
   "source": [
    "## 3.2 Different Perspective\n",
    "\n",
    "Perspective `view_2` recording `0` seems to have poor depth values. Use recording `1` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c653f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "root2_dir = Path(\"/home/argusm/CLUSTER/robot_recordings/hand_recordings/view_2/\")\n",
    "live_index = 1\n",
    "\n",
    "num_runs2 = len(list(root2_dir.iterdir()))\n",
    "loaders2 = [SimpleLoader(root2_dir, run=r) for r in range(num_runs2)]\n",
    "demo_seg = object_segmentation.copy()\n",
    "demo_seg = demo_seg[:, :, np.newaxis]\n",
    "\n",
    "live_rgb = loaders2[live_index].get_image(0)\n",
    "live_seg = flow_module.warp_mask(demo_seg, demo_rgb, live_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "live_seg_2 = dilation(erosion(live_seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15743ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "[x.set_axis_off() for x in ax]\n",
    "demo_rgb_plot = np.copy(demo_rgb)\n",
    "live_rgb_plot = np.copy(live_rgb)\n",
    "\n",
    "plot_mask_edge(demo_seg[:, :, 0], demo_rgb_plot)\n",
    "plot_mask_edge(live_seg_2[:, :, 0], live_rgb_plot)\n",
    "\n",
    "ax[0].imshow(demo_rgb_plot)\n",
    "ax[1].imshow(live_rgb_plot)\n",
    "#ax[1].imshow(live_seg_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c51995",
   "metadata": {},
   "source": [
    "## 3.3 Clean-Up Segmentation (incomplete)\n",
    "\n",
    "1. Use Pointcloud distances (nearest neighbors to the warped mask)\n",
    "2. Use Segment-Anything to clean up segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52532849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "def get_depth(demo_dir, run, frame_index) -> np.ndarray:\n",
    "        \"\"\"returns the image for a given frame\n",
    "        Returns:\n",
    "            depth: numpy array (w,h) in range (0, ~12m)\n",
    "        \"\"\"\n",
    "        depth_path = demo_dir / \"{0}/images.np.npz\".format(run)\n",
    "        depths = np.load(depth_path)[\"depths\"]\n",
    "        return depths[frame_index]\n",
    "\n",
    "rgb = o3d.geometry.Image(live_rgb)\n",
    "live_depth = get_depth(root2_dir, 0,0)\n",
    "depth = o3d.geometry.Image(live_depth)\n",
    "rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(rgb, depth,\n",
    "                                      depth_scale=1.0, depth_trunc=1.0,\n",
    "                                      convert_rgb_to_intensity=False)\n",
    "\n",
    "width = np.asarray(depth).shape[1]\n",
    "height = np.asarray(depth).shape[0]\n",
    "fx=700.819\n",
    "fy=700.819\n",
    "cx=665.465\n",
    "cy=371.953\n",
    "K_o3d = o3d.camera.PinholeCameraIntrinsic(width, height, fx, fy, cx, cy)\n",
    "pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd, K_o3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e0457",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "[x.set_axis_off() for x in ax]\n",
    "\n",
    "demo_index = 1\n",
    "demo_frame = 0\n",
    "live_rgb = loaders2[demo_index].get_image(demo_frame)\n",
    "live_depth = get_depth(root2_dir, demo_index, demo_frame)\n",
    "ax[0].imshow(live_rgb)\n",
    "ax[1].imshow(live_depth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867caab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf78310",
   "metadata": {},
   "source": [
    "# 4. Generate Grasps\n",
    "\n",
    "Send the segmented pointcloud from the live view to a grasp generation system.\n",
    "\n",
    "## 4.1 Filter Grasps\n",
    "\n",
    "Find a grasp close to the hand pose, in case we generate too many candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26f2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
