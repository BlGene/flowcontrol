{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f87b2d",
   "metadata": {},
   "source": [
    "# Record Random Views in Simulation\n",
    "\n",
    "Record random views in simulation. This type of data is easy to collect on a real robot, which is why its a good candidate to use it for pre-training a similarity function. Set `num_episodes` to control the number of scenes recorded and `max_steps` to control the numbers of views recorded.\n",
    "\n",
    "The equivalent recordings from the real robot can be found here:\n",
    "\n",
    "`/misc/lmbraid19/argusm/CLUSTER/robot_recordings/pose_estimation/ur3_objects`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e27844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "import unittest\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from gym_grasping.envs.robot_sim_env import RobotSimEnv\n",
    "from flow_control.demo.demo_episode_recorder import record_sim\n",
    "from flow_control.runner import evaluate_control\n",
    "from flow_control.servoing.module import ServoingModule\n",
    "from math import pi\n",
    "import getpass\n",
    "from robot_io.calibration.gripper_cam_calibration import GripperCamPoseSampler\n",
    "\n",
    "\n",
    "experiment = \"random_views\"\n",
    "goal = \"random_views_test\"\n",
    "\n",
    "def get_data_dir():\n",
    "    username = getpass.getuser()\n",
    "    if username == \"argusm\":\n",
    "        return \"/tmp/flow_experiments3\"\n",
    "    elif username == \"nayakab\":\n",
    "        return \"../tmp\"\n",
    "\n",
    "data_dir = get_data_dir()\n",
    "root_dir = os.path.join(data_dir, experiment)\n",
    "goal_dir = os.path.join(data_dir, goal)\n",
    "\n",
    "renderer = \"egl\"\n",
    "object_selected = \"trapeze\"\n",
    "task_variant = \"rP\"  # rotation plus (+-pi)\n",
    "\n",
    "def get_configurations(root_dir=root_dir, task=\"shape_sorting\", num_episodes=20, start_seed=0, prefix=\"\", object_selected='trapeze'):\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    save_dir_template = os.path.join(root_dir, f\"{prefix}_{task}_{object_selected}\")\n",
    "    for seed in range(start_seed, start_seed + num_episodes):\n",
    "        save_dir = save_dir_template + f\"_{task_variant}\"+f\"_seed{seed:03d}\"\n",
    "        yield object_selected, task_variant, seed, save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robot_io.recorder.simple_recorder import SimpleRecorder\n",
    "from tqdm import tqdm\n",
    "\n",
    "cfg = { 0: {'prefix': 'train', 'task': 'shape_sorting', 'num_episodes': 200, 'start_seed': 0, 'object_selected': object_selected, 'root_dir': root_dir},\n",
    "      #  1: {'prefix': 'test', 'task': 'shape_sorting', 'num_episodes': 20, 'start_seed': 1000, 'object_selected': object_selected, 'root_dir': root_dir}\n",
    "      }\n",
    "\n",
    "max_steps = 200\n",
    "for key, value in cfg.items():\n",
    "    demo_cfg = get_configurations(value['root_dir'], value['task'], \n",
    "                                  value['num_episodes'], value['start_seed'],\n",
    "                                  prefix=value['prefix'],\n",
    "                                  object_selected=value['object_selected'])\n",
    "    \n",
    "    for object_selected, task_variant, seed, save_dir in demo_cfg:\n",
    "        param_info = {\"object_selected\": value['object_selected'], \"task_selected\": value['task']}\n",
    "        env = RobotSimEnv(task='recombination', renderer=renderer, act_type='continuous',\n",
    "                          initial_pose='close', max_steps=max_steps, control='absolute-full',\n",
    "                          img_size=(256, 256),\n",
    "                          param_randomize=(\"geom\",),\n",
    "                          param_info=param_info,\n",
    "                          task_info=dict(object_rot_range={\"rP\":pi/2.,\"rR\":pi/6.}[task_variant]),\n",
    "                          seed=seed)\n",
    "\n",
    "        if task_variant == \"rP\":\n",
    "            assert env.params.variables[f\"{object_selected}_pose\"][\"d\"][3] == pi/2.\n",
    "        elif task_variant == \"rR\":\n",
    "            assert env.params.variables[f\"{object_selected}_pose\"][\"d\"][3] == pi/6.\n",
    "        #update_object_orn(env, object_selected, orn)\n",
    "\n",
    "        s = 2.0\n",
    "        inital_pos, initial_orn = env.robot.get_tcp_pos_orn()\n",
    "        pose_sampler = GripperCamPoseSampler(inital_pos, initial_orn,\n",
    "                                             theta_limits=[2.36, 3.93],\n",
    "                                             r_limits=[0.05, 0.1],\n",
    "                                             h_limits=[-0.05*s, 0.05*s],\n",
    "                                             trans_limits=[-0.05*s, 0.05*s],\n",
    "                                             yaw_limits=[-0.087*s, 0.087*s],\n",
    "                                             pitch_limit=[-0.087*s, 0.087*s],\n",
    "                                             roll_limit=[-0.087*s, 0.087*s])\n",
    "        rec = SimpleRecorder(env, save_dir=save_dir)\n",
    "    \n",
    "        for i in tqdm(range(max_steps)):\n",
    "            pos, orn = pose_sampler.sample_pose()\n",
    "            action = dict(motion=(pos,orn,1), ref=\"abs\")\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rec.step(obs, action, None, rew, done, info)\n",
    "            \n",
    "        rec.save()\n",
    "        \n",
    "        del env\n",
    "        time.sleep(.5)\n",
    "        print(f\"{seed}/{value['num_episodes']}\", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce53ee2",
   "metadata": {},
   "source": [
    "# View Recorded Data\n",
    "\n",
    "Show the dataset that we have recorded.\n",
    "\n",
    "TODO: include orientation in the relative distance computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_control.servoing.playback_env_servo import PlaybackEnvServo\n",
    "\n",
    "def get_recordings(directory):\n",
    "    return sorted([os.path.join(directory, rec) for rec in os.listdir(directory) if os.path.isdir(os.path.join(directory, rec))])\n",
    "\n",
    "demo_recordings = get_recordings(root_dir)\n",
    "#goal_recordings = get_recordings(goal_dir)\n",
    "\n",
    "recordings = demo_recordings\n",
    "print(\"Number of recordings:\", len(recordings))\n",
    "print(recordings[0])\n",
    "print(recordings[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e3a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the demonstration episodes\n",
    "playbacks = [PlaybackEnvServo(rec) for rec in recordings[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b36b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import widgets, interact, Layout\n",
    "\n",
    "demo_index = 0\n",
    "t_tcp_cam = playbacks[demo_index][0].cam.get_extrinsic_calibration()\n",
    "\n",
    "positions = []\n",
    "orientations = []\n",
    "orn_inv = []\n",
    "for frame_index in range(len(playbacks[demo_index])):\n",
    "    t_tcp_robot = playbacks[demo_index][frame_index].robot.get_tcp_pose()\n",
    "    trf = t_tcp_robot @ t_tcp_cam\n",
    "    trf = np.linalg.inv(trf)\n",
    "    positions.append(trf[0:3,3])\n",
    "    orientations.append(R.from_matrix(trf[:3,:3]))\n",
    "    orn_inv.append(R.from_matrix(trf[:3,:3]).inv())\n",
    "\n",
    "positions = np.array(positions)\n",
    "orientations = np.array(orientations)\n",
    "orn_inv = np.array(orn_inv)\n",
    "dist = np.linalg.norm(positions[:, None, :] - positions[None, :, :], axis=-1)\n",
    "\n",
    "dist_orn = np.zeros((len(orientations), len(orientations)))\n",
    "idx_x, idx_y = np.triu_indices(len(orientations))\n",
    "orn_diff = (orientations[idx_x]*orn_inv[idx_y])\n",
    "orn_diff = [x.magnitude() for x in orn_diff]\n",
    "dist_orn[idx_x, idx_y] = orn_diff\n",
    "dist_orn = dist_orn + dist_orn.T - np.diag(np.diag(dist_orn))\n",
    "\n",
    "dist_cmb = dist/np.max(dist) + dist_orn/np.max(dist_orn)\n",
    "\n",
    "# Plot the demonstrations\n",
    "%matplotlib notebook\n",
    "fig, ax = plt.subplots(1, 2,figsize=(8, 6))\n",
    "fig.suptitle(\"Demonstration Frames\")\n",
    "ax[0].set_axis_off()\n",
    "ax[1].set_axis_off()\n",
    "image_h = ax[0].imshow(playbacks[0].cam.get_image()[0])\n",
    "\n",
    "ax[1].imshow(dist-dist.T)\n",
    "np.fill_diagonal(dist, 1e3)\n",
    "np.fill_diagonal(dist_orn, 1e3)\n",
    "np.fill_diagonal(dist_cmb, 1e3)\n",
    "\n",
    "def update(demo_index, frame_index):\n",
    "    image = playbacks[demo_index][frame_index].cam.get_image()[0]\n",
    "    print(\"closest neighbor:\", np.argmin(dist_cmb[frame_index]), \"(frame index)\")\n",
    "    image_h.set_data(image)\n",
    "    fig.canvas.draw_idle()\n",
    "    \n",
    "slider_w = widgets.IntSlider(min=0, max=len(playbacks)-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "slider_i = widgets.IntSlider(min=0, max=200-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "\n",
    "interact(update, demo_index=slider_w, frame_index=slider_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd6e29",
   "metadata": {},
   "source": [
    "## Light Version: w/o flowcontrol dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d555996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recordings(directory):\n",
    "    return sorted([os.path.join(directory, rec) for rec in os.listdir(directory) if os.path.isdir(os.path.join(directory, rec))])\n",
    "\n",
    "root_dir = \"/home/argusm/CLUSTER/robot_recordings/pose_estimation/sim_objects/random_views\"\n",
    "demo_recordings = get_recordings(root_dir)\n",
    "\n",
    "recordings = demo_recordings\n",
    "print(\"Number of recordings:\", len(recordings))\n",
    "print(recordings[0])\n",
    "print(recordings[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d1aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "def get_image(demo_dir, frame_index, depth=False):\n",
    "    arr = np.load(os.path.join(demo_dir, f\"frame_{frame_index:06d}.npz\"))\n",
    "    rgb_gripper = arr[\"rgb_gripper\"]\n",
    "    return rgb_gripper\n",
    "\n",
    "def get_reward(demo_dir):\n",
    "    frame_names = sorted(glob(f\"{demo_dir}/frame_*.npz\"))\n",
    "    rew = np.load(frame_names[-1])[\"rew\"].item()\n",
    "    return rew\n",
    "\n",
    "def get_len(demo_dir):\n",
    "    frame_names = sorted(glob(f\"{demo_dir}/frame_*.npz\"))\n",
    "    return len(frame_names)\n",
    "\n",
    "def get_info(demo_dir, frame_index):\n",
    "    arr = np.load(os.path.join(demo_dir, f\"frame_{frame_index:06d}.npz\"), allow_pickle=True)\n",
    "    return arr[\"info\"].item()\n",
    "\n",
    "def pos_orn_to_matrix(pos, orn):\n",
    "    mat = np.eye(4)\n",
    "    if isinstance(orn, np.quaternion):\n",
    "        orn = np_quat_to_scipy_quat(orn)\n",
    "        mat[:3, :3] = R.from_quat(orn).as_matrix()\n",
    "    elif len(orn) == 4:\n",
    "        mat[:3, :3] = R.from_quat(orn).as_matrix()\n",
    "    elif len(orn) == 3:\n",
    "        mat[:3, :3] = R.from_euler('xyz', orn).as_matrix()\n",
    "    mat[:3, 3] = pos\n",
    "    return mat\n",
    "\n",
    "def get_tcp_pose(demo_dir, frame_index):\n",
    "    arr = np.load(os.path.join(demo_dir, f\"frame_{frame_index:06d}.npz\"),allow_pickle=True)\n",
    "    state = arr[\"robot_state\"].item()\n",
    "    return pos_orn_to_matrix(state[\"tcp_pos\"], state[\"tcp_orn\"])\n",
    "\n",
    "def get_extr_cal(demo_dir):\n",
    "    camera_info = np.load(Path(demo_dir) / \"camera_info.npz\", allow_pickle=True)\n",
    "    extr = camera_info[\"gripper_extrinsic_calibration\"]\n",
    "    return extr\n",
    "    \n",
    "#fi = 3\n",
    "#t_tcp_cam1 = playbacks[0][fi].cam.get_extrinsic_calibration()\n",
    "#t_tcp_cam2 = get_extr_cal(recordings[0])\n",
    "#pose1 = playbacks[0][fi].robot.get_tcp_pose()\n",
    "#pose2 = get_tcp_pose(recordings[0], fi)\n",
    "#diff = pose1 - pose2\n",
    "#print(diff)\n",
    "#print(t_tcp_cam1 -t_tcp_cam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import widgets, interact, Layout\n",
    "\n",
    "demo_index = 40\n",
    "demo_dir = recordings[demo_index]\n",
    "\n",
    "t_tcp_cam = get_extr_cal(demo_dir)\n",
    "\n",
    "positions = []\n",
    "orientations = []\n",
    "orn_inv = []\n",
    "for frame_index in range(get_len(demo_dir)):\n",
    "    t_tcp_robot = get_tcp_pose(demo_dir, frame_index)\n",
    "    trf = t_tcp_robot @ t_tcp_cam\n",
    "    trf = np.linalg.inv(trf)\n",
    "    positions.append(trf[0:3,3])\n",
    "    orientations.append(R.from_matrix(trf[:3,:3]))\n",
    "    orn_inv.append(R.from_matrix(trf[:3,:3]).inv())\n",
    "\n",
    "positions = np.array(positions)\n",
    "orientations = np.array(orientations)\n",
    "orn_inv = np.array(orn_inv)\n",
    "dist = np.linalg.norm(positions[:, None, :] - positions[None, :, :], axis=-1)\n",
    "\n",
    "dist_orn = np.zeros((len(orientations), len(orientations)))\n",
    "idx_x, idx_y = np.triu_indices(len(orientations))\n",
    "orn_diff = (orientations[idx_x]*orn_inv[idx_y])\n",
    "orn_diff = [x.magnitude() for x in orn_diff]\n",
    "dist_orn[idx_x, idx_y] = orn_diff\n",
    "dist_orn = dist_orn + dist_orn.T - np.diag(np.diag(dist_orn))\n",
    "\n",
    "dist_cmb = dist/np.max(dist) + dist_orn/np.max(dist_orn)\n",
    "\n",
    "# Plot the demonstrations\n",
    "%matplotlib notebook\n",
    "fig, ax = plt.subplots(1, 2,figsize=(8, 6))\n",
    "fig.suptitle(\"Demonstration Frames\")\n",
    "ax[0].set_axis_off()\n",
    "ax[1].set_axis_off()\n",
    "image_h = ax[0].imshow(get_image(demo_dir, 0))\n",
    "\n",
    "ax[1].imshow(dist-dist.T)\n",
    "np.fill_diagonal(dist, 1e3)\n",
    "np.fill_diagonal(dist_orn, 1e3)\n",
    "np.fill_diagonal(dist_cmb, 1e3)\n",
    "\n",
    "def update(demo_index, frame_index):\n",
    "    image = get_image(demo_dir, frame_index)\n",
    "    print(\"closest neighbor:\", np.argmin(dist_cmb[frame_index]), \"(frame index)\")\n",
    "    image_h.set_data(image)\n",
    "    fig.canvas.draw_idle()\n",
    "    \n",
    "slider_w = widgets.IntSlider(min=0, max=len(recordings)-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "slider_i = widgets.IntSlider(min=0, max=200-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "\n",
    "interact(update, demo_index=slider_w, frame_index=slider_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345e4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
