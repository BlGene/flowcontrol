{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Demonstration Viewer\n",
    "\n",
    "This notebook is for viewing and processing demonstrations so that they can be used for servoing. This has two main components. The first is subsampling the trajectory to obtain a set of keysteps for which we want to do servoing. It is not necessary to servo to all intermediate states, this increases the speed of servoing dramatically.\n",
    "\n",
    "The second is computing a foregreound segmentation mask. This is needed in order to be able to focus on the objects of interested and to avoid being confounded by various other objects.\n",
    "\n",
    "Script Arguments:\n",
    "\n",
    "    recording: the directory in which the recording is located. Should include:\n",
    "        `frame_000000.npz`\n",
    "        `camera_info.npz`\n",
    "   \n",
    "Returns:\n",
    "    1. `servo_keep.json`\n",
    "    2. `servo_mask.json`\n",
    "    \n",
    "\n",
    "# Setup\n",
    "\n",
    "First we start by loading the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def is_notebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True  # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False  # Probably standard Python interpreter\n",
    "\n",
    "interactive = is_notebook()  # becomes overwritten\n",
    "if interactive:\n",
    "    get_ipython().run_line_magic('matplotlib', 'notebook')\n",
    "    from ipywidgets import widgets, interact, Layout\n",
    "    import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from robot_io.recorder.simple_recorder import load_rec_list, unprocess_seg\n",
    "\n",
    "if interactive:    \n",
    "    # set parameters here\n",
    "    recording = \"../tmp_test/pick_n_place/\"\n",
    "    #recording = \"../tmp_test/shape_sorting_rY/\"\n",
    "    #recording = \"/home/argusm/CLUSTER/robot_recordings/flow/sick_vacuum/17-19-19/\"\n",
    "else:\n",
    "    # expect commandline input\n",
    "    import sys\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: Demonstration_Viewer.py <episode_dir>\")\n",
    "    recording = sys.argv[1]\n",
    "\n",
    "if not os.path.isdir(recording):\n",
    "    ValueError(f\"Recording directory not found: {recording}\")\n",
    "\n",
    "segment_conf_fn = os.path.join(recording, \"segment_conf.json\")\n",
    "keep_fn = os.path.join(recording, f\"servo_keep.json\")\n",
    "mask_fn = os.path.join(recording, f\"servo_mask.npz\")\n",
    "\n",
    "try:\n",
    "    with open(segment_conf_fn, \"r\") as f_obj:\n",
    "        orig_seg_conf = json.load(f_obj)\n",
    "        if type(orig_seg_conf[\"objects\"]) == list:\n",
    "            orig_seg_conf[\"objects\"] = orig_seg_conf[\"objects\"][0]\n",
    "        seg_conf = copy.deepcopy(orig_seg_conf)\n",
    "except FileNotFoundError:\n",
    "    seg_conf = None\n",
    "\n",
    "rec = load_rec_list(recording)\n",
    "video_recording = np.array([renv.cam.get_image()[0] for renv in rec])\n",
    "actions = np.array([renv.get_action()[\"motion\"] for renv in rec], dtype=object)\n",
    "tcp_pos = np.array([renv.robot.get_tcp_pos() for renv in rec])\n",
    "tcp_orn = np.array([renv.robot.get_tcp_orn() for renv in rec])\n",
    "gripper_width = np.array([renv.robot.gripper.width() for renv in rec])\n",
    "assert gripper_width.ndim == 1\n",
    "\n",
    "masks_sim_l = []\n",
    "anchors = []\n",
    "wp_names = []\n",
    "for rec_el in rec:\n",
    "    rec_info = rec_el.data[\"info\"].item()\n",
    "    \n",
    "    if \"seg_mask\" in rec_info:\n",
    "        tmp = unprocess_seg(rec_info[\"seg_mask\"])[0]\n",
    "        masks_sim_l.append(tmp)\n",
    "    else:\n",
    "        masks_sim_l.append(None)\n",
    "    \n",
    "    if \"move_anchor\" in rec_info:\n",
    "        anchors.append(rec_info[\"move_anchor\"])\n",
    "    else:\n",
    "        anchors.append(None)\n",
    "    \n",
    "    if \"wp_name\" in rec_info:\n",
    "        wp_names.append(rec_info[\"wp_name\"])\n",
    "    else:\n",
    "        wp_names.append(\"\")\n",
    "\n",
    "masks_sim = np.array(masks_sim_l)\n",
    "\n",
    "num_frames = len(rec)\n",
    "max_frame = num_frames-1\n",
    "\n",
    "print(f\"loaded segmentation masks {np.sum([s is not None for s in masks_sim_l])}/{num_frames}\" )\n",
    "print(f\"loaded anchors {np.sum([a is not None for a in anchors])}/{num_frames}\" )\n",
    "print(f\"loaded waypoint names {np.sum([wp !='' for wp in wp_names])}/{num_frames}\")\n",
    "\n",
    "if seg_conf is None and masks_sim is None:\n",
    "    print(\"Warning no segmentation config and no simulaltion segmentation\")\n",
    "    raise ValueError\n",
    "    \n",
    "print(\"loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Compute Keep Steps \n",
    "\n",
    "Decide which frames to keep, this information is saved as a dictionary whos keys are the frame index.\n",
    "\n",
    "There are several possible sources of information to use when deciding which frames to keep.\n",
    "\n",
    "1. The waypoint names. If these are provided they show when a motion segment ends.\n",
    "2. Gripper action and state. Segment by the state of the gripper, also keep only those where gripper is stable.\n",
    "3. TCP motion. Find those frames where movement is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use actions here instead of state position recordings as these\n",
    "# are more direct and reliable, but in general try to use states\n",
    "# as these are less suceptible to problems\n",
    "gripper_actions = actions[:, -1]\n",
    "gripper_change_steps = np.where(np.diff(gripper_actions))[0].tolist()\n",
    "\n",
    "# divide sequence into steps, defined by gripper action\n",
    "segment_steps = np.zeros(num_frames)\n",
    "segment_steps[np.array(gripper_change_steps)+1] = 1\n",
    "segment_steps = np.cumsum(segment_steps).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo_trajectory_utils import get_demo_continous \n",
    "from demo_trajectory_utils import get_keep_from_wpnames, get_keep_from_gripper, get_keep_from_motion\n",
    "from demo_trajectory_utils import filter_by_anchors, filter_by_motions, check_names_grip\n",
    "from demo_trajectory_utils import set_trajectory_actions, set_anchors\n",
    "from demo_trajectory_utils import get_rel_motion\n",
    "\n",
    "# module reloading for when updating demo_trajectory_utils functions\n",
    "from importlib import reload\n",
    "import demo_trajectory_utils\n",
    "reload(demo_trajectory_utils)\n",
    "from demo_trajectory_utils import set_trajectory_actions\n",
    "\n",
    "is_continous = get_demo_continous(tcp_pos)\n",
    "keep_wpnames = get_keep_from_wpnames(wp_names)\n",
    "keep_edge = {0: dict(name=\"demo_start\"), int(max_frame):dict(name=\"demo_end\")}\n",
    "keep_cmb = copy.copy(keep_edge)\n",
    "\n",
    "if keep_wpnames:\n",
    "    # if all our waypoints have names, we can use these to segment\n",
    "    # the trajectory into individual steps.\n",
    "    print(\"Using waypoint names to segment trajectory\")\n",
    "    check_names_grip(wp_names, gripper_change_steps)\n",
    "    keep_gripper = get_keep_from_gripper(gripper_actions)\n",
    "    if anchors:\n",
    "        filter_rel = [a == \"rel\" for a in anchors]\n",
    "        filter_by_anchors(keep_wpnames, wp_names, filter_rel)\n",
    "    keep_cmb.update(keep_wpnames)\n",
    "    keep_cmb.update(keep_gripper)\n",
    "\n",
    "elif not is_continous:\n",
    "    # if we have a hand-crafted recording, which includes only steps for\n",
    "    # servoing\n",
    "    keep_all = list(range(len(tcp_pos)))\n",
    "    keep_cmb.update(keep_all)\n",
    "    keep_cmb.update(keep_gripper)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    # this case is a bit more complicated, we can use heuristics like low velocity\n",
    "    # to extract segment our trajectory.\n",
    "    keep_motion  = get_keep_from_motion(tcp_pos)\n",
    "    keep_gripper = get_keep_from_gripper(gripper_actions)\n",
    "\n",
    "    keep_cmb = keep_motion + keep_gripper\n",
    "    cmb_f1 = filter_by_gripper(keep_cmb)\n",
    "    cmb_f2 = filter_by_motion(cmb_f1)\n",
    "    keep_cmb = cmb_f2\n",
    "\n",
    "keep_cmb = {k: keep_cmb[k] for k in sorted(keep_cmb)}\n",
    "\n",
    "# postprocess keep frames    \n",
    "filter_by_motions(keep_cmb, tcp_pos, tcp_orn, gripper_actions)\n",
    "print(keep_cmb)\n",
    "set_trajectory_actions(keep_cmb, segment_steps, tcp_pos, tcp_orn, gripper_actions)\n",
    "set_anchors(keep_cmb, anchors)\n",
    "\n",
    "# save keep frames\n",
    "with open(keep_fn, 'w') as outfile:\n",
    "    json.dump(keep_cmb, outfile)\n",
    "print(\"Saved to\", keep_fn)\n",
    "\n",
    "#plot_motion_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep2plot(keep):\n",
    "    arr = np.zeros(len(actions))\n",
    "    arr[list(keep.keys())] = 1\n",
    "    return arr\n",
    "\n",
    "if interactive:\n",
    "    fig, (ax, ax2) = plt.subplots(2, 1)\n",
    "    line = ax.imshow(video_recording[0])\n",
    "    ax.set_axis_off()\n",
    "    #ax2.plot(gripper_width*10, label=\"grip raw\")\n",
    "    ax2.plot((gripper_actions+1)/2,\"--\", label=\"gripper action\")\n",
    "    ax2.plot(segment_steps/10, label=\"steps\")\n",
    "    ax2.plot(keep2plot(keep_wpnames), label=\"keep_wpnames\")\n",
    "    ax2.plot(keep2plot(keep_gripper), label=\"keep_gripper\")\n",
    "    ax2.plot(keep2plot(keep_edge), label=\"keep_edge\")\n",
    "    \n",
    "    if anchors:\n",
    "        ax2.plot([a == \"rel\" for a in anchors], \"--\", label=\"rel\")\n",
    "    \n",
    "    ax2.set_ylabel(\"value\")\n",
    "    ax2.set_xlabel(\"frame number\")\n",
    "    vline = ax2.axvline(x=2, color=\"k\")\n",
    "    ax2.legend()\n",
    "\n",
    "    def update(w):\n",
    "        if wp_names:\n",
    "            print(\"wp_name:\",wp_names[w])\n",
    "        if w in keep_cmb:\n",
    "            print(keep_cmb[w])\n",
    "            print(\"pos:\", tcp_pos[w],\"orn:\", tcp_orn[w])\n",
    "        vline.set_data([w, w], [0, 1])\n",
    "        line.set_data(video_recording[w])\n",
    "        fig.canvas.draw_idle()\n",
    "    slider_w = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, w=slider_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 TCP Stationary Filter\n",
    "\n",
    "There are to options here. The demonstration is recorded in a sparse way, this indicates that every frame should be kept, or the demonstration is recorded in a dense way, meaning that we need to figure out which frames we want to keep.\n",
    "\n",
    "In the case of dense demonstrations, a good heuristic to use is to use transitions in which there is slow robot motion indicates motion to a stable position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    vel_vec = np.diff(tcp_pos, axis=0)\n",
    "    vel_scl = np.linalg.norm(vel_vec, axis=1)\n",
    "\n",
    "    val, label =  vel_scl, \"velocity\"\n",
    "    fig, (ax, ax2) = plt.subplots(2, 1)\n",
    "    line = ax.imshow(video_recording[0])\n",
    "    ax.set_axis_off()\n",
    "    line1 = ax2.plot(tcp_pos[:,0], label=\"x\")\n",
    "    line2 = ax2.plot(tcp_pos[:,1], label=\"y\")\n",
    "    line3 = ax2.plot(tcp_pos[:,2], label=\"z\")\n",
    "    ax2.set_ylabel(\"position (x,y,z)\")\n",
    "    ax2.set_xlabel(\"frame number\")\n",
    "    ax2r = ax2.twinx()\n",
    "    ax2r.set_ylabel(\"velocity/threshold\")\n",
    "    line4 = ax2r.plot(val, label=label, color=\"b\")\n",
    "    #ax2r.axhline(y=vel_stable_threshold, linestyle=\"--\", color=\"k\")\n",
    "    vline = ax2.axvline(x=2, color=\"k\")\n",
    "    lns = line1+line2+line3+line4\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax2.legend(lns, labs)\n",
    "\n",
    "    #ax2.legend()\n",
    "\n",
    "    def update(w):\n",
    "        print(\"{} @ {} is {}\".format(label, w, val[w] if w<max_frame else \"?\"))\n",
    "        vline.set_data([w, w], [0, 1])\n",
    "        line.set_data(video_recording[w])\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    slider_w = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, w=slider_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Gripper Stationary Filter\n",
    "\n",
    "Gripper motion makes servoing difficult, filter out those frames where it moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gripper_unstable(gripper_pos, diff_t=.005, min_duration=5):\n",
    "    # check that the gripper velocity is below diff_t for at least min_duration\n",
    "    gripper_abs_vel = np.abs(np.diff(gripper_pos))\n",
    "    stable = np.concatenate(([True,], gripper_abs_vel < diff_t))\n",
    "       \n",
    "    grip_stable = []\n",
    "    grip_ends = []\n",
    "    for i in range(len(stable)):\n",
    "        snext = np.all(stable[i:min(i+min_duration, len(stable))])\n",
    "        grip_stable.append(snext)\n",
    "        if grip_stable[-2:] == [0, 1]:\n",
    "            grip_ends.append(i-1)\n",
    "            \n",
    "    # fix edge case, gripper dosen't stop in demo\n",
    "    if len(grip_ends) < len(gripper_change_steps):\n",
    "        grip_ends.append(max_frame)\n",
    "\n",
    "    grip_unstable = list(zip(gripper_change_steps, grip_ends))\n",
    "    return grip_unstable\n",
    "\n",
    "if is_continous:\n",
    "    grip_unstable_intervals = get_gripper_unstable(gripper_width)    \n",
    "else:\n",
    "    grip_unstable_intervals = get_gripper_unstable(gripper_width, min_duration=1)    \n",
    "print(\"grip_unstable\", grip_unstable_intervals)\n",
    "\n",
    "\n",
    "grip_stable_arr = np.ones(num_frames)\n",
    "for start,stop in grip_unstable_intervals:\n",
    "    grip_stable_arr[start:stop] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show when the gripping is done, depending on gripper motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gripper\n",
    "if interactive:\n",
    "    fig, (ax, ax2) = plt.subplots(2, 1)\n",
    "    line = ax.imshow(video_recording[0])\n",
    "    ax.set_axis_off()\n",
    "    line1 = ax2.plot((gripper_actions+1)/2,\"--\", label=\"gripper action\", color=\"r\")\n",
    "    ax2.set_ylabel(\"grip stable\")\n",
    "    ax2.set_xlabel(\"frame number\")\n",
    "    ax2r = ax2.twinx()\n",
    "    line3 = ax2r.plot(gripper_width, \"--\", label=\"gripper_widths\", color=\"b\")\n",
    "    vline = ax2.axvline(x=2, color=\"k\")\n",
    "    \n",
    "    line2 = ax2.plot(grip_stable_arr, label=\"grip stable\", color=\"g\")\n",
    "\n",
    "    lns = line1+line2+line3\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax2.legend(lns, labs)\n",
    "\n",
    "    def update(w):\n",
    "        print(\"{} @ {} is {}\".format(\"gripper_width\", w, gripper_width[w]))\n",
    "        vline.set_data([w, w], [0, 1])\n",
    "        line.set_data(video_recording[w])\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    slider_w = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, w=slider_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine information, filter out the unstable keyframes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# Option 1: trust grip unstable, filter out vel_stable\n",
    "def filter_grip_unstable(vel_stable, grip_unstable_intervals):\n",
    "    grip_stable_arr = np.ones(num_frames)\n",
    "    for start, stop in grip_unstable_intervals:\n",
    "        grip_stable_arr[start+1:stop+1] = False\n",
    "\n",
    "    vel_stable_filtered = []\n",
    "    for index in vel_stable:\n",
    "        print(index,grip_stable_arr[index],grip_stable_arr)\n",
    "        if grip_stable_arr[index]:\n",
    "            vel_stable_filtered.append(index)\n",
    "    return vel_stable_filtered, grip_stable_arr\n",
    "\n",
    "# Option 2: trust vel_stable, override grip_stable\n",
    "# this is probably a bit more reasonable.\n",
    "\n",
    "vel_stable, grip_stable_arr = filter_grip_unstable(keep_motion, grip_unstable_intervals)\n",
    "print(\"vel_stable\", vel_stable, \"(after filter with grip unstable)\")\n",
    "\n",
    "print(keep_wpnames.keys())\n",
    "keep_wp_name_grip, _ = filter_grip_unstable(list(keep_wpnames.keys()), grip_unstable_intervals)\n",
    "print(keep_wp_name_grip, _)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "def get_keep_cmb_sparse(gripper_change_steps, vel_stable, max_frame):\n",
    "    keep_cmb = {}\n",
    "    keep_cmb[0] = dict(name=\"demo_start\")\n",
    "    \n",
    "    # a) first update all stable\n",
    "    if keep_wp_name_grip:\n",
    "        for wpn in keep_wp_name_grip:\n",
    "            print()\n",
    "            keep_cmb[int(wpn)] = keep_wpnames[wpn]\n",
    "    else:\n",
    "        for vel_s in vel_stable:\n",
    "            keep_cmb[int(vel_s)] = dict(name=\"vel_stable\")\n",
    "    \n",
    "    \n",
    "    # b) then ovewrite with grips (order is important here)\n",
    "    keep_cmb.update(keep_gripper)\n",
    "        \n",
    "    keep_cmb[int(max_frame)] = dict(name=\"demo_end\")\n",
    "    keep_cmb = {k: keep_cmb[k] for k in sorted(keep_cmb)}\n",
    "    return keep_cmb\n",
    "\n",
    "print(\"keep_wp_names\", list(keep_wpnames.keys()))\n",
    "print(\"keep_gripper\", list(keep_gripper.keys()))\n",
    "keep_cmb = get_keep_cmb_sparse(gripper_change_steps, vel_stable, max_frame)    \n",
    "print(\"keep_cmb\", keep_cmb)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "stationary_dist_threshold = 0.020  # [m/iter]\n",
    "\n",
    "def filter_stationary(keep_cmb, stationary_dist_threshold):\n",
    "    remove_keys = []\n",
    "    prior_key = None\n",
    "    for key in keep_cmb:\n",
    "        if prior_key is None:\n",
    "            prior_key = key\n",
    "            continue\n",
    "        rel_motion = get_rel_motion(tcp_pos[prior_key], tcp_orn[prior_key],\n",
    "                                    tcp_pos[key], tcp_orn[key])\n",
    "        rel_dist = np.linalg.norm(rel_motion[0:3])\n",
    "        same_step = segment_steps[prior_key] == segment_steps[key]\n",
    "        if rel_dist < stationary_dist_threshold and same_step:\n",
    "            print(\"{} -> {}: {:.4f}\".format(prior_key, key, float(rel_dist)),\"(removing)\")\n",
    "            remove_keys.append(prior_key)\n",
    "        else:\n",
    "            prior_key = key\n",
    "            \n",
    "    keep_keys = keep_cmb.keys() - remove_keys\n",
    "    keep_cmb_filtered = { key: keep_cmb[key] for key in keep_keys}\n",
    "    return keep_cmb_filtered\n",
    "        \n",
    "keep_cmb = filter_stationary(keep_cmb, stationary_dist_threshold)\n",
    "print(\"keep_cmb\", list(keep_cmb.keys()), \"(after filter stationary)\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. C. Verify keep frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep2plot2(keep):\n",
    "    arr = np.zeros(len(actions))\n",
    "    for k,v in keep.items():\n",
    "        if v[\"grip_dist\"] < 2:\n",
    "            arr[k] = 1.0\n",
    "        else:\n",
    "            arr[k] = 0.8\n",
    "        if \"anchor\" in v and v[\"anchor\"] == \"rel\":\n",
    "            arr[k] = 0.1\n",
    "        #arr[k] = 1.0 - min(v[\"grip_dist\"], 5) * 0.1\n",
    "    return arr\n",
    "    \n",
    "\n",
    "if interactive:\n",
    "    fig, (ax, ax2) = plt.subplots(2, 1)\n",
    "    line = ax.imshow(video_recording[0])\n",
    "    ax.set_axis_off()\n",
    "    ax2.plot(gripper_width*10, label=\"grip raw\")\n",
    "    ax2.plot(segment_steps/10, label=\"steps\")\n",
    "    ax2.plot(keep2plot2(keep_cmb), label=\"keep\")\n",
    "    #ax2.plot((gripper_actions+1)/2, label=\"gripper action\")\n",
    "    if anchors:\n",
    "        ax2.plot([a == \"rel\" for a in anchors], label=\"rel\")\n",
    "\n",
    "    ax2.set_ylabel(\"value\")\n",
    "    ax2.set_xlabel(\"frame number\")\n",
    "    vline = ax2.axvline(x=2, color=\"k\")\n",
    "    ax2.legend()\n",
    "\n",
    "    def update(w):\n",
    "        if wp_names:\n",
    "            print(\"frame name:\", wp_names[w])\n",
    "        vline.set_data([w, w], [0, 1])\n",
    "        line.set_data(video_recording[w])\n",
    "        fig.canvas.draw_idle()\n",
    "        if w in keep_cmb:\n",
    "            print(keep_cmb[w])\n",
    "            print()\n",
    "    slider_w = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, w=slider_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compute Mask from Color Images\n",
    "\n",
    "Mask out the foreground object so that foreground specific flow can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if seg_conf is None:\n",
    "    print(f\"Skipping color segmentation, config file not found:\\n{segment_conf_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Compute Mask from Color Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "from scipy import ndimage\n",
    "from demo_segment_util import mask_color, erode_mask, label_mask, mask_center\n",
    "\n",
    "# create a segmentation mask\n",
    "def get_mask(frame, step_conf, depth=None):\n",
    "    \"\"\"\n",
    "    create segmentation mask for single frame\n",
    "    Args:\n",
    "        frame: input frame w x h x 3 [0,255] array\n",
    "        i: index of frame, for indexing parameters\n",
    "        threshold: threshold for color\n",
    "        \n",
    "    Returns:\n",
    "        mask: binary numpy array, with True == keep\n",
    "    \"\"\"\n",
    "    threshold = step_conf[0][\"threshold\"]    \n",
    "    image = frame.copy()\n",
    "    \n",
    "    for seg_option in step_conf:\n",
    "        name = seg_option[\"name\"]\n",
    "        \n",
    "        if name == \"color\":\n",
    "            color_choice = seg_option[\"color\"]\n",
    "            mask = mask_color(image, color_choice=color_choice, threshold=threshold)\n",
    "            \n",
    "        elif name == \"erode\":\n",
    "            mask = erode_mask(mask)\n",
    "            \n",
    "        elif name == \"height\":\n",
    "            raise NotImplementedError\n",
    "            depth2 = transform_depth(depth, np.linalg.inv(T_tcp_cam))\n",
    "            mask2 = get_mask_depth(depth2, 600, 1550)\n",
    "            mask[mask2] = True\n",
    "    \n",
    "        elif name == \"labels\":\n",
    "            raise NotImplementedError\n",
    "            mask = ndimage.morphology.binary_closing(mask, iterations=4)\n",
    "            mask = label_mask(mask, i)\n",
    "    \n",
    "        elif name == \"imgheight\":\n",
    "            height_val = seg_option[\"height\"]\n",
    "            mask[:height_val, :] = False\n",
    "            \n",
    "        elif name == \"center\":\n",
    "            mask = mask_center(mask)\n",
    "            \n",
    "    return mask\n",
    "\n",
    "def get_cur_mask(i):\n",
    "    # mask according to current fg object\n",
    "    cur_step = segment_steps[i]\n",
    "    cur_obj = seg_conf[\"sequence\"][cur_step]\n",
    "    step_conf = seg_conf[\"objects\"][cur_obj]\n",
    "    mask = get_mask(video_recording[i], step_conf)\n",
    "    return mask\n",
    "\n",
    "# Plot\n",
    "if seg_conf and interactive:\n",
    "    print(\"Colored stuff is keept (mask==True)\")\n",
    "    print(\"gripper_change_steps:\", gripper_change_steps)\n",
    "    print(\"segments: \", len(seg_conf))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    line = ax.imshow(video_recording[0])\n",
    "    ax.set_axis_off()\n",
    "    prev_step = 0\n",
    "    def update(i, t):\n",
    "        cur_step = segment_steps[i]\n",
    "        cur_obj = seg_conf[\"sequence\"][cur_step]\n",
    "        global prev_step\n",
    "        if cur_step != prev_step:\n",
    "            # don't change order here, without double checking\n",
    "            saved_t = seg_conf[\"objects\"][cur_obj][0][\"threshold\"]\n",
    "            print(f\"switching step {prev_step} -> {cur_step}, loading t={saved_t}\")\n",
    "            prev_step = cur_step\n",
    "            slider_t.value = saved_t*100\n",
    "        else:\n",
    "            seg_conf[\"objects\"][cur_obj][0][\"threshold\"] = t/100\n",
    "            \n",
    "        mask = get_cur_mask(i)\n",
    "        image = video_recording[i].copy()\n",
    "        image[np.logical_not(mask)] = 255, 255, 255\n",
    "        line.set_data(image)\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    slider_i = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    first_obj = seg_conf[\"sequence\"][0]\n",
    "    slider_t = widgets.IntSlider(min=0, max=100, step=1, value=seg_conf[\"objects\"][first_obj][0][\"threshold\"]*100,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, i=slider_i, t=slider_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if seg_conf:\n",
    "    switch_frame = gripper_change_steps\n",
    "    print(\"switching at:\", switch_frame)\n",
    "    if orig_seg_conf is None:\n",
    "        orig_seg_conf = seg_conf\n",
    "\n",
    "    \"\"\"\n",
    "    # display changes files to selected.\n",
    "    for seg_option, seg_option_orig in zip(seg_conf, orig_seg_conf):\n",
    "        c = seg_option[0][\"color\"]\n",
    "        t = seg_option[0][\"threshold\"]\n",
    "        t_i = seg_option_orig[0][\"threshold\"]\n",
    "        if t != t_i:\n",
    "            print(\"c={}, t={} | t'={}\".format(c, t, t_i))\n",
    "        else:\n",
    "            print(\"c={}, t={}\".format(c, t))\n",
    "    \"\"\"\n",
    "    obj_ids = {}\n",
    "    obj_ids_list = []\n",
    "    for i, obj in enumerate(seg_conf[\"objects\"]):\n",
    "        obj_ids[obj] = i+1\n",
    "        obj_ids_list.append(i+1)\n",
    "        print(f\"{obj} -> {i+1}\")\n",
    "\n",
    "\n",
    "    fg_obj = []\n",
    "    masks_list = []\n",
    "    for i in tqdm(range(len(video_recording))):\n",
    "        # get foreground object\n",
    "        cur_step = segment_steps[i]\n",
    "        cur_obj = seg_conf[\"sequence\"][cur_step]\n",
    "        fg_obj.append(obj_ids[cur_obj])\n",
    "\n",
    "        m_masks = []\n",
    "        for obj_name in seg_conf[\"objects\"]:\n",
    "            mask = get_mask(video_recording[i], seg_conf[\"objects\"][obj_name])\n",
    "            m_masks.append(mask)\n",
    "\n",
    "        overlapp = np.sum(m_masks, axis=0) > 1\n",
    "        if np.any(overlapp):\n",
    "            print(\"WARNING: There is overlapp.\")\n",
    "        masks_list.append(m_masks)\n",
    "\n",
    "    fg_obj = np.array(fg_obj)\n",
    "    assert fg_obj.ndim == 1\n",
    "\n",
    "    masks_list = np.array(masks_list)\n",
    "    masks_list = masks_list.transpose(1, 0, 2, 3).astype(np.uint8)\n",
    "    obj_ids_arr = np.array(obj_ids_list).reshape(-1, 1, 1, 1)\n",
    "    masks_list = obj_ids_arr*masks_list\n",
    "    masks_list = masks_list.sum(axis=0)\n",
    "    \n",
    "    np.savez_compressed(mask_fn, mask=masks_list, fg=fg_obj)\n",
    "    print(\"Saved to\", mask_fn)\n",
    "\n",
    "    if seg_conf != orig_seg_conf:\n",
    "        print(\"Warning using new segmentation config values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 B. Check Masks from Color with Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if seg_conf and masks_sim is not None:\n",
    "    for i in range(num_frames):\n",
    "        image = video_recording[i].copy()\n",
    "        mask = masks_list[i] == fg_obj[i]\n",
    "        # mask segmentation mask(gt) with fg mask (computed)\n",
    "        ma = np.ma.array(masks_sim[i], mask=np.logical_not(mask))\n",
    "        ma_unique = np.unique(ma, return_counts=True)\n",
    "        # unique is sorted by size, pick the biggest\n",
    "        idx_largest = np.where(ma_unique[0])[-1][0]\n",
    "        seg_id, mask_count = ma_unique[0][idx_largest], ma_unique[1][idx_largest]\n",
    "        seg_count = np.sum(masks_sim[i] == seg_id)\n",
    "        # test how much we segmented / how much there is\n",
    "        score = mask_count / seg_count\n",
    "        assert score > .9\n",
    "        \n",
    "    print(\"Segmentation test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Compute Masks from Simulation\n",
    "\n",
    "This cell extracts foreground masks from simulation recordings. It does this by looking at the recordings info variables, where a anchor object UID can be specified. This is usually done by the task policy.\n",
    "\n",
    "The move_anchor is the object with which we are moving relative to, this is most often but not always the object of interest or the foreground object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if masks_sim is not None and anchors and not seg_conf:\n",
    "    fg_obj_sim = np.array([rec_el.data[\"info\"].item()[\"move_anchor\"] for rec_el in rec])\n",
    "    fg_obj_sim = np.array([int(fg) if fg!=\"rel\" else -1 for fg in fg_obj_sim])\n",
    "    np.savez_compressed(mask_fn, mask=masks_sim, fg=fg_obj_sim)\n",
    "    print(\"Saved to\", mask_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "if masks_sim is not None and anchors and interactive:\n",
    "    fig, ax = plt.subplots(1)\n",
    "    handle = ax.imshow(video_recording[0])\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    def update(i):\n",
    "        image = video_recording[i].copy()\n",
    "        \n",
    "        # this is the code used by the seroving module, so don't change.\n",
    "        mask = masks_sim[i] == fg_obj_sim[i]\n",
    "        \n",
    "        print(round(np.mean(mask)*100), \"% fg, mask shape\", mask.shape)\n",
    "        image[np.logical_not(mask)] = 255, 255, 255\n",
    "        handle.set_data(image)\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    slider_i2 = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, i=slider_i2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  interactive:\n",
    "    tmp = np.load(mask_fn)\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "    handle = ax.imshow(tmp[\"mask\"][0] == tmp[\"fg\"][0])\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    def update(i):\n",
    "        image = video_recording[i].copy()\n",
    "        mask = tmp[\"mask\"][i] == tmp[\"fg\"][i]\n",
    "        print(round(np.mean(mask)*100), \"% fg, mask shape\", mask.shape)\n",
    "        image[np.logical_not(mask)] = 255, 255, 255\n",
    "        handle.set_data(image)\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    slider_i2 = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, i=slider_i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.load(mask_fn)\n",
    "for k, info in keep_cmb.items():\n",
    "    mask = tmp[\"mask\"][k] == tmp[\"fg\"][k]\n",
    "    if info[\"anchor\"] == \"rel\":\n",
    "        pixels_segmented = np.sum(mask==True)\n",
    "        if pixels_segmented > 0:\n",
    "            print(f\"Warning: Keyframe {k}: segmentation given for relative motion.\")\n",
    "    else:\n",
    "        percent_segmented = np.mean(mask==True)\n",
    "        if percent_segmented < .01:\n",
    "            print(f\"Warning: Keyframe {k}: low fraction of image segmented for keyframe {round(percent_segmented*100)}\")\n",
    "    \n",
    "print(f\"Checking {mask_fn} passed.\")                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Masking based on Depth"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#episode_data = np.load(recording_fn)\n",
    "#keys = list(episode_data.keys())\n",
    "#camera_calibration = dict(width=640,height=480,\n",
    "#                     fx = 617.8902587890625, fy=617.8903198242188, \n",
    "#                     ppx=315.20367431640625, ppy=245.70614624023438 )\n",
    "#\n",
    "#T_tcp_cam = np.array([\n",
    "#    [0.99987185, -0.00306941, -0.01571176, 0.00169436],\n",
    "#    [-0.00515523, 0.86743151, -0.49752989, 0.11860651],\n",
    "#    [0.015156,    0.49754713,  0.86730453, -0.18967231],\n",
    "#    [0., 0., 0., 1.]])\n",
    "#\n",
    "#depth = episode_data[\"depth_imgs\"]\n",
    "#depth_scale = 8000\n",
    "#i = 200\n",
    "#print(\"loaded.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#from demon_segment_util import transform_depth\n",
    "#\n",
    "#depth_flat = transform_depth(depth[i], np.linalg.inv(T_tcp_cam))\n",
    "#fig, (ax, ax2) = plt.subplots(1, 2)\n",
    "#line = ax.imshow(depth_flat)\n",
    "#ax2.plot(np.sort(depth[i].flatten()))\n",
    "#ax2.plot(np.sort(depth_flat.flatten()))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#def get_mask_depth(frame, l, h):\n",
    "#    mask = np.logical_or(frame < l/depth_scale, frame > h/depth_scale)\n",
    "#    mask = np.logical_not(mask)\n",
    "#    return mask\n",
    "#\n",
    "#def erode_mask(mask):\n",
    "#    return mask\n",
    "#    mask = ndimage.binary_closing(mask, iterations=5)\n",
    "#    mask = ndimage.morphology.binary_erosion(mask, iterations=10)\n",
    "#    return mask\n",
    "#\n",
    "#\n",
    "#x = np.linspace(0, 2*np.pi)\n",
    "#fig, ax = plt.subplots(1)\n",
    "#line = ax.imshow(video_recording[0])\n",
    "#\n",
    "#def update(w,l,h):\n",
    "#    depth2 = transform_depth(depth[w], np.linalg.inv(T_tcp_cam))\n",
    "#    mask = get_mask_depth(depth2, l, h)\n",
    "#    mask = erode_mask(mask)\n",
    "#    mask = np.logical_not(mask)\n",
    "#    display_image = video_recording[w].copy()\n",
    "#    display_image[mask] = 0\n",
    "#    line.set_data(display_image)\n",
    "#    fig.canvas.draw_idle()\n",
    "#    \n",
    "#depth_min, depth_max = int(depth.min()*depth_scale), int(depth.max()*depth_scale)\n",
    "#slider_w = widgets.IntSlider(min=0, max=max_frame, step=1, value=205,\n",
    "#                             layout=Layout(width='70%'))\n",
    "#slider_l = widgets.IntSlider(min=depth_min, max=depth_max, step=1, value=1560,\n",
    "#                             layout=Layout(width='70%'))\n",
    "#slider_h = widgets.IntSlider(min=depth_min, max=depth_max, step=1, value=1650,\n",
    "#                             layout=Layout(width='70%'))\n",
    "#\n",
    "#interact(update, w=slider_w, l=slider_l, h=slider_h)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# next steps: anneal the edge, and run connected component algorithm.\n",
    "#from scipy import ndimage\n",
    "#\n",
    "#w = slider_w.value\n",
    "#l = slider_l.value\n",
    "#h = slider_h.value\n",
    "#print(\"w={w}, l={l}, h={h}\".format(w=w, l=l, h=h))\n",
    "#\n",
    "#depth2 = transform_depth(depth[w], np.linalg.inv(T_tcp_cam))\n",
    "#mask_s = get_mask_depth(depth2, l, h)\n",
    "#mask_s = erode_mask(mask_s.copy())\n",
    "#mask_s = np.logical_not(mask_s)\n",
    "#display_image = video_recording[w].copy()\n",
    "#display_image[mask_s] = 0\n",
    "#\n",
    "#fig, ax = plt.subplots(1)\n",
    "#line = ax.imshow(display_image)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#threshold_low = slider_l.value\n",
    "#threshold_high = slider_h.value\n",
    "#\n",
    "#masks = np.zeros(video_recording.shape[:3], dtype=bool)\n",
    "#\n",
    "#for i in range(len(video_recording)):\n",
    "#    mask = get_mask_depth(depth[i], threshold_low, threshold_high)\n",
    "#    mask = erode_mask(mask)\n",
    "#    masks[i] = mask\n",
    "#\n",
    "#print(np.mean(masks)*100, \"percent of pixels fg\")\n",
    "#mask_fn = recording_fn.replace(\".npz\", \"_mask.npz\")\n",
    "#np.savez_compressed(mask_fn, mask=masks)\n",
    "#print(\"Saved to\", mask_fn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#fig, ax = plt.subplots(1, 1)\n",
    "#line = ax.imshow(masks[25])\n",
    "#ax.set_axis_off()\n",
    "#\n",
    "#def update(i):\n",
    "#    image = video_recording[i].copy()\n",
    "#    mask = masks[i]\n",
    "#    image[mask] = 255, 255, 255\n",
    "#    line.set_data(image)\n",
    "#    fig.canvas.draw_idle()\n",
    "#    \n",
    "#slider_i2 = widgets.IntSlider(min=0, max=max_frame, step=1, value=200,\n",
    "#                             layout=Layout(width='70%'))\n",
    "#\n",
    "#interact(update, i=slider_i2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
