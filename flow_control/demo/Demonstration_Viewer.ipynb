{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration View\n",
    "\n",
    "1. Compute Keep Steps\n",
    "    1. Trajectory Analysis\n",
    "    2. Compute Keep Steps\n",
    "    3. Verify\n",
    "2. Compute Masks\n",
    "    1. Compute Mask\n",
    "    2. Verify\n",
    "\n",
    "\n",
    "\n",
    "View a demonstration by sliding through the frames and generate foreground segmentation.\n",
    "\n",
    "This script creates the files that flowcontrol requires: `episode_0_keep.npz`, `episode_0.json`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "interactive = isnotebook()  # becomes overwritten\n",
    "if interactive:\n",
    "    get_ipython().run_line_magic('matplotlib', 'notebook')\n",
    "    from ipywidgets import *\n",
    "    import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# default parameter values\n",
    "segment_height = False\n",
    "segment_labels = False  # this seems deprecated\n",
    "segment_imgheight = False\n",
    "segment_center = False\n",
    "segment_erode = False\n",
    "keep_frames_method = \"sparse\"\n",
    "demonstration_type = None  # leave as None, set automatically\n",
    "\n",
    "if interactive:    \n",
    "    # set parameters here\n",
    "    recording, episode_num = \"./tmp_test/pick_n_place/\", 0\n",
    "    conf = [ (dict(name=\"color\", color=(0, 0, 1), threshold=.65), dict(name=\"center\")),\n",
    "             (dict(name=\"color\", color=(1, 0, 0), threshold=.90), dict(name=\"center\")),\n",
    "             (dict(name=\"color\", color=(1, 0, 0), threshold=.90), dict(name=\"center\"))]\n",
    "    \n",
    "    recording, episode_num = \"/media/kuka/Seagate Expansion Drive/kuka_recordings/flow/multi2\", 1\n",
    "    conf = [ (dict(name=\"color\", color=(1, 1, 0), threshold=.90), dict(name=\"center\")),\n",
    "             (dict(name=\"color\", color=\"bw\", threshold=.90), dict(name=\"center\")) ]\n",
    "    #conf = [ (dict(name=\"color\", color=(200/255, 230/255, 220/255), threshold=.90), dict(name=\"center\")),\n",
    "    #         (dict(name=\"color\", color=\"bw\", threshold=.90), dict(name=\"center\")) ]\n",
    "    \n",
    "else:\n",
    "    # expect commandline input\n",
    "    import sys\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: Demonstration_Viewer.py <episode_dir> <episode_num>\")\n",
    "    recording = sys.argv[1]\n",
    "    episode_num = int(sys.argv[2])\n",
    "    \n",
    "    with open(os.path.join(recording, \"segment_conf.json\"), \"r\") as f_obj:\n",
    "        conf = json.load(f_obj)\n",
    "    print(conf)\n",
    "\n",
    "\n",
    "recording_fn = os.path.join(recording, \"episode_{}.npz\".format(episode_num))\n",
    "recording_dict = np.load(recording_fn)\n",
    "state_recording = recording_dict[\"robot_state_full\"]\n",
    "actions = recording_dict[\"actions\"]\n",
    "ee_positions = state_recording[:, :3]\n",
    "video_recording = recording_dict[\"rgb_unscaled\"]\n",
    "try:\n",
    "    seg_masks = recording_dict[\"seg_masks\"]\n",
    "except (KeyError,ValueError):\n",
    "    seg_masks = None\n",
    "\n",
    "num_frames = video_recording.shape[0]\n",
    "max_frame = num_frames-1\n",
    "print(\"loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Compute Keep Steps \n",
    "\n",
    "Decide which frames to keep, saved as per-frame boolean array.\n",
    "\n",
    "Various options are possible, current choice is find frames where movement is minimal.\n",
    "\n",
    "Then look at gripper motion and keep only those where gripper is stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRIPPER_OPEN, GRIPPER_CLOSE = 1.0, -1.0  # assume normalized actions\n",
    "\n",
    "# use actions here instead of state position recordings as these\n",
    "# are more direct and reliable\n",
    "gr_actions = actions[:, -1]\n",
    "keysteps = np.where(np.diff(gr_actions))[0].tolist()\n",
    "keystep_names = [\"\" for _ in range(len(keysteps))]\n",
    "\n",
    "# divide sequence into steps, defined by gripper action\n",
    "segment_steps = np.zeros(num_frames)\n",
    "segment_steps[np.array(keysteps)+1] = 1\n",
    "segment_steps = np.cumsum(segment_steps).astype(int)\n",
    "\n",
    "if demonstration_type is None:\n",
    "    demonstration_type = [\"navigate\", \"grasp\", \"grasp_insert\"][len(keysteps)]\n",
    "else:\n",
    "    demonstration_type_auto = [\"navigate\", \"grasp\", \"grasp_insert\"][len(keysteps)]\n",
    "    assert demonstration_type == demonstration_type_auto\n",
    "\n",
    "def check_gripper_opens(idx):\n",
    "    # check that we transition open->close & iter segment\n",
    "    assert gr_actions[idx] == GRIPPER_OPEN\n",
    "    assert gr_actions[idx+1] == GRIPPER_CLOSE\n",
    "    assert segment_steps[idx+1] == segment_steps[idx] + 1  # next\n",
    "\n",
    "def check_gripper_closes(idx):\n",
    "    # check that we transition open->close & iter segment\n",
    "    assert gr_actions[idx] == GRIPPER_CLOSE\n",
    "    assert gr_actions[idx+1] == GRIPPER_OPEN\n",
    "    assert segment_steps[idx+1] == segment_steps[idx] + 1  # next\n",
    "\n",
    "# run some checks\n",
    "if demonstration_type == \"grasp\":\n",
    "    assert(len(keysteps) == 1)\n",
    "    check_gripper_opens(keysteps[0])\n",
    "    keystep_names[0] = \"gripper_open\"    \n",
    "elif demonstration_type == \"grasp_insert\":\n",
    "    assert(len(keysteps) == 2)\n",
    "    check_gripper_opens(keysteps[0])\n",
    "    check_gripper_closes(keysteps[1])\n",
    "    keystep_names[0] = \"gripper_open\"\n",
    "    keystep_names[1] = \"gripper_close\"\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "print(\"demonstration type:\", demonstration_type)\n",
    "for kn, kidx in zip(keystep_names, keysteps):\n",
    "    print(kn, \"@\", kidx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gripper Transtions\n",
    "\n",
    "Gripper motion makes servoing difficult, filter out those frames where it moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gripper_transitions(gripper_pos, diff_t=.0005, time_t=5):\n",
    "    gripper_abs_vel = np.abs(np.diff(gripper_pos))\n",
    "    stable = gripper_abs_vel < diff_t\n",
    "    grip_stable = []\n",
    "    grip_ends = []\n",
    "    for i in range(len(stable)):\n",
    "        snext = np.all(stable[i:min(i+time_t, len(stable))])\n",
    "        grip_stable.append(snext)\n",
    "        if grip_stable[-2:] == [0, 1]:\n",
    "            grip_ends.append(i-1)\n",
    "\n",
    "    return grip_ends, grip_stable\n",
    "\n",
    "grip_ends, grip_stable = get_gripper_transitions(state_recording[:, -2])\n",
    "\n",
    "# fix edge case, gripper dosen't stop in demo\n",
    "if len(grip_ends) < len(keysteps):\n",
    "    grip_ends.append(max_frame)    \n",
    "assert len(keysteps) == len(grip_ends)\n",
    "\n",
    "grip_unstable = list(zip(keysteps, grip_ends))\n",
    "\n",
    "print(\"grip_unstable\", grip_unstable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movement Transitions\n",
    "\n",
    "Slow robot motion indicates motion to a stable position, which we want to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_threshold = .002  # [m/s]?\n",
    "\n",
    "def get_stable_points(pos_vec):\n",
    "    vel_vec = np.diff(pos_vec, axis=0)\n",
    "    vel_scl = np.linalg.norm(vel_vec, axis=1)\n",
    "\n",
    "    # This first loop gets minimal regions\n",
    "    active = False\n",
    "    start, stop = -1, -1\n",
    "    min_regions = []\n",
    "    for i in range(len(vel_scl)):\n",
    "        if vel_scl[i] < vel_threshold:\n",
    "            if active:\n",
    "               stop = i\n",
    "            else:\n",
    "                active = True\n",
    "                start, stop = i, i\n",
    "        else:\n",
    "            if active:\n",
    "                min_regions.append((start, stop))\n",
    "                active = False\n",
    "                start, stop = -1, -1\n",
    "\n",
    "    # This second loop gets minimal value\n",
    "    vel_stable = []\n",
    "    for start, stop in min_regions:\n",
    "        try:\n",
    "            min_idx = start + 1 + np.argmin(vel_scl[start:stop])\n",
    "        except ValueError:\n",
    "            min_inde = 0\n",
    "        if len(vel_stable) == 0 or vel_stable[-1] != min_idx:\n",
    "            vel_stable.append(min_idx)\n",
    "    return vel_stable, vel_scl\n",
    "\n",
    "vel_stable, vel_scl = get_stable_points(state_recording[:, :3])\n",
    "print(\"vel_stable\", vel_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: trust grip unstable, filter out vel_stable\n",
    "grip_stable_arr = np.ones(num_frames)\n",
    "for start, stop in grip_unstable:\n",
    "    grip_stable_arr[start:stop] = False\n",
    "vel_stable_filtered = []\n",
    "for index in vel_stable:\n",
    "    if grip_stable_arr[index]:\n",
    "        vel_stable_filtered.append(index)\n",
    "vel_stable = vel_stable_filtered\n",
    "print(\"vel_stable\", vel_stable, \"(filtered with grip unstable)\")\n",
    "\n",
    "# Option 2: trust vel_stable, override grip_stable\n",
    "# this is probably a bit more reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keep_dict_sparse():\n",
    "    keep_dict = {}\n",
    "    keep_dict[0] = dict(name=\"demo_start\")\n",
    "    #keep_dict[0]['pre'] = dict(abs=state_recording[key].tolist())\n",
    "    \n",
    "    for k_idx, k_name in zip(keysteps, keystep_names):\n",
    "        keep_dict[k_idx] = dict(name=k_name)\n",
    "        \n",
    "    for vel_s in vel_stable:\n",
    "        keep_dict[int(vel_s)] = dict(name=\"vel_stable\")\n",
    "        \n",
    "    keep_dict[int(max_frame)] = dict(name=\"demo_end\")\n",
    "    return keep_dict\n",
    "\n",
    "# decide which frames to keep, after gripping mask a few steps\n",
    "if keep_frames_method == \"all\":\n",
    "    raise NotImplemetedError\n",
    "    #keep_array = np.ones(num_frames, dtype=bool)\n",
    "elif keep_frames_method == \"dense\":\n",
    "    raise NotImplemetedError\n",
    "    #keep_array = np.ones(num_frames, dtype=bool)\n",
    "    #grip_step = keysteps[0]\n",
    "    #gripper_close_steps = 30\n",
    "    #keep_array[grip_step:grip_step+gripper_close_steps] = False\n",
    "elif keep_frames_method == \"sparse\":\n",
    "    keep_dict = get_keep_dict_sparse()\n",
    "else:\n",
    "    raise ValueError\n",
    "    \n",
    "# sort dictionary\n",
    "keep_dict = {k: keep_dict[k] for k in sorted(keep_dict)}\n",
    "print(\"keep_dict\", list(keep_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "dist_threshold = 0.020\n",
    "\n",
    "def get_rel_motion(start, finish):\n",
    "    # position\n",
    "    pos_diff = finish[:3] - start[:3]\n",
    "    ord_diff = R.from_quat(finish[3:7]).inv() * R.from_quat(start[3:7])\n",
    "    #assert ord_diff.magnitude() < .35, ord_diff.magnitude() # for now\n",
    "    return pos_diff.tolist() + ord_diff.as_quat().tolist()\n",
    "\n",
    "\n",
    "remove_keys = []\n",
    "prior_key = None\n",
    "for key in keep_dict:\n",
    "    if prior_key is None:\n",
    "        prior_key = key\n",
    "        continue\n",
    "    rel_motion = get_rel_motion(state_recording[prior_key],\n",
    "                                state_recording[key])\n",
    "    rel_dist = np.linalg.norm(rel_motion[0:3])\n",
    "    same_step = segment_steps[prior_key] == segment_steps[key]\n",
    "    if rel_dist < dist_threshold and same_step:\n",
    "        print(\"{} -> {}: {:.4f}\".format(prior_key, key, float(rel_dist)),\"(removing)\")\n",
    "        remove_keys.append(prior_key)\n",
    "    else:\n",
    "        prior_key = key\n",
    "\n",
    "remove_keys = list(set(remove_keys))\n",
    "\n",
    "for key in remove_keys:\n",
    "    del keep_dict[key]\n",
    "    \n",
    "print(\"keep_dict\", list(keep_dict.keys()), \"(filtered with dist)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate backward and save dist to grasp\n",
    "max_dist = 10\n",
    "step_since_grasp = max_dist\n",
    "for key in reversed(sorted(keep_dict)):\n",
    "    name = keep_dict[key][\"name\"]\n",
    "    if name.startswith(\"gripper_\"):\n",
    "        step_since_grasp = 0\n",
    "    else:\n",
    "        step_since_grasp = min(step_since_grasp+1, max_dist)\n",
    "    keep_dict[key][\"grip_dist\"] = step_since_grasp\n",
    "    \n",
    "prior_key = None\n",
    "for key in sorted(keep_dict):\n",
    "    if prior_key is None:\n",
    "        prior_key = key\n",
    "        continue\n",
    "    pre_dict = {}\n",
    "    \n",
    "    same_segment = segment_steps[key] == segment_steps[prior_key]\n",
    "    if not same_segment:\n",
    "        pre_dict[\"grip\"] = gr_actions[key]\n",
    "    \n",
    "    if keep_dict[prior_key][\"grip_dist\"] < 2:\n",
    "        rel_motion = get_rel_motion(state_recording[prior_key],\n",
    "                                    state_recording[key])\n",
    "        pre_dict[\"rel\"] = rel_motion\n",
    "    else:\n",
    "        abs_motion = state_recording[key].tolist()\n",
    "        pre_dict[\"abs\"] = abs_motion\n",
    "        \n",
    "    keep_dict[key][\"pre\"] = pre_dict\n",
    "    prior_key = key\n",
    "\n",
    "    \n",
    "# double check that we retain all keep steps\n",
    "assert(np.all([k in keep_dict.keys() for k in keysteps]))\n",
    "\n",
    "keep_fn = recording_fn.replace(\".npz\", \"_keep.json\")\n",
    "with open(keep_fn, 'w') as outfile:\n",
    "    json.dump(keep_dict, outfile)\n",
    "print(\"Saved to\", keep_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. C. Verify keep frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    keep_array = np.zeros(segment_steps.shape)\n",
    "    keep_array[sorted(keep_dict.keys())] = True\n",
    "    fig, (ax, ax2) = plt.subplots(2, 1)\n",
    "    line = ax.imshow(video_recording[0])\n",
    "    ax.set_axis_off()\n",
    "    ax2.plot(state_recording[:, -2]*10, label=\"grip raw\")\n",
    "    ax2.plot(segment_steps/10, label=\"steps\")\n",
    "    ax2.plot(keep_array, label=\"keep\")\n",
    "    ax2.plot((gr_actions+1)/2, label=\"gripper action\")\n",
    "    ax2.set_ylabel(\"value\")\n",
    "    ax2.set_xlabel(\"frame number\")\n",
    "    vline = ax2.axvline(x=2, color=\"k\")\n",
    "    ax2.legend()\n",
    "\n",
    "    def update(w):\n",
    "        vline.set_data([w, w], [0, 1])\n",
    "        line.set_data(video_recording[w])\n",
    "        fig.canvas.draw_idle()\n",
    "        if w in keep_dict:\n",
    "            print(keep_dict[w])\n",
    "            print()\n",
    "    slider_w = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, w=slider_w)\n",
    "\n",
    "    print(\"What I want to know: do I servo y/n, do I translate?\")\n",
    "    # Convert this keep_array stuff into a dict\n",
    "    # then do one iteration of look ahead to set a servoing flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Gripper Motion\n",
    "Show when the gripping is done, depending on gripper motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gripper\n",
    "if interactive:\n",
    "    val, label = state_recording[:, -2], \"gripper_pos\"\n",
    "    fig, (ax, ax2) = plt.subplots(2, 1)\n",
    "    line = ax.imshow(video_recording[0])\n",
    "    ax.set_axis_off()\n",
    "    line1 = ax2.plot((gr_actions+1)/2, label=\"gripper action\", color=\"r\")\n",
    "    line2 = ax2.plot(grip_stable, label=\"grip stable\")\n",
    "    ax2.set_ylabel(\"value\")\n",
    "    ax2.set_xlabel(\"frame number\")\n",
    "    ax2r = ax2.twinx()\n",
    "    line3 = ax2r.plot(val, label=label, color=\"b\")\n",
    "    vline = ax2.axvline(x=2, color=\"k\")\n",
    "    lns = line1+line2+line3\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax2.legend(lns, labs)\n",
    "\n",
    "    def update(w):\n",
    "        print(\"{} @ {} is {}\".format(label, w, val[w]))\n",
    "        vline.set_data([w, w], [0, 1])\n",
    "        line.set_data(video_recording[w])\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    slider_w = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, w=slider_w)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Velocities\n",
    "Look at the end effector motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    val, label =  vel_scl, \"velocity\"\n",
    "    fig, (ax, ax2) = plt.subplots(2, 1)\n",
    "    line = ax.imshow(video_recording[0])\n",
    "    ax.set_axis_off()\n",
    "    line1 = ax2.plot(state_recording[:,0], label=\"x\")\n",
    "    line2 = ax2.plot(state_recording[:,1], label=\"y\")\n",
    "    line3 = ax2.plot(state_recording[:,2], label=\"z\")\n",
    "    ax2.set_ylabel(\"value\")\n",
    "    ax2.set_xlabel(\"frame number\")\n",
    "    ax2r = ax2.twinx()\n",
    "    line4 = ax2r.plot(val, label=label, color=\"b\")\n",
    "    ax2r.axhline(y=vel_threshold, linestyle=\"--\", color=\"k\")\n",
    "    vline = ax2.axvline(x=2, color=\"k\")\n",
    "    lns = line1+line2+line3+line4\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax2.legend(lns, labs)\n",
    "\n",
    "    #ax2.legend()\n",
    "\n",
    "    def update(w):\n",
    "        print(\"{} @ {} is {}\".format(label, w, val[w]))\n",
    "        vline.set_data([w, w], [0, 1])\n",
    "        line.set_data(video_recording[w])\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    slider_w = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, w=slider_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compute Mask\n",
    "\n",
    "Mask out the foreground object so that foreground specific flow can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "from scipy import ndimage\n",
    "from demo_segment_util import mask_color, erode_mask, label_mask, mask_center\n",
    "\n",
    "# create a segmentation mask\n",
    "def get_mask(frame, i=None, threshold=0):\n",
    "    \"\"\"\n",
    "    create segmentation mask for single frame\n",
    "    Args:\n",
    "        frame: input frame w x h x 3 [0,255] array\n",
    "        i: index of frame, for indexing parameters\n",
    "        threshold: threshold for color\n",
    "        \n",
    "    Returns:\n",
    "        mask: binary numpy array, with True == keep\n",
    "    \"\"\"    \n",
    "    image = frame.copy()\n",
    "    step = segment_steps[i]\n",
    "    step_conf = conf[step]\n",
    "    \n",
    "    for seg_option in step_conf:\n",
    "        name = seg_option[\"name\"]\n",
    "        \n",
    "        if name == \"color\":\n",
    "            color_choice = seg_option[\"color\"]\n",
    "            mask = mask_color(image, color_choice=color_choice, threshold=threshold)\n",
    "            \n",
    "        elif name == \"erode\":\n",
    "            mask = erode_mask(mask)\n",
    "            \n",
    "        elif name == \"height\":\n",
    "            raise NotImplementedError\n",
    "            depth2 = transform_depth(depth[i], np.linalg.inv(T_tcp_cam))\n",
    "            mask2 = get_mask_depth(depth2, 600, 1550)\n",
    "            mask[mask2] = True\n",
    "    \n",
    "        elif name == \"labels\":\n",
    "            raise NotImplementedError\n",
    "            mask = ndimage.morphology.binary_closing(mask, iterations=4)\n",
    "            mask = label_mask(mask, i)\n",
    "    \n",
    "        elif name == \"imgheight\":\n",
    "            height_val = seg_option[\"height\"]\n",
    "            mask[:height_val, :] = False\n",
    "            \n",
    "        elif name == \"center\":\n",
    "            mask = mask_center(mask)\n",
    "            \n",
    "    return mask\n",
    "\n",
    "# Plot\n",
    "if interactive:\n",
    "    print(\"Colored stuff is keept - mask==True\")\n",
    "    print(\"keysteps:\", keysteps)\n",
    "    print(\"segments: \", len(conf))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    line = ax.imshow(video_recording[25])\n",
    "    ax.set_axis_off()\n",
    "    def update(i, t):\n",
    "        # detect first frame of next step\n",
    "        first_frame = i==0 or i-1 in keysteps\n",
    "        if first_frame:\n",
    "            step = segment_steps[i]\n",
    "            new_t = conf[step][0][\"threshold\"]\n",
    "            print(\"setting t =\", new_t, \"because i =\", i)\n",
    "            slider_t.value = new_t*100\n",
    "\n",
    "        image = video_recording[i].copy()\n",
    "        mask = get_mask(image, i=i, threshold=t/100)    \n",
    "        image[np.logical_not(mask)] = 255, 255, 255\n",
    "        line.set_data(image)\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    slider_i = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    slider_t = widgets.IntSlider(min=0, max=100, step=1, value=conf[0][0][\"threshold\"]*100,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, i=slider_i, t=slider_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = np.zeros(video_recording.shape[:3], dtype=bool)\n",
    "switch_frame = keysteps\n",
    "print(\"switching at:\", switch_frame)\n",
    "\n",
    "for seg_option in conf:\n",
    "    c = seg_option[0][\"color\"]\n",
    "    t = seg_option[0][\"threshold\"]\n",
    "    print(\"c={}, t={}\".format(c, t))\n",
    "\n",
    "for i in tqdm(range(len(video_recording))):\n",
    "    try:\n",
    "        step = segment_steps[i]\n",
    "        threshold = conf[step][0][\"threshold\"]\n",
    "    except IndexError:\n",
    "        break\n",
    "    mask = get_mask(video_recording[i], i, threshold)\n",
    "    masks[i] = mask\n",
    "\n",
    "print(np.mean(masks) * 100, \"% of pixels fg\")\n",
    "mask_fn = recording_fn.replace(\".npz\", \"_mask.npz\")\n",
    "np.savez_compressed(mask_fn, mask=masks)\n",
    "print(\"Saved to\", mask_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. B. Verify Masking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    fig, ax = plt.subplots(1)\n",
    "    handle = ax.imshow(masks[25])\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    def update(i):\n",
    "        image = video_recording[i].copy()\n",
    "        mask = masks[i]\n",
    "        print(round(np.mean(mask)*100),\"%fg, mask shape\", mask.shape)\n",
    "        image[np.logical_not(mask)] = 255, 255, 255\n",
    "        handle.set_data(image)\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    slider_i2 = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, i=slider_i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if seg_masks is not None:\n",
    "    for i in range(num_frames):\n",
    "        image = video_recording[i].copy()\n",
    "        mask = masks[i]\n",
    "        # mask segmentation mask(gt) with fg mask (computed)\n",
    "        ma = np.ma.array(seg_masks[i], mask=np.logical_not(masks[i]))\n",
    "        ma_unique = np.unique(ma, return_counts=True)\n",
    "        # unique is sorted by size, pick the biggest\n",
    "        idx_largest = np.where(ma_unique[0])[-1][0]\n",
    "        seg_id, mask_count = ma_unique[0][idx_largest], ma_unique[1][idx_largest]\n",
    "        seg_count = np.sum(seg_masks[i] == seg_id)\n",
    "        # test how much we segmented / how much there is\n",
    "        score = mask_count / seg_count\n",
    "        assert score > .9\n",
    "        \n",
    "    print(\"Segmentation test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking based on Depth"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#episode_data = np.load(recording_fn)\n",
    "#keys = list(episode_data.keys())\n",
    "#camera_calibration = dict(width=640,height=480,\n",
    "#                     fx = 617.8902587890625, fy=617.8903198242188, \n",
    "#                     ppx=315.20367431640625, ppy=245.70614624023438 )\n",
    "#\n",
    "#T_tcp_cam = np.array([\n",
    "#    [0.99987185, -0.00306941, -0.01571176, 0.00169436],\n",
    "#    [-0.00515523, 0.86743151, -0.49752989, 0.11860651],\n",
    "#    [0.015156,    0.49754713,  0.86730453, -0.18967231],\n",
    "#    [0., 0., 0., 1.]])\n",
    "#\n",
    "#depth = episode_data[\"depth_imgs\"]\n",
    "#depth_scale = 8000\n",
    "#i = 200\n",
    "#print(\"loaded.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#from demon_segment_util import transform_depth\n",
    "#\n",
    "#depth_flat = transform_depth(depth[i], np.linalg.inv(T_tcp_cam))\n",
    "#fig, (ax, ax2) = plt.subplots(1, 2)\n",
    "#line = ax.imshow(depth_flat)\n",
    "#ax2.plot(np.sort(depth[i].flatten()))\n",
    "#ax2.plot(np.sort(depth_flat.flatten()))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#def get_mask_depth(frame, l, h):\n",
    "#    mask = np.logical_or(frame < l/depth_scale, frame > h/depth_scale)\n",
    "#    mask = np.logical_not(mask)\n",
    "#    return mask\n",
    "#\n",
    "#def erode_mask(mask):\n",
    "#    return mask\n",
    "#    mask = ndimage.binary_closing(mask, iterations=5)\n",
    "#    mask = ndimage.morphology.binary_erosion(mask, iterations=10)\n",
    "#    return mask\n",
    "#\n",
    "#\n",
    "#x = np.linspace(0, 2*np.pi)\n",
    "#fig, ax = plt.subplots(1)\n",
    "#line = ax.imshow(video_recording[0])\n",
    "#\n",
    "#def update(w,l,h):\n",
    "#    depth2 = transform_depth(depth[w], np.linalg.inv(T_tcp_cam))\n",
    "#    mask = get_mask_depth(depth2, l, h)\n",
    "#    mask = erode_mask(mask)\n",
    "#    mask = np.logical_not(mask)\n",
    "#    display_image = video_recording[w].copy()\n",
    "#    display_image[mask] = 0\n",
    "#    line.set_data(display_image)\n",
    "#    fig.canvas.draw_idle()\n",
    "#    \n",
    "#depth_min, depth_max = int(depth.min()*depth_scale), int(depth.max()*depth_scale)\n",
    "#slider_w = widgets.IntSlider(min=0, max=max_frame, step=1, value=205,\n",
    "#                             layout=Layout(width='70%'))\n",
    "#slider_l = widgets.IntSlider(min=depth_min, max=depth_max, step=1, value=1560,\n",
    "#                             layout=Layout(width='70%'))\n",
    "#slider_h = widgets.IntSlider(min=depth_min, max=depth_max, step=1, value=1650,\n",
    "#                             layout=Layout(width='70%'))\n",
    "#\n",
    "#interact(update, w=slider_w, l=slider_l, h=slider_h)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# next steps: anneal the edge, and run connected component algorithm.\n",
    "#from scipy import ndimage\n",
    "#\n",
    "#w = slider_w.value\n",
    "#l = slider_l.value\n",
    "#h = slider_h.value\n",
    "#print(\"w={w}, l={l}, h={h}\".format(w=w, l=l, h=h))\n",
    "#\n",
    "#depth2 = transform_depth(depth[w], np.linalg.inv(T_tcp_cam))\n",
    "#mask_s = get_mask_depth(depth2, l, h)\n",
    "#mask_s = erode_mask(mask_s.copy())\n",
    "#mask_s = np.logical_not(mask_s)\n",
    "#display_image = video_recording[w].copy()\n",
    "#display_image[mask_s] = 0\n",
    "#\n",
    "#fig, ax = plt.subplots(1)\n",
    "#line = ax.imshow(display_image)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#threshold_low = slider_l.value\n",
    "#threshold_high = slider_h.value\n",
    "#\n",
    "#masks = np.zeros(video_recording.shape[:3], dtype=bool)\n",
    "#\n",
    "#for i in range(len(video_recording)):\n",
    "#    mask = get_mask_depth(depth[i], threshold_low, threshold_high)\n",
    "#    mask = erode_mask(mask)\n",
    "#    masks[i] = mask\n",
    "#\n",
    "#print(np.mean(masks)*100, \"percent of pixels fg\")\n",
    "#mask_fn = recording_fn.replace(\".npz\", \"_mask.npz\")\n",
    "#np.savez_compressed(mask_fn, mask=masks)\n",
    "#print(\"Saved to\", mask_fn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#fig, ax = plt.subplots(1, 1)\n",
    "#line = ax.imshow(masks[25])\n",
    "#ax.set_axis_off()\n",
    "#\n",
    "#def update(i):\n",
    "#    image = video_recording[i].copy()\n",
    "#    mask = masks[i]\n",
    "#    image[mask] = 255, 255, 255\n",
    "#    line.set_data(image)\n",
    "#    fig.canvas.draw_idle()\n",
    "#    \n",
    "#slider_i2 = widgets.IntSlider(min=0, max=max_frame, step=1, value=200,\n",
    "#                             layout=Layout(width='70%'))\n",
    "#\n",
    "#interact(update, i=slider_i2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
