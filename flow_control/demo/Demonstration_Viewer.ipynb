{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Demonstration View\n",
    "\n",
    "1. Compute Keep Steps\n",
    "    1. Trajectory Analysis\n",
    "    2. Compute Keep Steps\n",
    "    3. Verify\n",
    "2. Compute Masks\n",
    "    1. Compute Mask\n",
    "    2. Verify\n",
    "\n",
    "\n",
    "\n",
    "View a demonstration by sliding through the frames and generate foreground segmentation.\n",
    "\n",
    "This script creates the files that flowcontrol requires: `episode_0_keep.npz`, `episode_0.json`, etc.\n",
    "\n",
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_notebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True  # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False  # Probably standard Python interpreter\n",
    "\n",
    "interactive = is_notebook()  # becomes overwritten\n",
    "if interactive:\n",
    "    get_ipython().run_line_magic('matplotlib', 'notebook')\n",
    "    from ipywidgets import widgets, interact, Layout\n",
    "    import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "from robot_io.recorder.simple_recorder import load_rec_list, unprocess_seg\n",
    "\n",
    "if interactive:    \n",
    "    # set parameters here\n",
    "    #recording, episode_num = \"/home/argusm/lang/flowcontrol/flow_control/tmp_test/pick_n_place/\", 0\n",
    "    recording, episode_num = \"/home/argusm/lang/flowcontrol/flow_control/tests/tmp_test/shape_sorting_rN/\", 0\n",
    "    #recording, episode_num = \"/home/argusm/CLUSTER/robot_recordings/flow/sick_vacuum/17-19-19/\", 0\n",
    "else:\n",
    "    # expect commandline input\n",
    "    import sys\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: Demonstration_Viewer.py <episode_dir> <episode_num>\")\n",
    "    recording = sys.argv[1]\n",
    "    episode_num = int(sys.argv[2])\n",
    "\n",
    "\n",
    "if not os.path.isdir(recording):\n",
    "    ValueError(f\"Recording directory not found: {recording}\")\n",
    "\n",
    "segment_conf_fn = os.path.join(recording, \"segment_conf.json\")\n",
    "keep_fn = os.path.join(recording, f\"servo_keep.json\")\n",
    "mask_fn = os.path.join(recording, f\"servo_mask.npz\")\n",
    "\n",
    "try:\n",
    "    with open(segment_conf_fn, \"r\") as f_obj:\n",
    "        orig_conf = json.load(f_obj)\n",
    "        conf = copy.deepcopy(orig_conf)\n",
    "except FileNotFoundError:\n",
    "    orig_conf = None\n",
    "    conf = [ (dict(name=\"color\", color=(0, 0, 1), threshold=.72), dict(name=\"center\")),\n",
    "             (dict(name=\"color\", color=(1, 0, 0), threshold=.90), dict(name=\"center\")),\n",
    "             (dict(name=\"color\", color=(1, 0, 0), threshold=.90), dict(name=\"center\"))]\n",
    "    with open(segment_conf_fn, \"w\") as f_obj:\n",
    "        json.dump(conf, f_obj)\n",
    "\n",
    "# XXX\n",
    "conf = dict(objects=dict(blue_block=[{'name': 'color', 'color': [0, 0, 1], 'threshold': 0.65}, {'name': 'center'}],\n",
    "                         red_nest=[{'name': 'color', 'color': [1, 0, 0], 'threshold': 0.9}, {'name': 'center'}]),\n",
    "            sequence=(\"blue_block\",\"red_nest\",\"red_nest\"))\n",
    "orig_conf = copy.deepcopy(conf)\n",
    "\n",
    "rec = load_rec_list(recording)\n",
    "actions = np.array([renv.get_action()[\"motion\"] for renv in rec],dtype=object)\n",
    "tcp_pos = np.array([renv.robot.get_tcp_pos() for renv in rec])\n",
    "tcp_orn = np.array([renv.robot.get_tcp_orn() for renv in rec])\n",
    "gripper_width = np.array([renv.robot.gripper.width() for renv in rec])\n",
    "video_recording = np.array([renv.cam.get_image()[0] for renv in rec])\n",
    "\n",
    "if \"seg_mask\" in rec[0].data[\"info\"].item():\n",
    "    masks_sim_l = [unprocess_seg(rec_el.data[\"info\"].item()[\"seg_mask\"])[0] for rec_el in rec]\n",
    "    masks_sim = np.array(masks_sim_l)\n",
    "    print(\"loaded segmentations.\")\n",
    "else:\n",
    "    masks_sim = None\n",
    "\n",
    "assert gripper_width.ndim == 1\n",
    "num_frames = video_recording.shape[0]\n",
    "max_frame = num_frames-1\n",
    "print(\"loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Compute Keep Steps \n",
    "\n",
    "Decide which frames to keep, saved as per-frame boolean array.\n",
    "Various options are possible, current choice is:\n",
    "1. TCP Stationary Filter: find frames where movement is minimal.\n",
    "2. Gripper Stable Filter: look at gripper motion and keep only those where gripper is stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRIPPER_OPEN, GRIPPER_CLOSE = 1.0, -1.0  # assume normalized actions\n",
    "\n",
    "# use actions here instead of state position recordings as these\n",
    "# are more direct and reliable\n",
    "gr_actions = actions[:, -1]\n",
    "keysteps = np.where(np.diff(gr_actions))[0].tolist()\n",
    "keystep_names = [\"\" for _ in range(len(keysteps))]\n",
    "\n",
    "# divide sequence into steps, defined by gripper action\n",
    "segment_steps = np.zeros(num_frames)\n",
    "segment_steps[np.array(keysteps)+1] = 1\n",
    "segment_steps = np.cumsum(segment_steps).astype(int)\n",
    "\n",
    "demonstration_type = [\"navigate\", \"grasp\", \"grasp_insert\"][len(keysteps)]\n",
    "\n",
    "def check_gripper_opens(idx):\n",
    "    # check that we transition open->close & iter segment\n",
    "    assert gr_actions[idx] == GRIPPER_OPEN\n",
    "    assert gr_actions[idx+1] == GRIPPER_CLOSE\n",
    "    assert segment_steps[idx+1] == segment_steps[idx] + 1  # next\n",
    "\n",
    "def check_gripper_closes(idx):\n",
    "    # check that we transition open->close & iter segment\n",
    "    assert gr_actions[idx] == GRIPPER_CLOSE\n",
    "    assert gr_actions[idx+1] == GRIPPER_OPEN\n",
    "    assert segment_steps[idx+1] == segment_steps[idx] + 1  # next\n",
    "\n",
    "# run some checks\n",
    "if demonstration_type == \"grasp\":\n",
    "    assert(len(keysteps) == 1)\n",
    "    check_gripper_opens(keysteps[0])\n",
    "    keystep_names[0] = \"gripper_open\"    \n",
    "elif demonstration_type == \"grasp_insert\":\n",
    "    assert(len(keysteps) == 2)\n",
    "    check_gripper_opens(keysteps[0])\n",
    "    check_gripper_closes(keysteps[1])\n",
    "    keystep_names[0] = \"gripper_open\"\n",
    "    keystep_names[1] = \"gripper_close\"\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "print(\"demonstration type:\", demonstration_type)\n",
    "for kn, kidx in zip(keystep_names, keysteps):\n",
    "    print(kn, \"@\", kidx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 TCP Stationary Filter\n",
    "\n",
    "Slow robot motion indicates motion to a stable position, which we want to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_cont_threshold = .02  # [m/iter] if mean vel above this assume\n",
    "vel_stable_threshold = .002  # [m/iter] if vel below this assume stable\n",
    "\n",
    "def is_demo_continous(pos_vec):\n",
    "    # check if the demonstration is conintous video or individual frames\n",
    "    vel_vec = np.diff(pos_vec, axis=0)\n",
    "    vel_scl = np.linalg.norm(vel_vec, axis=1)\n",
    "    \n",
    "    if np.mean(vel_scl) > vel_cont_threshold:\n",
    "        print(\"Auto-detected: non-continous trajectory\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_stable_points(pos_vec):\n",
    "    vel_vec = np.diff(pos_vec, axis=0)\n",
    "    vel_scl = np.linalg.norm(vel_vec, axis=1)\n",
    "\n",
    "    # This first loop gets minimal regions\n",
    "    active = False\n",
    "    start, stop = -1, -1\n",
    "    min_regions = []\n",
    "    for i in range(len(vel_scl)):\n",
    "        if vel_scl[i] < vel_stable_threshold:\n",
    "            if active:\n",
    "               stop = i\n",
    "            else:\n",
    "                active = True\n",
    "                start, stop = i, i\n",
    "        else:\n",
    "            if active:\n",
    "                min_regions.append((start, stop))\n",
    "                active = False\n",
    "                start, stop = -1, -1\n",
    "\n",
    "    # This second loop gets minimal value\n",
    "    vel_stable = []\n",
    "    for start, stop in min_regions:\n",
    "        try:\n",
    "            min_idx = start + 1 + np.argmin(vel_scl[start:stop])\n",
    "        except ValueError:\n",
    "            min_idx = 0\n",
    "        if len(vel_stable) == 0 or vel_stable[-1] != min_idx:\n",
    "            vel_stable.append(min_idx)\n",
    "    return vel_stable, vel_scl\n",
    "\n",
    "is_continous = is_demo_continous(tcp_pos)\n",
    "if is_continous:\n",
    "    vel_stable, vel_scl = get_stable_points(tcp_pos)\n",
    "else:\n",
    "    vel_vec = np.diff(tcp_pos, axis=0)\n",
    "    vel_scl = np.linalg.norm(vel_vec, axis=1)\n",
    "    vel_stable = list(range(len(tcp_pos)))\n",
    "print(\"vel_stable\", vel_stable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Gripper Stationary Filter\n",
    "\n",
    "Gripper motion makes servoing difficult, filter out those frames where it moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gripper_transitions(gripper_pos, diff_t=.0005, min_duration=5):\n",
    "    # check that the gripper velocity is below diff_t for at least min_duration\n",
    "    gripper_abs_vel = np.abs(np.diff(gripper_pos))\n",
    "    stable = np.concatenate(([True,], gripper_abs_vel < diff_t))\n",
    "       \n",
    "    grip_stable = []\n",
    "    grip_ends = []\n",
    "    for i in range(len(stable)):\n",
    "        snext = np.all(stable[i:min(i+min_duration, len(stable))])\n",
    "        grip_stable.append(snext)\n",
    "        if grip_stable[-2:] == [0, 1]:\n",
    "            grip_ends.append(i-1)\n",
    "            \n",
    "    # fix edge case, gripper dosen't stop in demo\n",
    "    if len(grip_ends) < len(keysteps):\n",
    "        grip_ends.append(max_frame)\n",
    "\n",
    "    grip_unstable = list(zip(keysteps, grip_ends))\n",
    "    return grip_unstable\n",
    "\n",
    "if is_continous:\n",
    "    grip_unstable_intervals = get_gripper_transitions(gripper_width)    \n",
    "else:\n",
    "    grip_unstable_intervals = get_gripper_transitions(gripper_width, min_duration=1)    \n",
    "print(\"grip_unstable\", grip_unstable_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: trust grip unstable, filter out vel_stable\n",
    "def filter_grip_unstable(vel_stable, grip_unstable_intervals):\n",
    "    grip_stable_arr = np.ones(num_frames)\n",
    "    for start, stop in grip_unstable_intervals:\n",
    "        grip_stable_arr[start+1:stop+1] = False\n",
    "\n",
    "    vel_stable_filtered = []\n",
    "    for index in vel_stable:\n",
    "        if grip_stable_arr[index]:\n",
    "            vel_stable_filtered.append(index)\n",
    "    return vel_stable_filtered, grip_stable_arr\n",
    "\n",
    "# Option 2: trust vel_stable, override grip_stable\n",
    "# this is probably a bit more reasonable.\n",
    "\n",
    "vel_stable, grip_stable_arr = filter_grip_unstable(vel_stable, grip_unstable_intervals)\n",
    "print(\"vel_stable\", vel_stable, \"(after filter with grip unstable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keep_dict_sparse(keysteps, vel_stable, max_frame):\n",
    "    keep_dict = {}\n",
    "    keep_dict[0] = dict(name=\"demo_start\")\n",
    "    \n",
    "    # a) first update all stable\n",
    "    for vel_s in vel_stable:\n",
    "        keep_dict[int(vel_s)] = dict(name=\"vel_stable\")\n",
    "        \n",
    "    # b) then ovewrite with grips (order is important here)\n",
    "    for k_idx, k_name in zip(keysteps, keystep_names):\n",
    "        keep_dict[k_idx] = dict(name=k_name)\n",
    "        \n",
    "    keep_dict[int(max_frame)] = dict(name=\"demo_end\")\n",
    "    keep_dict = {k: keep_dict[k] for k in sorted(keep_dict)}\n",
    "    return keep_dict\n",
    "\n",
    "keep_dict = get_keep_dict_sparse(keysteps, vel_stable, max_frame)    \n",
    "print(\"keep_dict\", list(keep_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "stationary_dist_threshold = 0.020  # [m/iter]\n",
    "\n",
    "def get_rel_motion(start_pos, start_orn, finish_pos, finish_orn):\n",
    "    # position\n",
    "    pos_diff = finish_pos - start_pos\n",
    "    ord_diff = R.from_quat(finish_orn).inv() * R.from_quat(start_orn)\n",
    "    #assert ord_diff.magnitude() < .35, ord_diff.magnitude() # for now\n",
    "    return pos_diff.tolist() + ord_diff.as_quat().tolist()\n",
    "\n",
    "def filter_stationary(keep_dict, stationary_dist_threshold):\n",
    "    remove_keys = []\n",
    "    prior_key = None\n",
    "    for key in keep_dict:\n",
    "        if prior_key is None:\n",
    "            prior_key = key\n",
    "            continue\n",
    "        rel_motion = get_rel_motion(tcp_pos[prior_key], tcp_orn[prior_key],\n",
    "                                    tcp_pos[key], tcp_orn[key])\n",
    "        rel_dist = np.linalg.norm(rel_motion[0:3])\n",
    "        same_step = segment_steps[prior_key] == segment_steps[key]\n",
    "        if rel_dist < stationary_dist_threshold and same_step:\n",
    "            print(\"{} -> {}: {:.4f}\".format(prior_key, key, float(rel_dist)),\"(removing)\")\n",
    "            remove_keys.append(prior_key)\n",
    "        else:\n",
    "            prior_key = key\n",
    "            \n",
    "    keep_keys = keep_dict.keys() - remove_keys\n",
    "    keep_dict_filtered = { key: keep_dict[key] for key in keep_keys}\n",
    "    return keep_dict_filtered\n",
    "        \n",
    "keep_dict = filter_stationary(keep_dict, stationary_dist_threshold)\n",
    "print(\"keep_dict\", list(keep_dict.keys()), \"(after filter stationary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_grip_dist(keep_dict, max_dist=10):\n",
    "    step_since_grasp = max_dist\n",
    "    # Iterate backward and save dist to grasp\n",
    "    for key in reversed(sorted(keep_dict)):\n",
    "        name = keep_dict[key][\"name\"]\n",
    "        if name.startswith(\"gripper_\"):\n",
    "            step_since_grasp = 0\n",
    "        else:\n",
    "            step_since_grasp = min(step_since_grasp+1, max_dist)\n",
    "        keep_dict[key][\"grip_dist\"] = step_since_grasp\n",
    "\n",
    "    prior_key = None\n",
    "    for key in sorted(keep_dict):\n",
    "        if prior_key is None:\n",
    "            prior_key = key\n",
    "            continue\n",
    "        pre_dict = {}\n",
    "\n",
    "        same_segment = segment_steps[key] == segment_steps[prior_key]\n",
    "        if not same_segment:\n",
    "            pre_dict[\"grip\"] = gr_actions[key]\n",
    "\n",
    "        if keep_dict[prior_key][\"grip_dist\"] < 2:\n",
    "            rel_motion = get_rel_motion(tcp_pos[prior_key], tcp_orn[prior_key],\n",
    "                                        tcp_pos[key], tcp_orn[key])\n",
    "            pre_dict[\"rel\"] = rel_motion\n",
    "        else:\n",
    "            abs_motion = [*tcp_pos[key], *tcp_orn[key]]\n",
    "            pre_dict[\"abs\"] = abs_motion\n",
    "\n",
    "        keep_dict[key][\"pre\"] = pre_dict\n",
    "        prior_key = key\n",
    "\n",
    "        # double check that we retain all keep steps\n",
    "        #assert(np.all([k in keep_dict.keys() for k in keysteps]))\n",
    "\n",
    "set_grip_dist(keep_dict)\n",
    "\n",
    "with open(keep_fn, 'w') as outfile:\n",
    "    json.dump(keep_dict, outfile)\n",
    "print(\"Saved to\", keep_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. C. Verify keep frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    keep_array = np.zeros(segment_steps.shape)\n",
    "    keep_array[sorted(keep_dict.keys())] = True\n",
    "    fig, (ax, ax2) = plt.subplots(2, 1)\n",
    "    line = ax.imshow(video_recording[0])\n",
    "    ax.set_axis_off()\n",
    "    ax2.plot(gripper_width*10, label=\"grip raw\")\n",
    "    ax2.plot(segment_steps/10, label=\"steps\")\n",
    "    ax2.plot(keep_array, label=\"keep\")\n",
    "    ax2.plot((gr_actions+1)/2, label=\"gripper action\")\n",
    "    ax2.set_ylabel(\"value\")\n",
    "    ax2.set_xlabel(\"frame number\")\n",
    "    vline = ax2.axvline(x=2, color=\"k\")\n",
    "    ax2.legend()\n",
    "\n",
    "    def update(w):\n",
    "        vline.set_data([w, w], [0, 1])\n",
    "        line.set_data(video_recording[w])\n",
    "        fig.canvas.draw_idle()\n",
    "        if w in keep_dict:\n",
    "            print(keep_dict[w])\n",
    "            print()\n",
    "    slider_w = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, w=slider_w)\n",
    "\n",
    "    print(\"What I want to know: do I servo y/n, do I translate?\")\n",
    "    # Convert this keep_array stuff into a dict\n",
    "    # then do one iteration of look ahead to set a servoing flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Gripper Motion\n",
    "Show when the gripping is done, depending on gripper motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gripper\n",
    "if interactive:\n",
    "    val, label = gripper_width, \"gripper_pos\"\n",
    "    fig, (ax, ax2) = plt.subplots(2, 1)\n",
    "    line = ax.imshow(video_recording[0])\n",
    "    ax.set_axis_off()\n",
    "    line1 = ax2.plot((gr_actions+1)/2, label=\"gripper action\", color=\"r\")\n",
    "    line2 = ax2.plot(grip_stable_arr, label=\"grip stable\")\n",
    "    ax2.set_ylabel(\"value\")\n",
    "    ax2.set_xlabel(\"frame number\")\n",
    "    ax2r = ax2.twinx()\n",
    "    line3 = ax2r.plot(val, label=label, color=\"b\")\n",
    "    vline = ax2.axvline(x=2, color=\"k\")\n",
    "    lns = line1+line2+line3\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax2.legend(lns, labs)\n",
    "\n",
    "    def update(w):\n",
    "        print(\"{} @ {} is {}\".format(label, w, val[w]))\n",
    "        vline.set_data([w, w], [0, 1])\n",
    "        line.set_data(video_recording[w])\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    slider_w = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, w=slider_w)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Velocities\n",
    "Look at the end effector motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    val, label =  vel_scl, \"velocity\"\n",
    "    fig, (ax, ax2) = plt.subplots(2, 1)\n",
    "    line = ax.imshow(video_recording[0])\n",
    "    ax.set_axis_off()\n",
    "    line1 = ax2.plot(tcp_pos[:,0], label=\"x\")\n",
    "    line2 = ax2.plot(tcp_pos[:,1], label=\"y\")\n",
    "    line3 = ax2.plot(tcp_pos[:,2], label=\"z\")\n",
    "    ax2.set_ylabel(\"position (x,y,z)\")\n",
    "    ax2.set_xlabel(\"frame number\")\n",
    "    ax2r = ax2.twinx()\n",
    "    ax2r.set_ylabel(\"velocity/threshold\")\n",
    "    line4 = ax2r.plot(val, label=label, color=\"b\")\n",
    "    ax2r.axhline(y=vel_stable_threshold, linestyle=\"--\", color=\"k\")\n",
    "    vline = ax2.axvline(x=2, color=\"k\")\n",
    "    lns = line1+line2+line3+line4\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax2.legend(lns, labs)\n",
    "\n",
    "    #ax2.legend()\n",
    "\n",
    "    def update(w):\n",
    "        print(\"{} @ {} is {}\".format(label, w, val[w] if w<max_frame else \"?\"))\n",
    "        vline.set_data([w, w], [0, 1])\n",
    "        line.set_data(video_recording[w])\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    slider_w = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, w=slider_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compute Mask from Color Images\n",
    "\n",
    "Mask out the foreground object so that foreground specific flow can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "from scipy import ndimage\n",
    "from demo_segment_util import mask_color, erode_mask, label_mask, mask_center\n",
    "\n",
    "# create a segmentation mask\n",
    "def get_mask(frame, step_conf, depth=None):\n",
    "    \"\"\"\n",
    "    create segmentation mask for single frame\n",
    "    Args:\n",
    "        frame: input frame w x h x 3 [0,255] array\n",
    "        i: index of frame, for indexing parameters\n",
    "        threshold: threshold for color\n",
    "        \n",
    "    Returns:\n",
    "        mask: binary numpy array, with True == keep\n",
    "    \"\"\"\n",
    "    threshold = step_conf[0][\"threshold\"]    \n",
    "    image = frame.copy()\n",
    "    \n",
    "    for seg_option in step_conf:\n",
    "        name = seg_option[\"name\"]\n",
    "        \n",
    "        if name == \"color\":\n",
    "            color_choice = seg_option[\"color\"]\n",
    "            mask = mask_color(image, color_choice=color_choice, threshold=threshold)\n",
    "            \n",
    "        elif name == \"erode\":\n",
    "            mask = erode_mask(mask)\n",
    "            \n",
    "        elif name == \"height\":\n",
    "            raise NotImplementedError\n",
    "            depth2 = transform_depth(depth, np.linalg.inv(T_tcp_cam))\n",
    "            mask2 = get_mask_depth(depth2, 600, 1550)\n",
    "            mask[mask2] = True\n",
    "    \n",
    "        elif name == \"labels\":\n",
    "            raise NotImplementedError\n",
    "            mask = ndimage.morphology.binary_closing(mask, iterations=4)\n",
    "            mask = label_mask(mask, i)\n",
    "    \n",
    "        elif name == \"imgheight\":\n",
    "            height_val = seg_option[\"height\"]\n",
    "            mask[:height_val, :] = False\n",
    "            \n",
    "        elif name == \"center\":\n",
    "            mask = mask_center(mask)\n",
    "            \n",
    "    return mask\n",
    "\n",
    "def get_cur_mask(i):\n",
    "    # mask according to current fg object\n",
    "    cur_step = segment_steps[i]\n",
    "    cur_obj = conf[\"sequence\"][cur_step]\n",
    "    step_conf = conf[\"objects\"][cur_obj]\n",
    "    mask = get_mask(video_recording[i], step_conf)\n",
    "    return mask\n",
    "\n",
    "# Plot\n",
    "if interactive:\n",
    "    print(\"Colored stuff is keept (mask==True)\")\n",
    "    print(\"keysteps:\", keysteps)\n",
    "    print(\"segments: \", len(conf))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    line = ax.imshow(video_recording[0])\n",
    "    ax.set_axis_off()\n",
    "    prev_step = 0\n",
    "    def update(i, t):\n",
    "        cur_step = segment_steps[i]\n",
    "        cur_obj = conf[\"sequence\"][cur_step]\n",
    "        global prev_step\n",
    "        if cur_step != prev_step:\n",
    "            # don't change order here, without double checking\n",
    "            saved_t = conf[\"objects\"][cur_obj][0][\"threshold\"]\n",
    "            print(f\"switching step {prev_step} -> {cur_step}, loading t={saved_t}\")\n",
    "            prev_step = cur_step\n",
    "            slider_t.value = saved_t*100\n",
    "        else:\n",
    "            conf[\"objects\"][cur_obj][0][\"threshold\"] = t/100\n",
    "            \n",
    "        mask = get_cur_mask(i)\n",
    "        image = video_recording[i].copy()\n",
    "        image[np.logical_not(mask)] = 255, 255, 255\n",
    "        line.set_data(image)\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    slider_i = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    first_obj = conf[\"sequence\"][0]\n",
    "    slider_t = widgets.IntSlider(min=0, max=100, step=1, value=conf[\"objects\"][first_obj][0][\"threshold\"]*100,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, i=slider_i, t=slider_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = np.zeros(video_recording.shape[:3], dtype=bool)\n",
    "switch_frame = keysteps\n",
    "print(\"switching at:\", switch_frame)\n",
    "if orig_conf is None:\n",
    "    orig_conf = conf\n",
    "\n",
    "\"\"\"\n",
    "# display changes files to selected.\n",
    "for seg_option, seg_option_orig in zip(conf, orig_conf):\n",
    "    c = seg_option[0][\"color\"]\n",
    "    t = seg_option[0][\"threshold\"]\n",
    "    t_i = seg_option_orig[0][\"threshold\"]\n",
    "    if t != t_i:\n",
    "        print(\"c={}, t={} | t'={}\".format(c, t, t_i))\n",
    "    else:\n",
    "        print(\"c={}, t={}\".format(c, t))\n",
    "\"\"\"\n",
    "obj_ids = {}\n",
    "obj_ids_list = []\n",
    "for i, obj in enumerate(conf[\"objects\"]):\n",
    "    obj_ids[obj] = i+1\n",
    "    obj_ids_list.append(i+1)\n",
    "    print(f\"{obj} -> {i+1}\")\n",
    "\n",
    "\n",
    "fg_obj = []\n",
    "masks_list = []\n",
    "for i in tqdm(range(len(video_recording))):\n",
    "    # get foreground object\n",
    "    cur_step = segment_steps[i]\n",
    "    cur_obj = conf[\"sequence\"][cur_step]\n",
    "    fg_obj.append(obj_ids[cur_obj])\n",
    "    \n",
    "    m_masks = []\n",
    "    for obj_name in conf[\"objects\"]:\n",
    "        mask = get_mask(video_recording[i], conf[\"objects\"][obj_name])\n",
    "        m_masks.append(mask)\n",
    "    \n",
    "    overlapp = np.sum(m_masks, axis=0) > 1\n",
    "    if np.any(overlapp):\n",
    "        print(\"WARNING: There is overlapp.\")\n",
    "    masks_list.append(m_masks)\n",
    "    \n",
    "fg_obj = np.array(fg_obj)\n",
    "assert fg_obj.ndim == 1\n",
    "\n",
    "masks_list = np.array(masks_list)\n",
    "masks_list = masks_list.transpose(1, 0, 2, 3).astype(np.uint8)\n",
    "obj_ids_arr = np.array(obj_ids_list).reshape(-1, 1, 1, 1)\n",
    "masks_list = obj_ids_arr*masks_list\n",
    "masks_list = masks_list.sum(axis=0)\n",
    "assert masks_list.ndim == 3\n",
    "\n",
    "\n",
    "print(round(np.mean(masks) * 100), \"% of pixels fg\")\n",
    "np.savez_compressed(mask_fn, mask=masks, fg=fg_obj)\n",
    "print(\"Saved to\", mask_fn)\n",
    "\n",
    "if conf != orig_conf:\n",
    "    print(\"Warning using new conf values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. B. Verify Masking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    fig, ax = plt.subplots(1)\n",
    "    handle = ax.imshow(masks[0])\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    def update(i):\n",
    "        image = video_recording[i].copy()\n",
    "        mask = masks_list[i] == fg_obj[i]\n",
    "        print(round(np.mean(mask)*100), \"% fg, mask shape\", mask.shape)\n",
    "        image[np.logical_not(mask)] = 255, 255, 255\n",
    "        handle.set_data(image)\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    slider_i2 = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, i=slider_i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if masks_sim is not None:\n",
    "    for i in range(num_frames):\n",
    "        image = video_recording[i].copy()\n",
    "        mask = masks_list[i] == fg_obj[i]\n",
    "        # mask segmentation mask(gt) with fg mask (computed)\n",
    "        ma = np.ma.array(masks_sim[i], mask=np.logical_not(mask))\n",
    "        ma_unique = np.unique(ma, return_counts=True)\n",
    "        # unique is sorted by size, pick the biggest\n",
    "        idx_largest = np.where(ma_unique[0])[-1][0]\n",
    "        seg_id, mask_count = ma_unique[0][idx_largest], ma_unique[1][idx_largest]\n",
    "        seg_count = np.sum(masks_sim[i] == seg_id)\n",
    "        # test how much we segmented / how much there is\n",
    "        score = mask_count / seg_count\n",
    "        assert score > .9\n",
    "        \n",
    "    print(\"Segmentation test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Mask from Simulation\n",
    "\n",
    "This cell extracts foreground masks from simulation recordings. It does this by looking at the recordings info variables, where a anchor object UID can be specified. This is usually done by the task policy.\n",
    "\n",
    "The move_anchor is the object with which we are moving relative to, this is most often but not always the object of interest or the foreground object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if masks_sim is not None:    \n",
    "    fg_obj_sim = np.array([rec_el.data[\"info\"].item()[\"move_anchor\"] for rec_el in rec])    \n",
    "    #print(foreground_uids)\n",
    "    #print(np.unique(masks_sim))\n",
    "    \n",
    "    np.savez_compressed(mask_fn, mask=masks_sim, fg=fg_obj_sim)\n",
    "    print(\"Saved to\", mask_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    fig, ax = plt.subplots(1)\n",
    "    handle = ax.imshow(video_recording[0])\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    def update(i):\n",
    "        image = video_recording[i].copy()\n",
    "        mask = masks_sim[i] == fg_obj_sim[i]\n",
    "        print(round(np.mean(mask)*100), \"% fg, mask shape\", mask.shape)\n",
    "        image[np.logical_not(mask)] = 255, 255, 255\n",
    "        handle.set_data(image)\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    slider_i2 = widgets.IntSlider(min=0, max=max_frame, step=1, value=0,\n",
    "                                 layout=Layout(width='70%'))\n",
    "    interact(update, i=slider_i2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Masking based on Depth"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#episode_data = np.load(recording_fn)\n",
    "#keys = list(episode_data.keys())\n",
    "#camera_calibration = dict(width=640,height=480,\n",
    "#                     fx = 617.8902587890625, fy=617.8903198242188, \n",
    "#                     ppx=315.20367431640625, ppy=245.70614624023438 )\n",
    "#\n",
    "#T_tcp_cam = np.array([\n",
    "#    [0.99987185, -0.00306941, -0.01571176, 0.00169436],\n",
    "#    [-0.00515523, 0.86743151, -0.49752989, 0.11860651],\n",
    "#    [0.015156,    0.49754713,  0.86730453, -0.18967231],\n",
    "#    [0., 0., 0., 1.]])\n",
    "#\n",
    "#depth = episode_data[\"depth_imgs\"]\n",
    "#depth_scale = 8000\n",
    "#i = 200\n",
    "#print(\"loaded.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#from demon_segment_util import transform_depth\n",
    "#\n",
    "#depth_flat = transform_depth(depth[i], np.linalg.inv(T_tcp_cam))\n",
    "#fig, (ax, ax2) = plt.subplots(1, 2)\n",
    "#line = ax.imshow(depth_flat)\n",
    "#ax2.plot(np.sort(depth[i].flatten()))\n",
    "#ax2.plot(np.sort(depth_flat.flatten()))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#def get_mask_depth(frame, l, h):\n",
    "#    mask = np.logical_or(frame < l/depth_scale, frame > h/depth_scale)\n",
    "#    mask = np.logical_not(mask)\n",
    "#    return mask\n",
    "#\n",
    "#def erode_mask(mask):\n",
    "#    return mask\n",
    "#    mask = ndimage.binary_closing(mask, iterations=5)\n",
    "#    mask = ndimage.morphology.binary_erosion(mask, iterations=10)\n",
    "#    return mask\n",
    "#\n",
    "#\n",
    "#x = np.linspace(0, 2*np.pi)\n",
    "#fig, ax = plt.subplots(1)\n",
    "#line = ax.imshow(video_recording[0])\n",
    "#\n",
    "#def update(w,l,h):\n",
    "#    depth2 = transform_depth(depth[w], np.linalg.inv(T_tcp_cam))\n",
    "#    mask = get_mask_depth(depth2, l, h)\n",
    "#    mask = erode_mask(mask)\n",
    "#    mask = np.logical_not(mask)\n",
    "#    display_image = video_recording[w].copy()\n",
    "#    display_image[mask] = 0\n",
    "#    line.set_data(display_image)\n",
    "#    fig.canvas.draw_idle()\n",
    "#    \n",
    "#depth_min, depth_max = int(depth.min()*depth_scale), int(depth.max()*depth_scale)\n",
    "#slider_w = widgets.IntSlider(min=0, max=max_frame, step=1, value=205,\n",
    "#                             layout=Layout(width='70%'))\n",
    "#slider_l = widgets.IntSlider(min=depth_min, max=depth_max, step=1, value=1560,\n",
    "#                             layout=Layout(width='70%'))\n",
    "#slider_h = widgets.IntSlider(min=depth_min, max=depth_max, step=1, value=1650,\n",
    "#                             layout=Layout(width='70%'))\n",
    "#\n",
    "#interact(update, w=slider_w, l=slider_l, h=slider_h)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# next steps: anneal the edge, and run connected component algorithm.\n",
    "#from scipy import ndimage\n",
    "#\n",
    "#w = slider_w.value\n",
    "#l = slider_l.value\n",
    "#h = slider_h.value\n",
    "#print(\"w={w}, l={l}, h={h}\".format(w=w, l=l, h=h))\n",
    "#\n",
    "#depth2 = transform_depth(depth[w], np.linalg.inv(T_tcp_cam))\n",
    "#mask_s = get_mask_depth(depth2, l, h)\n",
    "#mask_s = erode_mask(mask_s.copy())\n",
    "#mask_s = np.logical_not(mask_s)\n",
    "#display_image = video_recording[w].copy()\n",
    "#display_image[mask_s] = 0\n",
    "#\n",
    "#fig, ax = plt.subplots(1)\n",
    "#line = ax.imshow(display_image)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#threshold_low = slider_l.value\n",
    "#threshold_high = slider_h.value\n",
    "#\n",
    "#masks = np.zeros(video_recording.shape[:3], dtype=bool)\n",
    "#\n",
    "#for i in range(len(video_recording)):\n",
    "#    mask = get_mask_depth(depth[i], threshold_low, threshold_high)\n",
    "#    mask = erode_mask(mask)\n",
    "#    masks[i] = mask\n",
    "#\n",
    "#print(np.mean(masks)*100, \"percent of pixels fg\")\n",
    "#mask_fn = recording_fn.replace(\".npz\", \"_mask.npz\")\n",
    "#np.savez_compressed(mask_fn, mask=masks)\n",
    "#print(\"Saved to\", mask_fn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#fig, ax = plt.subplots(1, 1)\n",
    "#line = ax.imshow(masks[25])\n",
    "#ax.set_axis_off()\n",
    "#\n",
    "#def update(i):\n",
    "#    image = video_recording[i].copy()\n",
    "#    mask = masks[i]\n",
    "#    image[mask] = 255, 255, 255\n",
    "#    line.set_data(image)\n",
    "#    fig.canvas.draw_idle()\n",
    "#    \n",
    "#slider_i2 = widgets.IntSlider(min=0, max=max_frame, step=1, value=200,\n",
    "#                             layout=Layout(width='70%'))\n",
    "#\n",
    "#interact(update, i=slider_i2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
