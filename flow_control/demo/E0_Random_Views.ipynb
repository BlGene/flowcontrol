{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c85e47ab",
   "metadata": {},
   "source": [
    "# Record Random Views\n",
    "\n",
    "Record random views in simulation. This type of data is easy to collect on a real robot, which is why its a good candidate to use it for pre-training a similarity function. Set `num_episodes` to control the number of scenes recorded and `max_steps` to control the numbers of views recorded.\n",
    "\n",
    "The equivalent recordings from the real robot can be found here:\n",
    "\n",
    "`/misc/lmbraid19/argusm/CLUSTER/robot_recordings/pose_estimation/ur3_objects`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e27844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 20 2022 19:44:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buechner/servoing/robot_io/robot_io/calibration/gripper_cam_calibration.py:108: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"../../conf\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "import unittest\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from gym_grasping.envs.robot_sim_env import RobotSimEnv\n",
    "from flow_control.demo.demo_episode_recorder import record_sim\n",
    "from flow_control.runner import evaluate_control\n",
    "from flow_control.servoing.module import ServoingModule\n",
    "from math import pi\n",
    "import getpass\n",
    "from robot_io.calibration.gripper_cam_calibration import GripperCamPoseSampler\n",
    "\n",
    "\n",
    "experiment = \"random_views\"\n",
    "goal = \"random_views_test\"\n",
    "\n",
    "def get_data_dir():\n",
    "    username = getpass.getuser()\n",
    "    if username == \"argusm\":\n",
    "        return \"/tmp/flow_experiments3\"\n",
    "    elif username == \"nayakab\":\n",
    "        return \"../tmp\"\n",
    "    elif username == \"buechner\":\n",
    "        return \"/home/buechner/servoing/data/\"\n",
    "\n",
    "data_dir = get_data_dir()\n",
    "root_dir = os.path.join(data_dir, experiment)\n",
    "goal_dir = os.path.join(data_dir, goal)\n",
    "\n",
    "renderer = \"egl\"\n",
    "object_selected = \"trapeze\"\n",
    "task_variant = \"rP\"  # rotation plus (+-pi)\n",
    "\n",
    "def get_configurations(root_dir=root_dir, task=\"shape_sorting\", num_episodes=20, start_seed=0, prefix=\"\", object_selected='trapeze'):\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    save_dir_template = os.path.join(root_dir, f\"{prefix}_{task}_{object_selected}\")\n",
    "    for seed in range(start_seed, start_seed + num_episodes):\n",
    "        save_dir = save_dir_template + f\"_{task_variant}\"+f\"_seed{seed:03d}\"\n",
    "        yield object_selected, task_variant, seed, save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9050bf6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EGL device choice: -1 of 17.\n",
      "/home/buechner/venvs/servo/lib/python3.8/site-packages/gym-0.26.2-py3.8.egg/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=--width=256\n",
      "argv[1]=--height=256\n",
      "Loaded EGL 1.5 after reload.\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 470.141.10\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 470.141.10\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "b3Warning[src/BulletInverseDynamics/MultiBodyTree.cpp,268]:\n",
      "axis of motion not a unit axis ([-0.000796 -0.000796 -0.999999]), will use normalized vector\n",
      "b3Warning[src/BulletInverseDynamics/MultiBodyTree.cpp,268]:\n",
      "axis of motion not a unit axis ([0.000796 -0.000796 -0.999999]), will use normalized vector\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m\n\u001b[1;32m     17\u001b[0m env \u001b[38;5;241m=\u001b[39m RobotSimEnv(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecombination\u001b[39m\u001b[38;5;124m'\u001b[39m, renderer\u001b[38;5;241m=\u001b[39mrenderer, act_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m                   initial_pose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m, max_steps\u001b[38;5;241m=\u001b[39mmax_steps, control\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabsolute-full\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m                   img_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m                   task_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(object_rot_range\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrP\u001b[39m\u001b[38;5;124m\"\u001b[39m:pi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrR\u001b[39m\u001b[38;5;124m\"\u001b[39m:pi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m6.\u001b[39m}[task_variant]),\n\u001b[1;32m     23\u001b[0m                   seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_variant \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m env\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mvariables[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_selected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_pose\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m pi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_variant \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrR\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m env\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mvariables[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_selected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_pose\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m pi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m6.\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from robot_io.recorder.simple_recorder import SimpleRecorder\n",
    "from tqdm import tqdm\n",
    "\n",
    "cfg = { 0: {'prefix': 'train', 'task': 'shape_sorting', 'num_episodes': 200, 'start_seed': 0, 'object_selected': object_selected, 'root_dir': root_dir},\n",
    "      #  1: {'prefix': 'test', 'task': 'shape_sorting', 'num_episodes': 20, 'start_seed': 1000, 'object_selected': object_selected, 'root_dir': root_dir}\n",
    "      }\n",
    "\n",
    "max_steps = 200\n",
    "for key, value in cfg.items():\n",
    "    demo_cfg = get_configurations(value['root_dir'], value['task'], \n",
    "                                  value['num_episodes'], value['start_seed'],\n",
    "                                  prefix=value['prefix'],\n",
    "                                  object_selected=value['object_selected'])\n",
    "    \n",
    "    for object_selected, task_variant, seed, save_dir in demo_cfg:\n",
    "        param_info = {\"object_selected\": value['object_selected'], \"task_selected\": value['task']}\n",
    "        env = RobotSimEnv(task='recombination', renderer=renderer, act_type='continuous',\n",
    "                          initial_pose='close', max_steps=max_steps, control='absolute-full',\n",
    "                          img_size=(256, 256),\n",
    "                          param_randomize=(\"geom\",),\n",
    "                          param_info=param_info,\n",
    "                          task_info=dict(object_rot_range={\"rP\":pi/2.,\"rR\":pi/6.}[task_variant]),\n",
    "                          seed=seed)\n",
    "\n",
    "        if task_variant == \"rP\":\n",
    "            assert env.params.variables[f\"{object_selected}_pose\"][\"d\"][3] == pi/2.\n",
    "        elif task_variant == \"rR\":\n",
    "            assert env.params.variables[f\"{object_selected}_pose\"][\"d\"][3] == pi/6.\n",
    "        #update_object_orn(env, object_selected, orn)\n",
    "\n",
    "        s = 2.0\n",
    "        inital_pos, initial_orn = env.robot.get_tcp_pos_orn()\n",
    "        pose_sampler = GripperCamPoseSampler(inital_pos, initial_orn,\n",
    "                                             theta_limits=[2.36, 3.93],\n",
    "                                             r_limits=[0.05, 0.1],\n",
    "                                             h_limits=[-0.05*s, 0.05*s],\n",
    "                                             trans_limits=[-0.05*s, 0.05*s],\n",
    "                                             yaw_limits=[-0.087*s, 0.087*s],\n",
    "                                             pitch_limit=[-0.087*s, 0.087*s],\n",
    "                                             roll_limit=[-0.087*s, 0.087*s])\n",
    "        rec = SimpleRecorder(env, save_dir=save_dir)\n",
    "    \n",
    "        for i in tqdm(range(max_steps)):\n",
    "            pos, orn = pose_sampler.sample_pose()\n",
    "            action = dict(motion=(pos,orn,1), ref=\"abs\")\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rec.step(obs, action, None, rew, done, info)\n",
    "            \n",
    "        rec.save()\n",
    "        \n",
    "        del env\n",
    "        time.sleep(.5)\n",
    "        print(f\"{seed}/{value['num_episodes']}\", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e818190b",
   "metadata": {},
   "source": [
    "# View Recorded Data\n",
    "\n",
    "Show the dataset that we have recorded.\n",
    "\n",
    "TODO: include orientation in the relative distance computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_control.servoing.playback_env_servo import PlaybackEnvServo\n",
    "\n",
    "def get_recordings(directory):\n",
    "    return sorted([os.path.join(directory, rec) for rec in os.listdir(directory) if os.path.isdir(os.path.join(directory, rec))])\n",
    "\n",
    "demo_recordings = get_recordings(root_dir)\n",
    "\n",
    "recordings = demo_recordings\n",
    "print(\"Number of recordings:\", len(recordings))\n",
    "print(recordings[0])\n",
    "print(recordings[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fa3474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the demonstration episodes\n",
    "playbacks = [PlaybackEnvServo(rec) for rec in recordings[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b36b9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import widgets, interact, Layout\n",
    "\n",
    "# Plot the demonstrations\n",
    "%matplotlib notebook\n",
    "fig, ax = plt.subplots(1, 2,figsize=(8, 6))\n",
    "fig.suptitle(\"Demonstration Frames\")\n",
    "ax[0].set_axis_off()\n",
    "ax[1].set_axis_off()\n",
    "image_h = ax[0].imshow(playbacks[0].cam.get_image()[0])\n",
    "\n",
    "t_tcp_cam = playbacks[demo_index][0].cam.get_extrinsic_calibration()\n",
    "\n",
    "positions = []\n",
    "orientations = []\n",
    "demo_index = 0\n",
    "for frame_index in range(len(playbacks[demo_index])):\n",
    "    t_tcp_robot = playbacks[demo_index][frame_index].robot.get_tcp_pose()\n",
    "    trf = t_tcp_robot @ t_tcp_cam\n",
    "    trf = np.linalg.inv(trf)\n",
    "    positions.append(trf[0:3,3])\n",
    "    orientations.append(trf[:3,:3])\n",
    "    \n",
    "positions = np.array(positions)\n",
    "dist = np.linalg.norm(positions[:, None, :] - positions[None, :, :], axis=-1)\n",
    "\n",
    "# distance computation for orientation would be \n",
    "# orn_diff = (R.from_quat(goal_orn)*R.from_quat(cur_orn).inv()).magnitude()\n",
    "\n",
    "ax[1].imshow(dist)\n",
    "np.fill_diagonal(dist, 1e3)\n",
    "\n",
    "def update(demo_index, frame_index):\n",
    "    image = playbacks[demo_index][frame_index].cam.get_image()[0]\n",
    "    print(\"closest neighbor:\", np.argmin(dist[frame_index]), \"(frame index)\")\n",
    "    image_h.set_data(image)\n",
    "    fig.canvas.draw_idle()\n",
    "    \n",
    "slider_w = widgets.IntSlider(min=0, max=len(playbacks)-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "slider_i = widgets.IntSlider(min=0, max=200-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "\n",
    "interact(update, demo_index=slider_w, frame_index=slider_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98785a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "servo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (default, Dec  9 2021, 17:53:27) \n[GCC 8.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1b46e996318d00ee8b18208f5e070fd91c5c69b566ca27af63ee58c19832482"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
